{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tabulate import tabulate\n",
    "import matplotlib.pyplot as plt\n",
    "from decimal import Decimal, ROUND_DOWN\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,roc_auc_score, classification_report, \n",
    "    confusion_matrix, ConfusionMatrixDisplay, roc_curve\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import (\n",
    "    StackingClassifier, GradientBoostingClassifier, \n",
    "    GradientBoostingClassifier, HistGradientBoostingClassifier,\n",
    "    AdaBoostClassifier, RandomForestClassifier\n",
    ")\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from snapml import BoostingMachineClassifier  \n",
    "from xgboost import XGBClassifier\n",
    "from ngboost import NGBClassifier\n",
    "from ngboost.distns import Bernoulli "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/preprocessed_data_full_encoded_new_v4.csv')\n",
    "cols_to_keep = ['age', \n",
    "                'height',\n",
    "                'weight',\n",
    "                'systolic', \n",
    "                'diastolic',\n",
    "                'bmi',\n",
    "                'map',\n",
    "                'pulse_pressure',\n",
    "                'gender',\n",
    "                'cholesterol', \n",
    "                'gluc',\n",
    "                'smoke', \n",
    "                'alco', \n",
    "                'active',\n",
    "                'cardio'\n",
    "                ]\n",
    "\n",
    "df = df[cols_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contoh data train setelah scaling:\n",
      "        age    height    weight  systolic  diastolic       bmi       map  \\\n",
      "0  0.047973  1.554091  1.702976 -0.416790  -0.183072  0.706152 -0.319255   \n",
      "1  0.342487 -0.430436 -0.774215 -0.416790  -0.183072 -0.570828 -0.319255   \n",
      "2  0.931516  0.363375  0.584244  0.923307  -0.183072  0.358079  0.382521   \n",
      "3  0.195230 -0.695040  0.184697 -1.086838  -1.456053  0.575892 -1.371393   \n",
      "4 -0.541056 -0.562738 -1.653219 -1.756886  -1.456053 -1.422859 -1.721755   \n",
      "\n",
      "   pulse_pressure  gender  cholesterol  gluc  smoke  alco  active  \n",
      "0       -0.450279     1.0          2.0   0.0    1.0   1.0     0.0  \n",
      "1       -0.450279     0.0          1.0   0.0    0.0   0.0     1.0  \n",
      "2        1.432884     0.0          1.0   2.0    0.0   0.0     0.0  \n",
      "3       -0.450279     0.0          0.0   0.0    0.0   0.0     1.0  \n",
      "4       -1.391861     0.0          0.0   0.0    0.0   1.0     1.0  \n"
     ]
    }
   ],
   "source": [
    "X = df.drop('cardio', axis=1)\n",
    "y = df['cardio']\n",
    "\n",
    "label_mapping = {0: 'Healthy', 1: 'Cardio Risk'}\n",
    "target_names = [label_mapping[label] for label in y.unique()]\n",
    "\n",
    "numerical_features = ['age', 'height', 'weight', 'systolic', 'diastolic', 'bmi', 'map', 'pulse_pressure']\n",
    "ordinal_features = ['cholesterol', 'gluc']\n",
    "binary_features = ['gender', 'smoke', 'alco', 'active']\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train_transformed = preprocessor.fit_transform(X_train)\n",
    "X_test_transformed = preprocessor.transform(X_test)\n",
    "\n",
    "passthrough_features = [col for col in X.columns if col not in numerical_features]\n",
    "transformed_feature_names = numerical_features + passthrough_features\n",
    "\n",
    "X_train_transformed_df = pd.DataFrame(X_train_transformed, columns=transformed_feature_names)\n",
    "X_test_transformed_df = pd.DataFrame(X_test_transformed, columns=transformed_feature_names)\n",
    "print(\"Contoh data train setelah scaling:\")\n",
    "print(X_train_transformed_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_np = np.array(X_train)\n",
    "y_train_np = np.array(y_train)\n",
    "x_test_np = np.array(X_test)\n",
    "y_test_np = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, x_train, y_train, x_test, y_test, model_name):        \n",
    "    model.fit(x_train, y_train)\n",
    "    predict_func = lambda x: model.predict(x)\n",
    "    predict_proba_func = lambda x: model.predict_proba(x)\n",
    "\n",
    "    y_pred_test = predict_func(x_test)\n",
    "    y_proba_test = predict_proba_func(x_test)\n",
    "    \n",
    "    if y_proba_test.shape[1] > 1:\n",
    "        y_probs_test = y_proba_test[:, 1]\n",
    "    else:\n",
    "        y_probs_test = y_proba_test[:, 0]\n",
    "    \n",
    "    y_pred_train = predict_func(x_train)\n",
    "    y_proba_train = predict_proba_func(x_train)\n",
    "    if y_proba_train.shape[1] > 1:\n",
    "        y_probs_train = y_proba_train[:, 1]\n",
    "    else:\n",
    "        y_probs_train = y_proba_train[:, 0]\n",
    "    \n",
    "    test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "    test_acc_str = f\"{int(test_accuracy * 100 * 100) / 100:.2f}%\"\n",
    "    test_auc = roc_auc_score(y_test, y_probs_test)\n",
    "    test_auc_str = f\"{test_auc:.4f}\"\n",
    "    test_report_dict = classification_report(y_test, y_pred_test, output_dict=True)\n",
    "    test_precision = test_report_dict['weighted avg']['precision'] \n",
    "    test_recall    = test_report_dict['weighted avg']['recall']   \n",
    "    test_f1        = test_report_dict['weighted avg']['f1-score']\n",
    "    \n",
    "    train_accuracy = accuracy_score(y_train, y_pred_train)\n",
    "    train_acc_str = f\"{(train_accuracy  * 100 * 100) / 100:.2f}%\"\n",
    "    train_auc = roc_auc_score(y_train, y_probs_train)\n",
    "    train_auc_str = f\"{train_auc:.4f}\"\n",
    "    train_report_dict = classification_report(y_train, y_pred_train, output_dict=True)\n",
    "    train_precision = train_report_dict['weighted avg']['precision']\n",
    "    train_recall    = train_report_dict['weighted avg']['recall']    \n",
    "    train_f1        = train_report_dict['weighted avg']['f1-score']  \n",
    "\n",
    "    data = [\n",
    "        [\"Test\", test_acc_str, test_auc_str],\n",
    "        [\"Train\", train_acc_str, train_auc_str]\n",
    "    ]\n",
    "\n",
    "    headers = [\"\", \"Accuracy\", \"AUC Score\"]\n",
    "\n",
    "    print(f\"\\n=== {model_name} ===\\n\")\n",
    "    print(tabulate(data, headers=headers, tablefmt=\"grid\"))\n",
    "    \n",
    "    print(\"\\nOverfitting Check :\")\n",
    "    if train_accuracy > test_accuracy + 5 or train_auc > test_auc + 0.05:\n",
    "        print(\"The model might be overfitting.\")\n",
    "    else:\n",
    "        print(\"No significant signs of overfitting.\\n\")\n",
    "    \n",
    "    # fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    # cm = confusion_matrix(y_test, y_pred_test)\n",
    "\n",
    "    # display_labels = list(label_mapping.values()) if 'label_mapping' in globals() else None\n",
    "    # disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=display_labels)\n",
    "    # disp.plot(ax=axes[0], cmap='viridis', colorbar=False)\n",
    "    # axes[0].set_title(f\"{model_name} - Confusion Matrix\")\n",
    "    \n",
    "    # fpr, tpr, _ = roc_curve(y_test, y_probs_test)\n",
    "    # axes[1].plot(fpr, tpr, label=f\"ROC Curve (AUC = {test_auc:.4f})\", linewidth=2)\n",
    "    # axes[1].plot([0, 1], [0, 1], 'k--', label=\"Random Guess\", linewidth=1)\n",
    "    # axes[1].set_title(f\"{model_name} - ROC Curve\")\n",
    "    # axes[1].legend(loc=\"lower right\")\n",
    "    # axes[1].grid(alpha=0.3)\n",
    "    \n",
    "    # plt.tight_layout()\n",
    "    # plt.show()\n",
    "    \n",
    "    return {\n",
    "        'model_name': model_name,\n",
    "        'train_accuracy': train_accuracy,\n",
    "        'train_precision': train_precision,\n",
    "        'train_recall': train_recall,\n",
    "        'train_f1': train_f1,\n",
    "        'test_accuracy': test_accuracy,\n",
    "        'test_precision': test_precision,\n",
    "        'test_recall': test_recall,\n",
    "        'test_f1': test_f1\n",
    "    }\n",
    "\n",
    "\n",
    "def create_summary_table(results):\n",
    "    test_summary = pd.DataFrame([{\n",
    "        'Algorithm': r['model_name'],\n",
    "        'Accuracy':  round(r['test_accuracy'], 4),\n",
    "        'Precision': round(r['test_precision'], 4),\n",
    "        'Recall':    round(r['test_recall'], 4),\n",
    "        'F1-Score':  round(r['test_f1'], 4)\n",
    "    } for r in results])\n",
    "    train_summary = pd.DataFrame([{\n",
    "        'Algorithm': r['model_name'],\n",
    "        'Accuracy':  round(r['train_accuracy'], 4),\n",
    "        'Precision': round(r['train_precision'], 4),\n",
    "        'Recall':    round(r['train_recall'], 4),\n",
    "        'F1-Score':  round(r['train_f1'], 4)\n",
    "    } for r in results])\n",
    "    \n",
    "    \n",
    "    print(\"\\nSummary Table - Test Metrics\")\n",
    "    print(tabulate(test_summary, headers='keys', tablefmt='grid', showindex=False))\n",
    "\n",
    "    print(\"Summary Table - Training Metrics\")\n",
    "    print(tabulate(train_summary, headers='keys', tablefmt='grid', showindex=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Akurasi model: 0.8828828828828829\n"
     ]
    }
   ],
   "source": [
    "best_params = {\n",
    "    'n_estimators': 233,\n",
    "    'learning_rate': 0.019519792757748358,\n",
    "    'num_leaves': 41,\n",
    "    'max_depth': 15,\n",
    "    'subsample': 0.758484089588373,\n",
    "    'colsample_bytree': 0.9592852139230149,\n",
    "    'random_state': 333\n",
    "}\n",
    "\n",
    "lgbm_model = LGBMClassifier(**best_params, verbose=-1)\n",
    "\n",
    "lgbm_model.fit(X_train, y_train)\n",
    "\n",
    "# Menghitung akurasi dengan metode score\n",
    "accuracy = lgbm_model.score(X_test, y_test)\n",
    "print(\"Akurasi model:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Akurasi model: 0.8811425061425061\n"
     ]
    }
   ],
   "source": [
    "lgbm_model.fit(X_train, y_train)\n",
    "\n",
    "# Menghitung akurasi dengan metode score\n",
    "accuracy = lgbm_model.score(X_test, y_test)\n",
    "print(\"Akurasi model:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== LightGBM ===\n",
      "\n",
      "+-------+------------+-------------+\n",
      "|       | Accuracy   |   AUC Score |\n",
      "+=======+============+=============+\n",
      "| Test  | 88.28%     |      0.9618 |\n",
      "+-------+------------+-------------+\n",
      "| Train | 88.96%     |      0.9678 |\n",
      "+-------+------------+-------------+\n",
      "\n",
      "Overfitting Check :\n",
      "No significant signs of overfitting.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lgbm_results = evaluate_model(lgbm_model, X_train, y_train, X_test, y_test, \"LightGBM\")\n",
    "lgbm_results;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_acc = 0\n",
    "# best_seed = None\n",
    "# best_model = None\n",
    "\n",
    "# # Coba berbagai random_state\n",
    "# for seed in range(1, 101):\n",
    "#     params = {\n",
    "#         'n_estimators': 233,\n",
    "#         'learning_rate': 0.019519792757748358,\n",
    "#         'num_leaves': 41,\n",
    "#         'max_depth': 15,\n",
    "#         'subsample': 0.758484089588373,\n",
    "#         'colsample_bytree': 0.9592852139230149,\n",
    "#         'random_state': seed\n",
    "#     }\n",
    "\n",
    "#     model = LGBMClassifier(**params, verbose=-1)\n",
    "#     model.fit(X_train, y_train)\n",
    "#     y_pred = model.predict(X_test)\n",
    "#     acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "#     if acc > best_acc:\n",
    "#         best_acc = acc\n",
    "#         best_seed = seed\n",
    "#         best_model = model\n",
    "    \n",
    "# print(f\"\\n🔍 Best Accuracy: {best_acc:.5f} with random_state = {best_seed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import optuna\n",
    "# from optuna.samplers import TPESampler\n",
    "# from lightgbm import LGBMClassifier\n",
    "# from sklearn.model_selection import cross_val_score, KFold\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# SEED = 104\n",
    "\n",
    "# def objective(trial):\n",
    "#     params = {\n",
    "#         'n_estimators': trial.suggest_int('n_estimators', 220, 250),\n",
    "#         'learning_rate': trial.suggest_float('learning_rate', 0.015, 0.025),\n",
    "#         'num_leaves': trial.suggest_int('num_leaves', 35, 50),\n",
    "#         'max_depth': trial.suggest_int('max_depth', 13, 16),\n",
    "#         'subsample': trial.suggest_float('subsample', 0.74, 0.78),\n",
    "#         'colsample_bytree': trial.suggest_float('colsample_bytree', 0.94, 0.97),\n",
    "#         'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 0.1),\n",
    "#         'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 0.1),\n",
    "#         'min_child_samples': trial.suggest_int('min_child_samples', 10, 40),\n",
    "#         'min_split_gain': trial.suggest_float('min_split_gain', 0.0, 0.1),\n",
    "#         'random_state': SEED\n",
    "#     }\n",
    "    \n",
    "#     model = LGBMClassifier(**params, n_jobs=-1, verbose=-1)\n",
    "#     kf = KFold(n_splits=3, shuffle=True, random_state=SEED)\n",
    "    \n",
    "#     score = cross_val_score(model, X_train, y_train, cv=kf, scoring='accuracy', n_jobs=-1)\n",
    "#     return score.mean()\n",
    "\n",
    "# study = optuna.create_study(direction='maximize', sampler=TPESampler(seed=SEED))\n",
    "# study.optimize(objective, n_trials=30, n_jobs=-1)\n",
    "\n",
    "# best_params = study.best_trial.params\n",
    "# print(\"Best Parameters:\", best_params)\n",
    "\n",
    "# best_lgbm = LGBMClassifier(**best_params, n_jobs=-1, verbose=-1)\n",
    "# best_lgbm.fit(X_train, y_train)\n",
    "\n",
    "# y_pred = best_lgbm.predict(X_test)\n",
    "# test_accuracy = accuracy_score(y_test, y_pred)\n",
    "# print(\"Test Accuracy after refined+regularized tuning:\", test_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# catb_model = CatBoostClassifier(verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# snb_model = BoostingMachineClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grb_model =  GradientBoostingClassifier(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hgrb_model =  HistGradientBoostingClassifier(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_models = [\n",
    "#     (\"lgbm\", lgbm_model),\n",
    "#     (\"catb\", catb_model),\n",
    "#     (\"grb\", grb_model),\n",
    "#     (\"snb\", snb_model),\n",
    "#     (\"xgb\", xgb_model)\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# meta_model = hgrb_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stacking_model = StackingClassifier(estimators=base_models,\n",
    "#                                     final_estimator=meta_model,\n",
    "#                                     cv=5,\n",
    "#                                     passthrough=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stacking_model.fit(X_train_transformed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = stacking_model.predict(X_test_transformed)\n",
    "\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# report = classification_report(y_test, y_pred)\n",
    "\n",
    "# print(\"Accuracy:\", accuracy)\n",
    "# print(\"\\nClassification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lgbm_results = evaluate_model(lgbm_model, X_train, y_train, X_test, y_test, \"LightGBM\")\n",
    "# lgbm_results;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgb_results = evaluate_model(xgb_model, X_train, y_train, X_test, y_test, \"XGBoost\")\n",
    "# xgb_results;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# catb_results = evaluate_model(catb_model, X_train, y_train, X_test, y_test, \"CatBoost\")\n",
    "# catb_results;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# snb_results = evaluate_model(snb_model, x_train_np, y_train, x_test_np, y_test, \"SnapBoost\")\n",
    "# snb_results;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grb_results = evaluate_model(grb_model, X_train, y_train, X_test, y_test, \"Gradient Boosting\")\n",
    "# grb_results;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hgrb_results = evaluate_model(hgrb_model, X_train, y_train, X_test, y_test, \"Hist Gradient Boosting\")\n",
    "# hgrb_results;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stacking_results = evaluate_model(stacking_model, x_train_np, y_train, x_test_np, y_test, \"Stacking Ensemble Classifier\")\n",
    "# stacking_results;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = [lgbm_results, xgb_results, catb_results, \n",
    "#         snb_results, grb_results, hgrb_results, stacking_results\n",
    "#         ]\n",
    "\n",
    "# create_summary_table(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_el",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
