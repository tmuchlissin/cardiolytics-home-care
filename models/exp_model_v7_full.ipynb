{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1743877636.037849   74810 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1743877636.041421   74810 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,roc_auc_score, classification_report, \n",
    "    confusion_matrix, ConfusionMatrixDisplay, roc_curve\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier, StackingClassifier, GradientBoostingClassifier, \n",
    "    HistGradientBoostingClassifier, AdaBoostClassifier\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "from interpret.glassbox import ExplainableBoostingClassifier \n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from ngboost import NGBClassifier\n",
    "from ngboost.distns import Bernoulli \n",
    "from snapml import BoostingMachineClassifier  \n",
    "\n",
    "from lib.utils import gcForest\n",
    "\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Load & Split Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Data 49818\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>systolic</th>\n",
       "      <th>diastolic</th>\n",
       "      <th>bmi</th>\n",
       "      <th>map</th>\n",
       "      <th>pulse_pressure</th>\n",
       "      <th>age_category</th>\n",
       "      <th>bmi_category</th>\n",
       "      <th>map_category</th>\n",
       "      <th>gender</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>gluc</th>\n",
       "      <th>smoke</th>\n",
       "      <th>alco</th>\n",
       "      <th>active</th>\n",
       "      <th>cardio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50</td>\n",
       "      <td>168</td>\n",
       "      <td>62.0</td>\n",
       "      <td>110</td>\n",
       "      <td>80</td>\n",
       "      <td>21.97</td>\n",
       "      <td>90.00</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>55</td>\n",
       "      <td>156</td>\n",
       "      <td>85.0</td>\n",
       "      <td>140</td>\n",
       "      <td>90</td>\n",
       "      <td>34.93</td>\n",
       "      <td>106.67</td>\n",
       "      <td>50</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>51</td>\n",
       "      <td>165</td>\n",
       "      <td>64.0</td>\n",
       "      <td>130</td>\n",
       "      <td>70</td>\n",
       "      <td>23.51</td>\n",
       "      <td>90.00</td>\n",
       "      <td>60</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>48</td>\n",
       "      <td>169</td>\n",
       "      <td>82.0</td>\n",
       "      <td>150</td>\n",
       "      <td>100</td>\n",
       "      <td>28.71</td>\n",
       "      <td>116.67</td>\n",
       "      <td>50</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60</td>\n",
       "      <td>151</td>\n",
       "      <td>67.0</td>\n",
       "      <td>120</td>\n",
       "      <td>80</td>\n",
       "      <td>29.38</td>\n",
       "      <td>93.33</td>\n",
       "      <td>40</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  height  weight  systolic  diastolic    bmi     map  pulse_pressure  \\\n",
       "0   50     168    62.0       110         80  21.97   90.00              30   \n",
       "1   55     156    85.0       140         90  34.93  106.67              50   \n",
       "2   51     165    64.0       130         70  23.51   90.00              60   \n",
       "3   48     169    82.0       150        100  28.71  116.67              50   \n",
       "4   60     151    67.0       120         80  29.38   93.33              40   \n",
       "\n",
       "   age_category  bmi_category  map_category  gender  cholesterol  gluc  smoke  \\\n",
       "0             2             1             1       1            0     0      0   \n",
       "1             2             3             3       0            2     0      0   \n",
       "2             2             1             1       0            2     0      0   \n",
       "3             2             2             3       1            0     0      0   \n",
       "4             3             2             2       0            1     1      0   \n",
       "\n",
       "   alco  active  cardio  \n",
       "0     0       1       0  \n",
       "1     0       1       1  \n",
       "2     0       0       1  \n",
       "3     0       1       1  \n",
       "4     0       0       0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_csv('data/preprocessed_data_full_encoded_new_v3.csv')\n",
    "\n",
    "cols_to_keep = ['age', \n",
    "                'height',\n",
    "                'weight',\n",
    "                'systolic', \n",
    "                'diastolic',\n",
    "                'bmi',\n",
    "                'map',\n",
    "                'pulse_pressure',\n",
    "                'age_category',\n",
    "                'bmi_category',\n",
    "                'map_category',\n",
    "                'gender',\n",
    "                'cholesterol', \n",
    "                'gluc',\n",
    "                'smoke', \n",
    "                'alco', \n",
    "                'active',\n",
    "                'cardio'\n",
    "                ]\n",
    "\n",
    "df = df[cols_to_keep]\n",
    "\n",
    "print('Sample Data', len(df)) \n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('cardio', axis=1)  \n",
    "y = df['cardio']   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapping = {0: 'Healthy', 1: 'Cardio Risk'}\n",
    "target_names = [label_mapping[label] for label in y.unique()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Scaling Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features = ['age', 'height', 'weight', 'systolic', 'diastolic', 'bmi', 'map', 'pulse_pressure']\n",
    "ordinal_features = ['cholesterol', 'gluc', 'age_category', 'bmi_category', 'map_category']\n",
    "binary_features = ['gender', 'smoke', 'alco', 'active']\n",
    "\n",
    "assert all(feature in X.columns for feature in numerical_features + ordinal_features + binary_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>systolic</th>\n",
       "      <th>diastolic</th>\n",
       "      <th>bmi</th>\n",
       "      <th>map</th>\n",
       "      <th>pulse_pressure</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>gluc</th>\n",
       "      <th>age_category</th>\n",
       "      <th>bmi_category</th>\n",
       "      <th>map_category</th>\n",
       "      <th>gender</th>\n",
       "      <th>smoke</th>\n",
       "      <th>alco</th>\n",
       "      <th>active</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.396496</td>\n",
       "      <td>0.497124</td>\n",
       "      <td>-0.936186</td>\n",
       "      <td>-1.077190</td>\n",
       "      <td>-0.16881</td>\n",
       "      <td>-1.155295</td>\n",
       "      <td>-0.655924</td>\n",
       "      <td>-1.389140</td>\n",
       "      <td>-0.575899</td>\n",
       "      <td>-0.421179</td>\n",
       "      <td>-0.362762</td>\n",
       "      <td>-1.024135</td>\n",
       "      <td>-0.880145</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.338776</td>\n",
       "      <td>-1.094188</td>\n",
       "      <td>0.908758</td>\n",
       "      <td>0.930288</td>\n",
       "      <td>1.09822</td>\n",
       "      <td>1.624188</td>\n",
       "      <td>1.092736</td>\n",
       "      <td>0.492485</td>\n",
       "      <td>2.230670</td>\n",
       "      <td>-0.421179</td>\n",
       "      <td>-0.362762</td>\n",
       "      <td>1.053030</td>\n",
       "      <td>0.977427</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.249442</td>\n",
       "      <td>0.099296</td>\n",
       "      <td>-0.775756</td>\n",
       "      <td>0.261129</td>\n",
       "      <td>-1.43584</td>\n",
       "      <td>-0.825017</td>\n",
       "      <td>-0.655924</td>\n",
       "      <td>1.433297</td>\n",
       "      <td>2.230670</td>\n",
       "      <td>-0.421179</td>\n",
       "      <td>-0.362762</td>\n",
       "      <td>-1.024135</td>\n",
       "      <td>-0.880145</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.690605</td>\n",
       "      <td>0.629733</td>\n",
       "      <td>0.668113</td>\n",
       "      <td>1.599448</td>\n",
       "      <td>2.36525</td>\n",
       "      <td>0.290208</td>\n",
       "      <td>2.141723</td>\n",
       "      <td>0.492485</td>\n",
       "      <td>-0.575899</td>\n",
       "      <td>-0.421179</td>\n",
       "      <td>-0.362762</td>\n",
       "      <td>0.014447</td>\n",
       "      <td>0.977427</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.074048</td>\n",
       "      <td>-1.757234</td>\n",
       "      <td>-0.535111</td>\n",
       "      <td>-0.408030</td>\n",
       "      <td>-0.16881</td>\n",
       "      <td>0.433900</td>\n",
       "      <td>-0.306612</td>\n",
       "      <td>-0.448328</td>\n",
       "      <td>0.827385</td>\n",
       "      <td>1.234620</td>\n",
       "      <td>1.967915</td>\n",
       "      <td>0.014447</td>\n",
       "      <td>0.048641</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49813</th>\n",
       "      <td>0.485830</td>\n",
       "      <td>-0.696360</td>\n",
       "      <td>-0.134036</td>\n",
       "      <td>0.261129</td>\n",
       "      <td>1.09822</td>\n",
       "      <td>0.240880</td>\n",
       "      <td>0.742375</td>\n",
       "      <td>-0.448328</td>\n",
       "      <td>0.827385</td>\n",
       "      <td>1.234620</td>\n",
       "      <td>-0.362762</td>\n",
       "      <td>0.014447</td>\n",
       "      <td>0.977427</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49814</th>\n",
       "      <td>-0.249442</td>\n",
       "      <td>-0.431141</td>\n",
       "      <td>-1.417476</td>\n",
       "      <td>2.937766</td>\n",
       "      <td>1.09822</td>\n",
       "      <td>-1.234648</td>\n",
       "      <td>2.141723</td>\n",
       "      <td>3.314921</td>\n",
       "      <td>-0.575899</td>\n",
       "      <td>-0.421179</td>\n",
       "      <td>-0.362762</td>\n",
       "      <td>-1.024135</td>\n",
       "      <td>0.977427</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49815</th>\n",
       "      <td>0.632885</td>\n",
       "      <td>0.099296</td>\n",
       "      <td>0.507683</td>\n",
       "      <td>1.599448</td>\n",
       "      <td>-0.16881</td>\n",
       "      <td>0.433900</td>\n",
       "      <td>0.742375</td>\n",
       "      <td>2.374109</td>\n",
       "      <td>-0.575899</td>\n",
       "      <td>-0.421179</td>\n",
       "      <td>-0.362762</td>\n",
       "      <td>0.014447</td>\n",
       "      <td>0.977427</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49816</th>\n",
       "      <td>1.221102</td>\n",
       "      <td>-0.165923</td>\n",
       "      <td>-0.134036</td>\n",
       "      <td>0.595709</td>\n",
       "      <td>-0.16881</td>\n",
       "      <td>-0.055083</td>\n",
       "      <td>0.217882</td>\n",
       "      <td>0.962891</td>\n",
       "      <td>-0.575899</td>\n",
       "      <td>1.234620</td>\n",
       "      <td>1.967915</td>\n",
       "      <td>0.014447</td>\n",
       "      <td>0.977427</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49817</th>\n",
       "      <td>0.485830</td>\n",
       "      <td>0.762342</td>\n",
       "      <td>-0.134036</td>\n",
       "      <td>-0.408030</td>\n",
       "      <td>-0.16881</td>\n",
       "      <td>-0.524764</td>\n",
       "      <td>-0.306612</td>\n",
       "      <td>-0.448328</td>\n",
       "      <td>0.827385</td>\n",
       "      <td>-0.421179</td>\n",
       "      <td>-0.362762</td>\n",
       "      <td>-1.024135</td>\n",
       "      <td>0.048641</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>49818 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            age    height    weight  systolic  diastolic       bmi       map  \\\n",
       "0     -0.396496  0.497124 -0.936186 -1.077190   -0.16881 -1.155295 -0.655924   \n",
       "1      0.338776 -1.094188  0.908758  0.930288    1.09822  1.624188  1.092736   \n",
       "2     -0.249442  0.099296 -0.775756  0.261129   -1.43584 -0.825017 -0.655924   \n",
       "3     -0.690605  0.629733  0.668113  1.599448    2.36525  0.290208  2.141723   \n",
       "4      1.074048 -1.757234 -0.535111 -0.408030   -0.16881  0.433900 -0.306612   \n",
       "...         ...       ...       ...       ...        ...       ...       ...   \n",
       "49813  0.485830 -0.696360 -0.134036  0.261129    1.09822  0.240880  0.742375   \n",
       "49814 -0.249442 -0.431141 -1.417476  2.937766    1.09822 -1.234648  2.141723   \n",
       "49815  0.632885  0.099296  0.507683  1.599448   -0.16881  0.433900  0.742375   \n",
       "49816  1.221102 -0.165923 -0.134036  0.595709   -0.16881 -0.055083  0.217882   \n",
       "49817  0.485830  0.762342 -0.134036 -0.408030   -0.16881 -0.524764 -0.306612   \n",
       "\n",
       "       pulse_pressure  cholesterol      gluc  age_category  bmi_category  \\\n",
       "0           -1.389140    -0.575899 -0.421179     -0.362762     -1.024135   \n",
       "1            0.492485     2.230670 -0.421179     -0.362762      1.053030   \n",
       "2            1.433297     2.230670 -0.421179     -0.362762     -1.024135   \n",
       "3            0.492485    -0.575899 -0.421179     -0.362762      0.014447   \n",
       "4           -0.448328     0.827385  1.234620      1.967915      0.014447   \n",
       "...               ...          ...       ...           ...           ...   \n",
       "49813       -0.448328     0.827385  1.234620     -0.362762      0.014447   \n",
       "49814        3.314921    -0.575899 -0.421179     -0.362762     -1.024135   \n",
       "49815        2.374109    -0.575899 -0.421179     -0.362762      0.014447   \n",
       "49816        0.962891    -0.575899  1.234620      1.967915      0.014447   \n",
       "49817       -0.448328     0.827385 -0.421179     -0.362762     -1.024135   \n",
       "\n",
       "       map_category  gender  smoke  alco  active  \n",
       "0         -0.880145     1.0    0.0   0.0     1.0  \n",
       "1          0.977427     0.0    0.0   0.0     1.0  \n",
       "2         -0.880145     0.0    0.0   0.0     0.0  \n",
       "3          0.977427     1.0    0.0   0.0     1.0  \n",
       "4          0.048641     0.0    0.0   0.0     0.0  \n",
       "...             ...     ...    ...   ...     ...  \n",
       "49813      0.977427     0.0    0.0   0.0     1.0  \n",
       "49814      0.977427     0.0    0.0   0.0     1.0  \n",
       "49815      0.977427     0.0    0.0   0.0     1.0  \n",
       "49816      0.977427     0.0    0.0   0.0     0.0  \n",
       "49817      0.048641     0.0    0.0   0.0     1.0  \n",
       "\n",
       "[49818 rows x 17 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler_standard = StandardScaler()\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num_scaler', scaler_standard, numerical_features),\n",
    "         ('ord_scaler', scaler_standard, ordinal_features)\n",
    "    ],\n",
    "    remainder='passthrough'  \n",
    ")\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor)\n",
    "])\n",
    "\n",
    "X_preprocessed = pipeline.fit_transform(X)\n",
    "\n",
    "X_preprocessed = pd.DataFrame(X_preprocessed, columns=numerical_features + ordinal_features + binary_features)\n",
    "X_preprocessed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Train, Val, Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_temp, y_train, y_temp = train_test_split(\n",
    "    X_preprocessed, y, test_size=0.3, random_state=42, stratify=y\n",
    ")  # Train 80%\n",
    "\n",
    "x_val, x_test, y_val, y_test = train_test_split(\n",
    "    x_temp, y_temp, test_size=(1/3), random_state=42\n",
    ")  # Val 10%, Test 10%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_np = np.array(x_train)\n",
    "y_train_np = np.array(y_train)\n",
    "x_test_np = np.array(x_test)\n",
    "y_test_np = np.array(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Base Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, x_train, y_train, x_test, y_test, model_name):\n",
    "    \n",
    "    if hasattr(model, 'cascade_forest'):\n",
    "        print(\"Using cascade_forest branch for training and prediction...\")\n",
    "        model.cascade_forest(x_train, y_train)\n",
    "        predict_func = lambda x: np.argmax(np.mean(model.cascade_forest(x), axis=0), axis=1)\n",
    "        predict_proba_func = lambda x: np.mean(model.cascade_forest(x), axis=0)\n",
    "    else:\n",
    "        print(\"Using standard branch (fit/predict/predict_proba)...\")\n",
    "        model.fit(x_train, y_train)\n",
    "        predict_func = lambda x: model.predict(x)\n",
    "        predict_proba_func = lambda x: model.predict_proba(x)\n",
    "\n",
    "    y_pred_test = predict_func(x_test)\n",
    "    y_proba_test = predict_proba_func(x_test)\n",
    "    \n",
    "    if y_proba_test.shape[1] > 1:\n",
    "        y_probs_test = y_proba_test[:, 1]\n",
    "    else:\n",
    "        y_probs_test = y_proba_test[:, 0]\n",
    "    \n",
    "    y_pred_train = predict_func(x_train)\n",
    "    y_proba_train = predict_proba_func(x_train)\n",
    "    if y_proba_train.shape[1] > 1:\n",
    "        y_probs_train = y_proba_train[:, 1]\n",
    "    else:\n",
    "        y_probs_train = y_proba_train[:, 0]\n",
    "    \n",
    "    test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "    test_acc_str = f\"{(test_accuracy * 100):.2f}%\"\n",
    "    test_auc = roc_auc_score(y_test, y_probs_test)\n",
    "    test_auc_str = f\"{test_auc:.4f}\"\n",
    "    test_report_dict = classification_report(y_test, y_pred_test, output_dict=True)\n",
    "    test_precision = test_report_dict['weighted avg']['precision'] \n",
    "    test_recall    = test_report_dict['weighted avg']['recall']   \n",
    "    test_f1        = test_report_dict['weighted avg']['f1-score']\n",
    "    \n",
    "    train_accuracy = accuracy_score(y_train, y_pred_train)\n",
    "    train_acc_str = f\"{(train_accuracy * 100):.2f}%\"\n",
    "    train_auc = roc_auc_score(y_train, y_probs_train)\n",
    "    train_auc_str = f\"{train_auc:.4f}\"\n",
    "    train_report_dict = classification_report(y_train, y_pred_train, output_dict=True)\n",
    "    train_precision = train_report_dict['weighted avg']['precision']\n",
    "    train_recall    = train_report_dict['weighted avg']['recall']    \n",
    "    train_f1        = train_report_dict['weighted avg']['f1-score']  \n",
    "\n",
    "    data = [\n",
    "        [\"Test\", test_acc_str, test_auc_str],\n",
    "        [\"Train\", train_acc_str, train_auc_str]\n",
    "    ]\n",
    "\n",
    "    headers = [\"\", \"Accuracy\", \"AUC Score\"]\n",
    "\n",
    "    print(f\"\\n=== {model_name} ===\\n\")\n",
    "    print(tabulate(data, headers=headers, tablefmt=\"grid\"))\n",
    "    \n",
    "    print(\"\\nOverfitting Check :\")\n",
    "    if train_accuracy > test_accuracy + 5 or train_auc > test_auc + 0.05:\n",
    "        print(\"The model might be overfitting.\")\n",
    "    else:\n",
    "        print(\"No significant signs of overfitting.\\n\")\n",
    "    \n",
    "    # # Plot Confusion Matrix and ROC Curve\n",
    "    # fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    # cm = confusion_matrix(y_test, y_pred_test)\n",
    "    # # If a global variable 'label_mapping' exists, use it for display labels\n",
    "    # display_labels = list(label_mapping.values()) if 'label_mapping' in globals() else None\n",
    "    # disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=display_labels)\n",
    "    # disp.plot(ax=axes[0], cmap='viridis', colorbar=False)\n",
    "    # axes[0].set_title(f\"{model_name} - Confusion Matrix\")\n",
    "    \n",
    "    # fpr, tpr, _ = roc_curve(y_test, y_probs_test)\n",
    "    # axes[1].plot(fpr, tpr, label=f\"ROC Curve (AUC = {test_auc:.4f})\", linewidth=2)\n",
    "    # axes[1].plot([0, 1], [0, 1], 'k--', label=\"Random Guess\", linewidth=1)\n",
    "    # axes[1].set_title(f\"{model_name} - ROC Curve\")\n",
    "    # axes[1].legend(loc=\"lower right\")\n",
    "    # axes[1].grid(alpha=0.3)\n",
    "    \n",
    "    # plt.tight_layout()\n",
    "    # plt.show()\n",
    "    \n",
    "    return {\n",
    "        'model_name': model_name,\n",
    "        'train_accuracy': train_accuracy,\n",
    "        'train_precision': train_precision,\n",
    "        'train_recall': train_recall,\n",
    "        'train_f1': train_f1,\n",
    "        'test_accuracy': test_accuracy,\n",
    "        'test_precision': test_precision,\n",
    "        'test_recall': test_recall,\n",
    "        'test_f1': test_f1\n",
    "    }\n",
    "\n",
    "\n",
    "def create_summary_table(results):\n",
    "    test_summary = pd.DataFrame([{\n",
    "        'Algorithm': r['model_name'],\n",
    "        'Accuracy':  round(r['test_accuracy'], 4),\n",
    "        'Precision': round(r['test_precision'], 4),\n",
    "        'Recall':    round(r['test_recall'], 4),\n",
    "        'F1-Score':  round(r['test_f1'], 4)\n",
    "    } for r in results])\n",
    "    train_summary = pd.DataFrame([{\n",
    "        'Algorithm': r['model_name'],\n",
    "        'Accuracy':  round(r['train_accuracy'], 4),\n",
    "        'Precision': round(r['train_precision'], 4),\n",
    "        'Recall':    round(r['train_recall'], 4),\n",
    "        'F1-Score':  round(r['train_f1'], 4)\n",
    "    } for r in results])\n",
    "    \n",
    "    \n",
    "    print(\"\\nSummary Table - Test Metrics\")\n",
    "    print(tabulate(test_summary, headers='keys', tablefmt='grid', showindex=False))\n",
    "\n",
    "    print(\"Summary Table - Training Metrics\")\n",
    "    print(tabulate(train_summary, headers='keys', tablefmt='grid', showindex=False))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `Logistic Regression`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using standard branch (fit/predict/predict_proba)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Logistic Regression ===\n",
      "\n",
      "+-------+------------+-------------+\n",
      "|       | Accuracy   |   AUC Score |\n",
      "+=======+============+=============+\n",
      "| Test  | 84.24%     |      0.9033 |\n",
      "+-------+------------+-------------+\n",
      "| Train | 84.37%     |      0.9061 |\n",
      "+-------+------------+-------------+\n",
      "\n",
      "Overfitting Check :\n",
      "No significant signs of overfitting.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logreg_model = LogisticRegression()\n",
    "logreg_results = evaluate_model(logreg_model, x_train_np, y_train_np, x_test_np, y_test_np, \"Logistic Regression\")\n",
    "logreg_results;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `Random Forest`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using standard branch (fit/predict/predict_proba)...\n",
      "\n",
      "=== Random Forest ===\n",
      "\n",
      "+-------+------------+-------------+\n",
      "|       | Accuracy   |   AUC Score |\n",
      "+=======+============+=============+\n",
      "| Test  | 85.31%     |      0.942  |\n",
      "+-------+------------+-------------+\n",
      "| Train | 99.56%     |      0.9999 |\n",
      "+-------+------------+-------------+\n",
      "\n",
      "Overfitting Check :\n",
      "The model might be overfitting.\n"
     ]
    }
   ],
   "source": [
    "rf_model = RandomForestClassifier()\n",
    "rf_results = evaluate_model(rf_model, x_train_np, y_train_np, x_test_np, y_test_np, \"Random Forest\")\n",
    "rf_results;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `Decision Tree`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using standard branch (fit/predict/predict_proba)...\n",
      "\n",
      "=== Decision Tree ===\n",
      "\n",
      "+-------+------------+-------------+\n",
      "|       | Accuracy   |   AUC Score |\n",
      "+=======+============+=============+\n",
      "| Test  | 83.26%     |      0.8341 |\n",
      "+-------+------------+-------------+\n",
      "| Train | 99.56%     |      1      |\n",
      "+-------+------------+-------------+\n",
      "\n",
      "Overfitting Check :\n",
      "The model might be overfitting.\n"
     ]
    }
   ],
   "source": [
    "dt_model = DecisionTreeClassifier()\n",
    "dt_results = evaluate_model(dt_model, x_train_np, y_train_np, x_test_np, y_test_np, \"Decision Tree\")\n",
    "dt_results;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `SVM`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using standard branch (fit/predict/predict_proba)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SVM ===\n",
      "\n",
      "+-------+------------+-------------+\n",
      "|       | Accuracy   |   AUC Score |\n",
      "+=======+============+=============+\n",
      "| Test  | 85.87%     |      0.9347 |\n",
      "+-------+------------+-------------+\n",
      "| Train | 86.40%     |      0.9417 |\n",
      "+-------+------------+-------------+\n",
      "\n",
      "Overfitting Check :\n",
      "No significant signs of overfitting.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svm_model = SVC(probability=True)\n",
    "svm_results = evaluate_model(svm_model, x_train_np, y_train_np, x_test_np, y_test_np, \"SVM\")\n",
    "svm_results;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `Naive Bayes`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using standard branch (fit/predict/predict_proba)...\n",
      "\n",
      "=== Naive Bayes ===\n",
      "\n",
      "+-------+------------+-------------+\n",
      "|       | Accuracy   |   AUC Score |\n",
      "+=======+============+=============+\n",
      "| Test  | 84.52%     |      0.8998 |\n",
      "+-------+------------+-------------+\n",
      "| Train | 84.28%     |      0.9031 |\n",
      "+-------+------------+-------------+\n",
      "\n",
      "Overfitting Check :\n",
      "No significant signs of overfitting.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nb_model = GaussianNB()\n",
    "nb_results = evaluate_model(nb_model, x_train_np, y_train_np, x_test_np, y_test_np, \"Naive Bayes\")\n",
    "nb_results;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `KNN`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using standard branch (fit/predict/predict_proba)...\n",
      "\n",
      "=== KNN ===\n",
      "\n",
      "+-------+------------+-------------+\n",
      "|       | Accuracy   |   AUC Score |\n",
      "+=======+============+=============+\n",
      "| Test  | 84.42%     |      0.9225 |\n",
      "+-------+------------+-------------+\n",
      "| Train | 89.37%     |      0.9686 |\n",
      "+-------+------------+-------------+\n",
      "\n",
      "Overfitting Check :\n",
      "No significant signs of overfitting.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "knn_model = KNeighborsClassifier()\n",
    "knn_results = evaluate_model(knn_model, x_train_np, y_train_np, x_test_np, y_test_np, \"KNN\")\n",
    "knn_results;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `XGBoost`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using standard branch (fit/predict/predict_proba)...\n",
      "\n",
      "=== XGBoost ===\n",
      "\n",
      "+-------+------------+-------------+\n",
      "|       | Accuracy   |   AUC Score |\n",
      "+=======+============+=============+\n",
      "| Test  | 85.77%     |      0.9468 |\n",
      "+-------+------------+-------------+\n",
      "| Train | 90.34%     |      0.9741 |\n",
      "+-------+------------+-------------+\n",
      "\n",
      "Overfitting Check :\n",
      "No significant signs of overfitting.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "xgb_results = evaluate_model(xgb_model, x_train_np, y_train_np, x_test_np, y_test_np, \"XGBoost\")\n",
    "xgb_results;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `Light GBM`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using standard branch (fit/predict/predict_proba)...\n",
      "\n",
      "=== LightGBM ===\n",
      "\n",
      "+-------+------------+-------------+\n",
      "|       | Accuracy   |   AUC Score |\n",
      "+=======+============+=============+\n",
      "| Test  | 86.51%     |      0.9495 |\n",
      "+-------+------------+-------------+\n",
      "| Train | 88.04%     |      0.9618 |\n",
      "+-------+------------+-------------+\n",
      "\n",
      "Overfitting Check :\n",
      "No significant signs of overfitting.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lgbm_model = LGBMClassifier(verbose=-1)\n",
    "lgbm_results = evaluate_model(lgbm_model, x_train_np, y_train_np, x_test_np, y_test_np, \"LightGBM\")\n",
    "lgbm_results;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `Cat Boost`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using standard branch (fit/predict/predict_proba)...\n",
      "\n",
      "=== CatBoost ===\n",
      "\n",
      "+-------+------------+-------------+\n",
      "|       | Accuracy   |   AUC Score |\n",
      "+=======+============+=============+\n",
      "| Test  | 86.41%     |      0.9474 |\n",
      "+-------+------------+-------------+\n",
      "| Train | 89.24%     |      0.9682 |\n",
      "+-------+------------+-------------+\n",
      "\n",
      "Overfitting Check :\n",
      "No significant signs of overfitting.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "catb_model = CatBoostClassifier(verbose=False)\n",
    "catb_results = evaluate_model(catb_model, x_train_np, y_train_np, x_test_np, y_test_np, \"CatBoost\")\n",
    "catb_results;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `SnapBoost`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using standard branch (fit/predict/predict_proba)...\n",
      "\n",
      "=== SnapBoost ===\n",
      "\n",
      "+-------+------------+-------------+\n",
      "|       | Accuracy   |   AUC Score |\n",
      "+=======+============+=============+\n",
      "| Test  | 85.39%     |      0.9423 |\n",
      "+-------+------------+-------------+\n",
      "| Train | 86.32%     |      0.9486 |\n",
      "+-------+------------+-------------+\n",
      "\n",
      "Overfitting Check :\n",
      "No significant signs of overfitting.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "snb_model = BoostingMachineClassifier()\n",
    "snb_results = evaluate_model(snb_model, x_train_np, y_train_np, x_test_np, y_test_np, \"SnapBoost\")\n",
    "snb_results;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `Explainable Boosting Machine (EBM)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using standard branch (fit/predict/predict_proba)...\n",
      "\n",
      "=== EBM ===\n",
      "\n",
      "+-------+------------+-------------+\n",
      "|       | Accuracy   |   AUC Score |\n",
      "+=======+============+=============+\n",
      "| Test  | 85.01%     |      0.9324 |\n",
      "+-------+------------+-------------+\n",
      "| Train | 85.74%     |      0.9385 |\n",
      "+-------+------------+-------------+\n",
      "\n",
      "Overfitting Check :\n",
      "No significant signs of overfitting.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ebm_model = ExplainableBoostingClassifier(n_jobs=1)\n",
    "ebm_results = evaluate_model(ebm_model, x_train_np, y_train_np, x_test_np, y_test_np, \"EBM\")\n",
    "ebm_results;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `NGBoost`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using standard branch (fit/predict/predict_proba)...\n",
      "\n",
      "=== NGBoost ===\n",
      "\n",
      "+-------+------------+-------------+\n",
      "|       | Accuracy   |   AUC Score |\n",
      "+=======+============+=============+\n",
      "| Test  | 85.03%     |      0.9324 |\n",
      "+-------+------------+-------------+\n",
      "| Train | 84.85%     |      0.9357 |\n",
      "+-------+------------+-------------+\n",
      "\n",
      "Overfitting Check :\n",
      "No significant signs of overfitting.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ngb_model = NGBClassifier(Dist=Bernoulli, verbose=False)\n",
    "ngb_results = evaluate_model(ngb_model, x_train_np, y_train_np, x_test_np, y_test_np, \"NGBoost\")\n",
    "ngb_results;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `AdaBoost`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using standard branch (fit/predict/predict_proba)...\n",
      "\n",
      "=== AdaBoost ===\n",
      "\n",
      "+-------+------------+-------------+\n",
      "|       | Accuracy   |   AUC Score |\n",
      "+=======+============+=============+\n",
      "| Test  | 84.16%     |      0.9118 |\n",
      "+-------+------------+-------------+\n",
      "| Train | 84.39%     |      0.9148 |\n",
      "+-------+------------+-------------+\n",
      "\n",
      "Overfitting Check :\n",
      "No significant signs of overfitting.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "adb_model = AdaBoostClassifier(random_state=42)\n",
    "adb_results = evaluate_model(adb_model, x_train_np, y_train_np, x_test_np, y_test_np, \"AdaBoost\")\n",
    "adb_results;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `GradientBoosting`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using standard branch (fit/predict/predict_proba)...\n",
      "\n",
      "=== Gradient Boosting ===\n",
      "\n",
      "+-------+------------+-------------+\n",
      "|       | Accuracy   |   AUC Score |\n",
      "+=======+============+=============+\n",
      "| Test  | 84.99%     |      0.9389 |\n",
      "+-------+------------+-------------+\n",
      "| Train | 85.98%     |      0.9433 |\n",
      "+-------+------------+-------------+\n",
      "\n",
      "Overfitting Check :\n",
      "No significant signs of overfitting.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grb_model =  GradientBoostingClassifier(random_state=42)\n",
    "grb_results = evaluate_model(grb_model, x_train_np, y_train_np, x_test_np, y_test_np, \"Gradient Boosting\")\n",
    "grb_results;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `Hist GradientBoosting`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using standard branch (fit/predict/predict_proba)...\n",
      "\n",
      "=== Hist Gradient Boosting ===\n",
      "\n",
      "+-------+------------+-------------+\n",
      "|       | Accuracy   |   AUC Score |\n",
      "+=======+============+=============+\n",
      "| Test  | 86.43%     |      0.9498 |\n",
      "+-------+------------+-------------+\n",
      "| Train | 87.43%     |      0.9574 |\n",
      "+-------+------------+-------------+\n",
      "\n",
      "Overfitting Check :\n",
      "No significant signs of overfitting.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hgrb_model =  HistGradientBoostingClassifier(random_state=42)\n",
    "hgrb_results = evaluate_model(hgrb_model, x_train_np, y_train_np, x_test_np, y_test_np, \"Hist Gradient Boosting\")\n",
    "hgrb_results;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Cascaded Random Forest`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cascade_forest branch for training and prediction...\n",
      "Adding/Training Layer, n_layer=1\n",
      "Layer validation accuracy = 0.8445878136200717\n",
      "Adding/Training Layer, n_layer=2\n",
      "Layer validation accuracy = 0.8484587813620071\n",
      "Adding/Training Layer, n_layer=3\n",
      "Layer validation accuracy = 0.8497491039426524\n",
      "Adding/Training Layer, n_layer=4\n",
      "Layer validation accuracy = 0.8513261648745519\n",
      "Adding/Training Layer, n_layer=5\n",
      "Layer validation accuracy = 0.8530465949820788\n",
      "Adding/Training Layer, n_layer=6\n",
      "Layer validation accuracy = 0.8541935483870968\n",
      "Adding/Training Layer, n_layer=7\n",
      "Layer validation accuracy = 0.8536200716845879\n",
      "\n",
      "=== Cascaded Random Forest ===\n",
      "\n",
      "+-------+------------+-------------+\n",
      "|       | Accuracy   |   AUC Score |\n",
      "+=======+============+=============+\n",
      "| Test  | 85.71%     |      0.9422 |\n",
      "+-------+------------+-------------+\n",
      "| Train | 85.97%     |      0.9458 |\n",
      "+-------+------------+-------------+\n",
      "\n",
      "Overfitting Check :\n",
      "No significant signs of overfitting.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gcf_model = gcForest(n_cascadeRF=2,n_cascadeRFtree=500) #Default values tolerance =0.0 ,n_cascadeRFtree=101\n",
    "gcf_results = evaluate_model(gcf_model, x_train_np, y_train_np, x_test_np, y_test_np, \"Cascaded Random Forest\")\n",
    "gcf_results;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `TabNet`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using standard branch (fit/predict/predict_proba)...\n",
      "\n",
      "=== TabNet Classifier ===\n",
      "\n",
      "+-------+------------+-------------+\n",
      "|       | Accuracy   |   AUC Score |\n",
      "+=======+============+=============+\n",
      "| Test  | 86.65%     |      0.9506 |\n",
      "+-------+------------+-------------+\n",
      "| Train | 86.86%     |      0.953  |\n",
      "+-------+------------+-------------+\n",
      "\n",
      "Overfitting Check :\n",
      "No significant signs of overfitting.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tabnet_model = TabNetClassifier(verbose=0)\n",
    "tabnet_results = evaluate_model(tabnet_model, x_train_np, y_train_np, x_test_np, y_test_np, \"TabNet Classifier\")\n",
    "tabnet_results;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `NN`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KerasModelWrapper(BaseEstimator):\n",
    "    def __init__(self, model, epochs=100, batch_size=32, validation_split=0.2, callbacks=None):\n",
    "        self.model = model\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.validation_split = validation_split\n",
    "        self.callbacks = callbacks\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        self.model.fit(\n",
    "            x, y,\n",
    "            epochs=self.epochs,\n",
    "            batch_size=self.batch_size,\n",
    "            validation_split=self.validation_split,\n",
    "            callbacks=self.callbacks,\n",
    "            verbose=0\n",
    "        )\n",
    "        return self\n",
    "\n",
    "    def predict(self, x):\n",
    "        proba = self.model.predict(x)\n",
    "        return (proba > 0.5).astype(int)\n",
    "\n",
    "    def predict_proba(self, x):\n",
    "        proba = self.model.predict(x)\n",
    "        return np.hstack([1 - proba, proba])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using standard branch (fit/predict/predict_proba)...\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 739us/step\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 511us/step\n",
      "\u001b[1m1090/1090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 450us/step\n",
      "\u001b[1m1090/1090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 449us/step\n",
      "\n",
      "=== Keras Sequential Model ===\n",
      "\n",
      "+-------+------------+-------------+\n",
      "|       | Accuracy   |   AUC Score |\n",
      "+=======+============+=============+\n",
      "| Test  | 86.37%     |      0.9485 |\n",
      "+-------+------------+-------------+\n",
      "| Train | 86.73%     |      0.9528 |\n",
      "+-------+------------+-------------+\n",
      "\n",
      "Overfitting Check :\n",
      "No significant signs of overfitting.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Dense(32, input_dim=x_train_np.shape[1], activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(1, activation='sigmoid')  \n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "wrapped_model = KerasModelWrapper(\n",
    "    model=model,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stop]\n",
    ")\n",
    "\n",
    "nn_results = evaluate_model(\n",
    "    wrapped_model, \n",
    "    x_train_np, y_train_np,\n",
    "    x_test_np, y_test_np,\n",
    "    model_name='Keras Sequential Model'\n",
    ")\n",
    "\n",
    "nn_results;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Summary Table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary Table - Test Metrics\n",
      "+------------------------+------------+-------------+----------+------------+\n",
      "| Algorithm              |   Accuracy |   Precision |   Recall |   F1-Score |\n",
      "+========================+============+=============+==========+============+\n",
      "| Logistic Regression    |     0.8424 |      0.8444 |   0.8424 |     0.8421 |\n",
      "+------------------------+------------+-------------+----------+------------+\n",
      "| Random Forest          |     0.8531 |      0.8532 |   0.8531 |     0.853  |\n",
      "+------------------------+------------+-------------+----------+------------+\n",
      "| Decision Tree          |     0.8326 |      0.8326 |   0.8326 |     0.8326 |\n",
      "+------------------------+------------+-------------+----------+------------+\n",
      "| SVM                    |     0.8587 |      0.8622 |   0.8587 |     0.8582 |\n",
      "+------------------------+------------+-------------+----------+------------+\n",
      "| Naive Bayes            |     0.8452 |      0.8502 |   0.8452 |     0.8445 |\n",
      "+------------------------+------------+-------------+----------+------------+\n",
      "| KNN                    |     0.8442 |      0.8445 |   0.8442 |     0.8442 |\n",
      "+------------------------+------------+-------------+----------+------------+\n",
      "| XGBoost                |     0.8577 |      0.8585 |   0.8577 |     0.8575 |\n",
      "+------------------------+------------+-------------+----------+------------+\n",
      "| LightGBM               |     0.8651 |      0.8665 |   0.8651 |     0.8649 |\n",
      "+------------------------+------------+-------------+----------+------------+\n",
      "| CatBoost               |     0.8641 |      0.8652 |   0.8641 |     0.8639 |\n",
      "+------------------------+------------+-------------+----------+------------+\n",
      "| SnapBoost              |     0.8539 |      0.8557 |   0.8539 |     0.8536 |\n",
      "+------------------------+------------+-------------+----------+------------+\n",
      "| EBM                    |     0.8501 |      0.8526 |   0.8501 |     0.8496 |\n",
      "+------------------------+------------+-------------+----------+------------+\n",
      "| NGBoost                |     0.8503 |      0.8557 |   0.8503 |     0.8495 |\n",
      "+------------------------+------------+-------------+----------+------------+\n",
      "| AdaBoost               |     0.8416 |      0.8486 |   0.8416 |     0.8406 |\n",
      "+------------------------+------------+-------------+----------+------------+\n",
      "| Gradient Boosting      |     0.8499 |      0.8525 |   0.8499 |     0.8494 |\n",
      "+------------------------+------------+-------------+----------+------------+\n",
      "| Hist Gradient Boosting |     0.8643 |      0.8657 |   0.8643 |     0.8641 |\n",
      "+------------------------+------------+-------------+----------+------------+\n",
      "| Cascaded Random Forest |     0.8571 |      0.8583 |   0.8571 |     0.8569 |\n",
      "+------------------------+------------+-------------+----------+------------+\n",
      "| TabNet Classifier      |     0.8665 |      0.8692 |   0.8665 |     0.8662 |\n",
      "+------------------------+------------+-------------+----------+------------+\n",
      "| Keras Sequential Model |     0.8637 |      0.8659 |   0.8637 |     0.8634 |\n",
      "+------------------------+------------+-------------+----------+------------+\n",
      "Summary Table - Training Metrics\n",
      "+------------------------+------------+-------------+----------+------------+\n",
      "| Algorithm              |   Accuracy |   Precision |   Recall |   F1-Score |\n",
      "+========================+============+=============+==========+============+\n",
      "| Logistic Regression    |     0.8437 |      0.8459 |   0.8437 |     0.8434 |\n",
      "+------------------------+------------+-------------+----------+------------+\n",
      "| Random Forest          |     0.9956 |      0.9956 |   0.9956 |     0.9956 |\n",
      "+------------------------+------------+-------------+----------+------------+\n",
      "| Decision Tree          |     0.9956 |      0.9956 |   0.9956 |     0.9956 |\n",
      "+------------------------+------------+-------------+----------+------------+\n",
      "| SVM                    |     0.864  |      0.8681 |   0.864  |     0.8636 |\n",
      "+------------------------+------------+-------------+----------+------------+\n",
      "| Naive Bayes            |     0.8428 |      0.848  |   0.8428 |     0.8422 |\n",
      "+------------------------+------------+-------------+----------+------------+\n",
      "| KNN                    |     0.8937 |      0.894  |   0.8937 |     0.8937 |\n",
      "+------------------------+------------+-------------+----------+------------+\n",
      "| XGBoost                |     0.9034 |      0.9049 |   0.9034 |     0.9033 |\n",
      "+------------------------+------------+-------------+----------+------------+\n",
      "| LightGBM               |     0.8804 |      0.882  |   0.8804 |     0.8802 |\n",
      "+------------------------+------------+-------------+----------+------------+\n",
      "| CatBoost               |     0.8924 |      0.894  |   0.8924 |     0.8923 |\n",
      "+------------------------+------------+-------------+----------+------------+\n",
      "| SnapBoost              |     0.8632 |      0.8654 |   0.8632 |     0.8629 |\n",
      "+------------------------+------------+-------------+----------+------------+\n",
      "| EBM                    |     0.8574 |      0.8601 |   0.8574 |     0.8571 |\n",
      "+------------------------+------------+-------------+----------+------------+\n",
      "| NGBoost                |     0.8485 |      0.8542 |   0.8485 |     0.8478 |\n",
      "+------------------------+------------+-------------+----------+------------+\n",
      "| AdaBoost               |     0.8439 |      0.8518 |   0.8439 |     0.8429 |\n",
      "+------------------------+------------+-------------+----------+------------+\n",
      "| Gradient Boosting      |     0.8598 |      0.8626 |   0.8598 |     0.8595 |\n",
      "+------------------------+------------+-------------+----------+------------+\n",
      "| Hist Gradient Boosting |     0.8743 |      0.8759 |   0.8743 |     0.8741 |\n",
      "+------------------------+------------+-------------+----------+------------+\n",
      "| Cascaded Random Forest |     0.8597 |      0.8611 |   0.8597 |     0.8595 |\n",
      "+------------------------+------------+-------------+----------+------------+\n",
      "| TabNet Classifier      |     0.8686 |      0.8718 |   0.8686 |     0.8682 |\n",
      "+------------------------+------------+-------------+----------+------------+\n",
      "| Keras Sequential Model |     0.8673 |      0.87   |   0.8673 |     0.867  |\n",
      "+------------------------+------------+-------------+----------+------------+\n"
     ]
    }
   ],
   "source": [
    "results = [logreg_results, rf_results, dt_results, \n",
    "        svm_results, nb_results, knn_results, \n",
    "        xgb_results, lgbm_results, catb_results, \n",
    "        snb_results, ebm_results, ngb_results, \n",
    "        adb_results, grb_results, hgrb_results, \n",
    "        gcf_results, tabnet_results, nn_results]\n",
    "\n",
    "create_summary_table(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_el",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
