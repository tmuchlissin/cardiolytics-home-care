{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1743911456.333071   14305 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1743911456.336926   14305 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,roc_auc_score, classification_report, \n",
    "    confusion_matrix, ConfusionMatrixDisplay, roc_curve\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier, StackingClassifier, GradientBoostingClassifier, \n",
    "    HistGradientBoostingClassifier, AdaBoostClassifier\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "from interpret.glassbox import ExplainableBoostingClassifier \n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from ngboost import NGBClassifier\n",
    "from ngboost.distns import Bernoulli \n",
    "from snapml import BoostingMachineClassifier  \n",
    "\n",
    "from lib.utils import gcForest\n",
    "\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Load & Split Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Data 49818\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>systolic</th>\n",
       "      <th>diastolic</th>\n",
       "      <th>bmi</th>\n",
       "      <th>bmi_category</th>\n",
       "      <th>map</th>\n",
       "      <th>map_category</th>\n",
       "      <th>age_category</th>\n",
       "      <th>pulse_pressure</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>gluc</th>\n",
       "      <th>gender</th>\n",
       "      <th>smoke</th>\n",
       "      <th>alco</th>\n",
       "      <th>active</th>\n",
       "      <th>cardio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50</td>\n",
       "      <td>168</td>\n",
       "      <td>62.0</td>\n",
       "      <td>110</td>\n",
       "      <td>80</td>\n",
       "      <td>21.97</td>\n",
       "      <td>1</td>\n",
       "      <td>90.00</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>55</td>\n",
       "      <td>156</td>\n",
       "      <td>85.0</td>\n",
       "      <td>140</td>\n",
       "      <td>90</td>\n",
       "      <td>34.93</td>\n",
       "      <td>3</td>\n",
       "      <td>106.67</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>51</td>\n",
       "      <td>165</td>\n",
       "      <td>64.0</td>\n",
       "      <td>130</td>\n",
       "      <td>70</td>\n",
       "      <td>23.51</td>\n",
       "      <td>1</td>\n",
       "      <td>90.00</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>60</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>48</td>\n",
       "      <td>169</td>\n",
       "      <td>82.0</td>\n",
       "      <td>150</td>\n",
       "      <td>100</td>\n",
       "      <td>28.71</td>\n",
       "      <td>2</td>\n",
       "      <td>116.67</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60</td>\n",
       "      <td>151</td>\n",
       "      <td>67.0</td>\n",
       "      <td>120</td>\n",
       "      <td>80</td>\n",
       "      <td>29.38</td>\n",
       "      <td>2</td>\n",
       "      <td>93.33</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  height  weight  systolic  diastolic    bmi  bmi_category     map  \\\n",
       "0   50     168    62.0       110         80  21.97             1   90.00   \n",
       "1   55     156    85.0       140         90  34.93             3  106.67   \n",
       "2   51     165    64.0       130         70  23.51             1   90.00   \n",
       "3   48     169    82.0       150        100  28.71             2  116.67   \n",
       "4   60     151    67.0       120         80  29.38             2   93.33   \n",
       "\n",
       "   map_category  age_category  pulse_pressure  cholesterol  gluc  gender  \\\n",
       "0             1             2              30            0     0       1   \n",
       "1             3             2              50            2     0       0   \n",
       "2             1             2              60            2     0       0   \n",
       "3             3             2              50            0     0       1   \n",
       "4             2             3              40            1     1       0   \n",
       "\n",
       "   smoke  alco  active  cardio  \n",
       "0      0     0       1       0  \n",
       "1      0     0       1       1  \n",
       "2      0     0       0       1  \n",
       "3      0     0       1       1  \n",
       "4      0     0       0       0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_csv('data/preprocessed_data_full_encoded_new_v3.csv')\n",
    "\n",
    "print('Sample Data', len(df)) \n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_keep = ['map', \n",
    "                'map_category', \n",
    "                'systolic', \n",
    "                'diastolic', \n",
    "                'pulse_pressure', \n",
    "                'bmi', \n",
    "                'bmi_category', \n",
    "                'cholesterol', \n",
    "                'age', \n",
    "                'weight', \n",
    "                'age_category', \n",
    "                'gluc', \n",
    "                'active', \n",
    "                'height', \n",
    "                'alco', \n",
    "                'smoke', \n",
    "                'cardio']\n",
    "\n",
    "df = df[cols_to_keep]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('cardio', axis=1)  \n",
    "y = df['cardio']   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapping = {0: 'Healthy', 1: 'Cardio Risk'}\n",
    "target_names = [label_mapping[label] for label in y.unique()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Scaling Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features = ['age', 'height', 'weight', 'systolic', 'diastolic', 'bmi', 'map', 'pulse_pressure']\n",
    "ordinal_features = ['cholesterol', 'gluc', 'age_category', 'bmi_category', 'map_category']\n",
    "binary_features = ['smoke', 'alco', 'active']\n",
    "\n",
    "\n",
    "assert all(feature in X.columns for feature in numerical_features + ordinal_features + binary_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>systolic</th>\n",
       "      <th>diastolic</th>\n",
       "      <th>bmi</th>\n",
       "      <th>map</th>\n",
       "      <th>pulse_pressure</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>gluc</th>\n",
       "      <th>age_category</th>\n",
       "      <th>bmi_category</th>\n",
       "      <th>map_category</th>\n",
       "      <th>smoke</th>\n",
       "      <th>alco</th>\n",
       "      <th>active</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.396496</td>\n",
       "      <td>0.497124</td>\n",
       "      <td>-0.936186</td>\n",
       "      <td>-1.077190</td>\n",
       "      <td>-0.16881</td>\n",
       "      <td>-1.155295</td>\n",
       "      <td>-0.655924</td>\n",
       "      <td>-1.389140</td>\n",
       "      <td>-0.575899</td>\n",
       "      <td>-0.421179</td>\n",
       "      <td>-0.362762</td>\n",
       "      <td>-1.024135</td>\n",
       "      <td>-0.880145</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.338776</td>\n",
       "      <td>-1.094188</td>\n",
       "      <td>0.908758</td>\n",
       "      <td>0.930288</td>\n",
       "      <td>1.09822</td>\n",
       "      <td>1.624188</td>\n",
       "      <td>1.092736</td>\n",
       "      <td>0.492485</td>\n",
       "      <td>2.230670</td>\n",
       "      <td>-0.421179</td>\n",
       "      <td>-0.362762</td>\n",
       "      <td>1.053030</td>\n",
       "      <td>0.977427</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.249442</td>\n",
       "      <td>0.099296</td>\n",
       "      <td>-0.775756</td>\n",
       "      <td>0.261129</td>\n",
       "      <td>-1.43584</td>\n",
       "      <td>-0.825017</td>\n",
       "      <td>-0.655924</td>\n",
       "      <td>1.433297</td>\n",
       "      <td>2.230670</td>\n",
       "      <td>-0.421179</td>\n",
       "      <td>-0.362762</td>\n",
       "      <td>-1.024135</td>\n",
       "      <td>-0.880145</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.690605</td>\n",
       "      <td>0.629733</td>\n",
       "      <td>0.668113</td>\n",
       "      <td>1.599448</td>\n",
       "      <td>2.36525</td>\n",
       "      <td>0.290208</td>\n",
       "      <td>2.141723</td>\n",
       "      <td>0.492485</td>\n",
       "      <td>-0.575899</td>\n",
       "      <td>-0.421179</td>\n",
       "      <td>-0.362762</td>\n",
       "      <td>0.014447</td>\n",
       "      <td>0.977427</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.074048</td>\n",
       "      <td>-1.757234</td>\n",
       "      <td>-0.535111</td>\n",
       "      <td>-0.408030</td>\n",
       "      <td>-0.16881</td>\n",
       "      <td>0.433900</td>\n",
       "      <td>-0.306612</td>\n",
       "      <td>-0.448328</td>\n",
       "      <td>0.827385</td>\n",
       "      <td>1.234620</td>\n",
       "      <td>1.967915</td>\n",
       "      <td>0.014447</td>\n",
       "      <td>0.048641</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49813</th>\n",
       "      <td>0.485830</td>\n",
       "      <td>-0.696360</td>\n",
       "      <td>-0.134036</td>\n",
       "      <td>0.261129</td>\n",
       "      <td>1.09822</td>\n",
       "      <td>0.240880</td>\n",
       "      <td>0.742375</td>\n",
       "      <td>-0.448328</td>\n",
       "      <td>0.827385</td>\n",
       "      <td>1.234620</td>\n",
       "      <td>-0.362762</td>\n",
       "      <td>0.014447</td>\n",
       "      <td>0.977427</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49814</th>\n",
       "      <td>-0.249442</td>\n",
       "      <td>-0.431141</td>\n",
       "      <td>-1.417476</td>\n",
       "      <td>2.937766</td>\n",
       "      <td>1.09822</td>\n",
       "      <td>-1.234648</td>\n",
       "      <td>2.141723</td>\n",
       "      <td>3.314921</td>\n",
       "      <td>-0.575899</td>\n",
       "      <td>-0.421179</td>\n",
       "      <td>-0.362762</td>\n",
       "      <td>-1.024135</td>\n",
       "      <td>0.977427</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49815</th>\n",
       "      <td>0.632885</td>\n",
       "      <td>0.099296</td>\n",
       "      <td>0.507683</td>\n",
       "      <td>1.599448</td>\n",
       "      <td>-0.16881</td>\n",
       "      <td>0.433900</td>\n",
       "      <td>0.742375</td>\n",
       "      <td>2.374109</td>\n",
       "      <td>-0.575899</td>\n",
       "      <td>-0.421179</td>\n",
       "      <td>-0.362762</td>\n",
       "      <td>0.014447</td>\n",
       "      <td>0.977427</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49816</th>\n",
       "      <td>1.221102</td>\n",
       "      <td>-0.165923</td>\n",
       "      <td>-0.134036</td>\n",
       "      <td>0.595709</td>\n",
       "      <td>-0.16881</td>\n",
       "      <td>-0.055083</td>\n",
       "      <td>0.217882</td>\n",
       "      <td>0.962891</td>\n",
       "      <td>-0.575899</td>\n",
       "      <td>1.234620</td>\n",
       "      <td>1.967915</td>\n",
       "      <td>0.014447</td>\n",
       "      <td>0.977427</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49817</th>\n",
       "      <td>0.485830</td>\n",
       "      <td>0.762342</td>\n",
       "      <td>-0.134036</td>\n",
       "      <td>-0.408030</td>\n",
       "      <td>-0.16881</td>\n",
       "      <td>-0.524764</td>\n",
       "      <td>-0.306612</td>\n",
       "      <td>-0.448328</td>\n",
       "      <td>0.827385</td>\n",
       "      <td>-0.421179</td>\n",
       "      <td>-0.362762</td>\n",
       "      <td>-1.024135</td>\n",
       "      <td>0.048641</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>49818 rows Ã— 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            age    height    weight  systolic  diastolic       bmi       map  \\\n",
       "0     -0.396496  0.497124 -0.936186 -1.077190   -0.16881 -1.155295 -0.655924   \n",
       "1      0.338776 -1.094188  0.908758  0.930288    1.09822  1.624188  1.092736   \n",
       "2     -0.249442  0.099296 -0.775756  0.261129   -1.43584 -0.825017 -0.655924   \n",
       "3     -0.690605  0.629733  0.668113  1.599448    2.36525  0.290208  2.141723   \n",
       "4      1.074048 -1.757234 -0.535111 -0.408030   -0.16881  0.433900 -0.306612   \n",
       "...         ...       ...       ...       ...        ...       ...       ...   \n",
       "49813  0.485830 -0.696360 -0.134036  0.261129    1.09822  0.240880  0.742375   \n",
       "49814 -0.249442 -0.431141 -1.417476  2.937766    1.09822 -1.234648  2.141723   \n",
       "49815  0.632885  0.099296  0.507683  1.599448   -0.16881  0.433900  0.742375   \n",
       "49816  1.221102 -0.165923 -0.134036  0.595709   -0.16881 -0.055083  0.217882   \n",
       "49817  0.485830  0.762342 -0.134036 -0.408030   -0.16881 -0.524764 -0.306612   \n",
       "\n",
       "       pulse_pressure  cholesterol      gluc  age_category  bmi_category  \\\n",
       "0           -1.389140    -0.575899 -0.421179     -0.362762     -1.024135   \n",
       "1            0.492485     2.230670 -0.421179     -0.362762      1.053030   \n",
       "2            1.433297     2.230670 -0.421179     -0.362762     -1.024135   \n",
       "3            0.492485    -0.575899 -0.421179     -0.362762      0.014447   \n",
       "4           -0.448328     0.827385  1.234620      1.967915      0.014447   \n",
       "...               ...          ...       ...           ...           ...   \n",
       "49813       -0.448328     0.827385  1.234620     -0.362762      0.014447   \n",
       "49814        3.314921    -0.575899 -0.421179     -0.362762     -1.024135   \n",
       "49815        2.374109    -0.575899 -0.421179     -0.362762      0.014447   \n",
       "49816        0.962891    -0.575899  1.234620      1.967915      0.014447   \n",
       "49817       -0.448328     0.827385 -0.421179     -0.362762     -1.024135   \n",
       "\n",
       "       map_category  smoke  alco  active  \n",
       "0         -0.880145    1.0   0.0     0.0  \n",
       "1          0.977427    1.0   0.0     0.0  \n",
       "2         -0.880145    0.0   0.0     0.0  \n",
       "3          0.977427    1.0   0.0     0.0  \n",
       "4          0.048641    0.0   0.0     0.0  \n",
       "...             ...    ...   ...     ...  \n",
       "49813      0.977427    1.0   0.0     0.0  \n",
       "49814      0.977427    1.0   0.0     0.0  \n",
       "49815      0.977427    1.0   0.0     0.0  \n",
       "49816      0.977427    0.0   0.0     0.0  \n",
       "49817      0.048641    1.0   0.0     0.0  \n",
       "\n",
       "[49818 rows x 16 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler_standard = StandardScaler()\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num_scaler', scaler_standard, numerical_features),\n",
    "         ('ord_scaler', scaler_standard, ordinal_features)\n",
    "    ],\n",
    "    remainder='passthrough'  \n",
    ")\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor)\n",
    "])\n",
    "\n",
    "X_preprocessed = pipeline.fit_transform(X)\n",
    "\n",
    "X_preprocessed = pd.DataFrame(X_preprocessed, columns=numerical_features + ordinal_features + binary_features)\n",
    "X_preprocessed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Train, Val, Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_temp, y_train, y_temp = train_test_split(\n",
    "    X_preprocessed, y, test_size=0.3, random_state=42, stratify=y\n",
    ")  # Train 80%\n",
    "\n",
    "x_val, x_test, y_val, y_test = train_test_split(\n",
    "    x_temp, y_temp, test_size=(1/3), random_state=42\n",
    ")  # Val 10%, Test 10%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_np = np.array(x_train)\n",
    "y_train_np = np.array(y_train)\n",
    "x_test_np = np.array(x_test)\n",
    "y_test_np = np.array(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Base Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, x_train, y_train, x_test, y_test, model_name):\n",
    "    \n",
    "    if hasattr(model, 'cascade_forest'):\n",
    "        print(\"Using cascade_forest branch for training and prediction...\")\n",
    "        model.cascade_forest(x_train, y_train)\n",
    "        predict_func = lambda x: np.argmax(np.mean(model.cascade_forest(x), axis=0), axis=1)\n",
    "        predict_proba_func = lambda x: np.mean(model.cascade_forest(x), axis=0)\n",
    "    else:\n",
    "        print(\"Using standard branch (fit/predict/predict_proba)...\")\n",
    "        model.fit(x_train, y_train)\n",
    "        predict_func = lambda x: model.predict(x)\n",
    "        predict_proba_func = lambda x: model.predict_proba(x)\n",
    "\n",
    "    y_pred_test = predict_func(x_test)\n",
    "    y_proba_test = predict_proba_func(x_test)\n",
    "    \n",
    "    if y_proba_test.shape[1] > 1:\n",
    "        y_probs_test = y_proba_test[:, 1]\n",
    "    else:\n",
    "        y_probs_test = y_proba_test[:, 0]\n",
    "    \n",
    "    y_pred_train = predict_func(x_train)\n",
    "    y_proba_train = predict_proba_func(x_train)\n",
    "    if y_proba_train.shape[1] > 1:\n",
    "        y_probs_train = y_proba_train[:, 1]\n",
    "    else:\n",
    "        y_probs_train = y_proba_train[:, 0]\n",
    "    \n",
    "    test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "    test_acc_str = f\"{(test_accuracy * 100):.2f}%\"\n",
    "    test_auc = roc_auc_score(y_test, y_probs_test)\n",
    "    test_auc_str = f\"{test_auc:.4f}\"\n",
    "    test_report_dict = classification_report(y_test, y_pred_test, output_dict=True)\n",
    "    test_precision = test_report_dict['weighted avg']['precision'] \n",
    "    test_recall    = test_report_dict['weighted avg']['recall']   \n",
    "    test_f1        = test_report_dict['weighted avg']['f1-score']\n",
    "    \n",
    "    train_accuracy = accuracy_score(y_train, y_pred_train)\n",
    "    train_acc_str = f\"{(train_accuracy * 100):.2f}%\"\n",
    "    train_auc = roc_auc_score(y_train, y_probs_train)\n",
    "    train_auc_str = f\"{train_auc:.4f}\"\n",
    "    train_report_dict = classification_report(y_train, y_pred_train, output_dict=True)\n",
    "    train_precision = train_report_dict['weighted avg']['precision']\n",
    "    train_recall    = train_report_dict['weighted avg']['recall']    \n",
    "    train_f1        = train_report_dict['weighted avg']['f1-score']  \n",
    "\n",
    "    data = [\n",
    "        [\"Test\", test_acc_str, test_auc_str],\n",
    "        [\"Train\", train_acc_str, train_auc_str]\n",
    "    ]\n",
    "\n",
    "    headers = [\"\", \"Accuracy\", \"AUC Score\"]\n",
    "\n",
    "    print(f\"\\n=== {model_name} ===\\n\")\n",
    "    print(tabulate(data, headers=headers, tablefmt=\"grid\"))\n",
    "    \n",
    "    print(\"\\nOverfitting Check :\")\n",
    "    if train_accuracy > test_accuracy + 5 or train_auc > test_auc + 0.05:\n",
    "        print(\"The model might be overfitting.\")\n",
    "    else:\n",
    "        print(\"No significant signs of overfitting.\\n\")\n",
    "    \n",
    "    # # Plot Confusion Matrix and ROC Curve\n",
    "    # fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    # cm = confusion_matrix(y_test, y_pred_test)\n",
    "    # # If a global variable 'label_mapping' exists, use it for display labels\n",
    "    # display_labels = list(label_mapping.values()) if 'label_mapping' in globals() else None\n",
    "    # disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=display_labels)\n",
    "    # disp.plot(ax=axes[0], cmap='viridis', colorbar=False)\n",
    "    # axes[0].set_title(f\"{model_name} - Confusion Matrix\")\n",
    "    \n",
    "    # fpr, tpr, _ = roc_curve(y_test, y_probs_test)\n",
    "    # axes[1].plot(fpr, tpr, label=f\"ROC Curve (AUC = {test_auc:.4f})\", linewidth=2)\n",
    "    # axes[1].plot([0, 1], [0, 1], 'k--', label=\"Random Guess\", linewidth=1)\n",
    "    # axes[1].set_title(f\"{model_name} - ROC Curve\")\n",
    "    # axes[1].legend(loc=\"lower right\")\n",
    "    # axes[1].grid(alpha=0.3)\n",
    "    \n",
    "    # plt.tight_layout()\n",
    "    # plt.show()\n",
    "    \n",
    "    return {\n",
    "        'model_name': model_name,\n",
    "        'train_accuracy': train_accuracy,\n",
    "        'train_precision': train_precision,\n",
    "        'train_recall': train_recall,\n",
    "        'train_f1': train_f1,\n",
    "        'test_accuracy': test_accuracy,\n",
    "        'test_precision': test_precision,\n",
    "        'test_recall': test_recall,\n",
    "        'test_f1': test_f1\n",
    "    }\n",
    "\n",
    "\n",
    "def create_summary_table(results):\n",
    "    test_summary = pd.DataFrame([{\n",
    "        'Algorithm': r['model_name'],\n",
    "        'Accuracy':  round(r['test_accuracy'], 4),\n",
    "        'Precision': round(r['test_precision'], 4),\n",
    "        'Recall':    round(r['test_recall'], 4),\n",
    "        'F1-Score':  round(r['test_f1'], 4)\n",
    "    } for r in results])\n",
    "    train_summary = pd.DataFrame([{\n",
    "        'Algorithm': r['model_name'],\n",
    "        'Accuracy':  round(r['train_accuracy'], 4),\n",
    "        'Precision': round(r['train_precision'], 4),\n",
    "        'Recall':    round(r['train_recall'], 4),\n",
    "        'F1-Score':  round(r['train_f1'], 4)\n",
    "    } for r in results])\n",
    "    \n",
    "    \n",
    "    print(\"\\nSummary Table - Test Metrics\")\n",
    "    print(tabulate(test_summary, headers='keys', tablefmt='grid', showindex=False))\n",
    "\n",
    "    print(\"Summary Table - Training Metrics\")\n",
    "    print(tabulate(train_summary, headers='keys', tablefmt='grid', showindex=False))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `Logistic Regression`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using standard branch (fit/predict/predict_proba)...\n",
      "\n",
      "=== Logistic Regression ===\n",
      "\n",
      "+-------+------------+-------------+\n",
      "|       | Accuracy   |   AUC Score |\n",
      "+=======+============+=============+\n",
      "| Test  | 84.30%     |      0.9032 |\n",
      "+-------+------------+-------------+\n",
      "| Train | 84.34%     |      0.9061 |\n",
      "+-------+------------+-------------+\n",
      "\n",
      "Overfitting Check :\n",
      "No significant signs of overfitting.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logreg_model = LogisticRegression()\n",
    "logreg_results = evaluate_model(logreg_model, x_train_np, y_train_np, x_test_np, y_test_np, \"Logistic Regression\")\n",
    "logreg_results;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `Random Forest`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using standard branch (fit/predict/predict_proba)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Random Forest ===\n",
      "\n",
      "+-------+------------+-------------+\n",
      "|       | Accuracy   |   AUC Score |\n",
      "+=======+============+=============+\n",
      "| Test  | 85.37%     |      0.9417 |\n",
      "+-------+------------+-------------+\n",
      "| Train | 99.44%     |      0.9999 |\n",
      "+-------+------------+-------------+\n",
      "\n",
      "Overfitting Check :\n",
      "The model might be overfitting.\n"
     ]
    }
   ],
   "source": [
    "rf_model = RandomForestClassifier()\n",
    "rf_results = evaluate_model(rf_model, x_train_np, y_train_np, x_test_np, y_test_np, \"Random Forest\")\n",
    "rf_results;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `Decision Tree`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using standard branch (fit/predict/predict_proba)...\n",
      "\n",
      "=== Decision Tree ===\n",
      "\n",
      "+-------+------------+-------------+\n",
      "|       | Accuracy   |   AUC Score |\n",
      "+=======+============+=============+\n",
      "| Test  | 83.38%     |      0.8357 |\n",
      "+-------+------------+-------------+\n",
      "| Train | 99.44%     |      0.9999 |\n",
      "+-------+------------+-------------+\n",
      "\n",
      "Overfitting Check :\n",
      "The model might be overfitting.\n"
     ]
    }
   ],
   "source": [
    "dt_model = DecisionTreeClassifier()\n",
    "dt_results = evaluate_model(dt_model, x_train_np, y_train_np, x_test_np, y_test_np, \"Decision Tree\")\n",
    "dt_results;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `SVM`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using standard branch (fit/predict/predict_proba)...\n",
      "\n",
      "=== SVM ===\n",
      "\n",
      "+-------+------------+-------------+\n",
      "|       | Accuracy   |   AUC Score |\n",
      "+=======+============+=============+\n",
      "| Test  | 86.05%     |      0.9343 |\n",
      "+-------+------------+-------------+\n",
      "| Train | 86.42%     |      0.9411 |\n",
      "+-------+------------+-------------+\n",
      "\n",
      "Overfitting Check :\n",
      "No significant signs of overfitting.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svm_model = SVC(probability=True)\n",
    "svm_results = evaluate_model(svm_model, x_train_np, y_train_np, x_test_np, y_test_np, \"SVM\")\n",
    "svm_results;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `Naive Bayes`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using standard branch (fit/predict/predict_proba)...\n",
      "\n",
      "=== Naive Bayes ===\n",
      "\n",
      "+-------+------------+-------------+\n",
      "|       | Accuracy   |   AUC Score |\n",
      "+=======+============+=============+\n",
      "| Test  | 84.52%     |      0.8998 |\n",
      "+-------+------------+-------------+\n",
      "| Train | 84.29%     |      0.9031 |\n",
      "+-------+------------+-------------+\n",
      "\n",
      "Overfitting Check :\n",
      "No significant signs of overfitting.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nb_model = GaussianNB()\n",
    "nb_results = evaluate_model(nb_model, x_train_np, y_train_np, x_test_np, y_test_np, \"Naive Bayes\")\n",
    "nb_results;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `KNN`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using standard branch (fit/predict/predict_proba)...\n",
      "\n",
      "=== KNN ===\n",
      "\n",
      "+-------+------------+-------------+\n",
      "|       | Accuracy   |   AUC Score |\n",
      "+=======+============+=============+\n",
      "| Test  | 83.84%     |       0.92  |\n",
      "+-------+------------+-------------+\n",
      "| Train | 89.46%     |       0.969 |\n",
      "+-------+------------+-------------+\n",
      "\n",
      "Overfitting Check :\n",
      "No significant signs of overfitting.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "knn_model = KNeighborsClassifier()\n",
    "knn_results = evaluate_model(knn_model, x_train_np, y_train_np, x_test_np, y_test_np, \"KNN\")\n",
    "knn_results;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `XGBoost`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using standard branch (fit/predict/predict_proba)...\n",
      "\n",
      "=== XGBoost ===\n",
      "\n",
      "+-------+------------+-------------+\n",
      "|       | Accuracy   |   AUC Score |\n",
      "+=======+============+=============+\n",
      "| Test  | 86.27%     |      0.9468 |\n",
      "+-------+------------+-------------+\n",
      "| Train | 90.27%     |      0.9734 |\n",
      "+-------+------------+-------------+\n",
      "\n",
      "Overfitting Check :\n",
      "No significant signs of overfitting.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "xgb_results = evaluate_model(xgb_model, x_train_np, y_train_np, x_test_np, y_test_np, \"XGBoost\")\n",
    "xgb_results;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `Light GBM`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using standard branch (fit/predict/predict_proba)...\n",
      "\n",
      "=== LightGBM ===\n",
      "\n",
      "+-------+------------+-------------+\n",
      "|       | Accuracy   |   AUC Score |\n",
      "+=======+============+=============+\n",
      "| Test  | 86.35%     |      0.9494 |\n",
      "+-------+------------+-------------+\n",
      "| Train | 87.80%     |      0.9613 |\n",
      "+-------+------------+-------------+\n",
      "\n",
      "Overfitting Check :\n",
      "No significant signs of overfitting.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lgbm_model = LGBMClassifier(verbose=-1)\n",
    "lgbm_results = evaluate_model(lgbm_model, x_train_np, y_train_np, x_test_np, y_test_np, \"LightGBM\")\n",
    "lgbm_results;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `Cat Boost`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using standard branch (fit/predict/predict_proba)...\n",
      "\n",
      "=== CatBoost ===\n",
      "\n",
      "+-------+------------+-------------+\n",
      "|       | Accuracy   |   AUC Score |\n",
      "+=======+============+=============+\n",
      "| Test  | 86.17%     |      0.9472 |\n",
      "+-------+------------+-------------+\n",
      "| Train | 89.10%     |      0.9677 |\n",
      "+-------+------------+-------------+\n",
      "\n",
      "Overfitting Check :\n",
      "No significant signs of overfitting.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "catb_model = CatBoostClassifier(verbose=False)\n",
    "catb_results = evaluate_model(catb_model, x_train_np, y_train_np, x_test_np, y_test_np, \"CatBoost\")\n",
    "catb_results;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `SnapBoost`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using standard branch (fit/predict/predict_proba)...\n",
      "\n",
      "=== SnapBoost ===\n",
      "\n",
      "+-------+------------+-------------+\n",
      "|       | Accuracy   |   AUC Score |\n",
      "+=======+============+=============+\n",
      "| Test  | 85.43%     |      0.9423 |\n",
      "+-------+------------+-------------+\n",
      "| Train | 86.35%     |      0.9488 |\n",
      "+-------+------------+-------------+\n",
      "\n",
      "Overfitting Check :\n",
      "No significant signs of overfitting.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "snb_model = BoostingMachineClassifier()\n",
    "snb_results = evaluate_model(snb_model, x_train_np, y_train_np, x_test_np, y_test_np, \"SnapBoost\")\n",
    "snb_results;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `Explainable Boosting Machine (EBM)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using standard branch (fit/predict/predict_proba)...\n",
      "\n",
      "=== EBM ===\n",
      "\n",
      "+-------+------------+-------------+\n",
      "|       | Accuracy   |   AUC Score |\n",
      "+=======+============+=============+\n",
      "| Test  | 85.13%     |      0.9302 |\n",
      "+-------+------------+-------------+\n",
      "| Train | 85.66%     |      0.9364 |\n",
      "+-------+------------+-------------+\n",
      "\n",
      "Overfitting Check :\n",
      "No significant signs of overfitting.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ebm_model = ExplainableBoostingClassifier(n_jobs=1)\n",
    "ebm_results = evaluate_model(ebm_model, x_train_np, y_train_np, x_test_np, y_test_np, \"EBM\")\n",
    "ebm_results;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `NGBoost`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using standard branch (fit/predict/predict_proba)...\n",
      "\n",
      "=== NGBoost ===\n",
      "\n",
      "+-------+------------+-------------+\n",
      "|       | Accuracy   |   AUC Score |\n",
      "+=======+============+=============+\n",
      "| Test  | 85.03%     |      0.9324 |\n",
      "+-------+------------+-------------+\n",
      "| Train | 84.85%     |      0.9357 |\n",
      "+-------+------------+-------------+\n",
      "\n",
      "Overfitting Check :\n",
      "No significant signs of overfitting.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ngb_model = NGBClassifier(Dist=Bernoulli, verbose=False)\n",
    "ngb_results = evaluate_model(ngb_model, x_train_np, y_train_np, x_test_np, y_test_np, \"NGBoost\")\n",
    "ngb_results;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `AdaBoost`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using standard branch (fit/predict/predict_proba)...\n",
      "\n",
      "=== AdaBoost ===\n",
      "\n",
      "+-------+------------+-------------+\n",
      "|       | Accuracy   |   AUC Score |\n",
      "+=======+============+=============+\n",
      "| Test  | 84.36%     |      0.9119 |\n",
      "+-------+------------+-------------+\n",
      "| Train | 84.52%     |      0.9148 |\n",
      "+-------+------------+-------------+\n",
      "\n",
      "Overfitting Check :\n",
      "No significant signs of overfitting.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "adb_model = AdaBoostClassifier(random_state=42)\n",
    "adb_results = evaluate_model(adb_model, x_train_np, y_train_np, x_test_np, y_test_np, \"AdaBoost\")\n",
    "adb_results;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `GradientBoosting`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using standard branch (fit/predict/predict_proba)...\n",
      "\n",
      "=== Gradient Boosting ===\n",
      "\n",
      "+-------+------------+-------------+\n",
      "|       | Accuracy   |   AUC Score |\n",
      "+=======+============+=============+\n",
      "| Test  | 85.13%     |      0.9391 |\n",
      "+-------+------------+-------------+\n",
      "| Train | 85.93%     |      0.9437 |\n",
      "+-------+------------+-------------+\n",
      "\n",
      "Overfitting Check :\n",
      "No significant signs of overfitting.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grb_model =  GradientBoostingClassifier(random_state=42)\n",
    "grb_results = evaluate_model(grb_model, x_train_np, y_train_np, x_test_np, y_test_np, \"Gradient Boosting\")\n",
    "grb_results;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `Hist GradientBoosting`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using standard branch (fit/predict/predict_proba)...\n",
      "\n",
      "=== Hist Gradient Boosting ===\n",
      "\n",
      "+-------+------------+-------------+\n",
      "|       | Accuracy   |   AUC Score |\n",
      "+=======+============+=============+\n",
      "| Test  | 86.43%     |      0.9494 |\n",
      "+-------+------------+-------------+\n",
      "| Train | 87.49%     |      0.9576 |\n",
      "+-------+------------+-------------+\n",
      "\n",
      "Overfitting Check :\n",
      "No significant signs of overfitting.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hgrb_model =  HistGradientBoostingClassifier(random_state=42)\n",
    "hgrb_results = evaluate_model(hgrb_model, x_train_np, y_train_np, x_test_np, y_test_np, \"Hist Gradient Boosting\")\n",
    "hgrb_results;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Cascaded Random Forest`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cascade_forest branch for training and prediction...\n",
      "Adding/Training Layer, n_layer=1\n",
      "Layer validation accuracy = 0.8434408602150537\n",
      "Adding/Training Layer, n_layer=2\n",
      "Layer validation accuracy = 0.8536200716845879\n",
      "Adding/Training Layer, n_layer=3\n",
      "Layer validation accuracy = 0.8554838709677419\n",
      "Adding/Training Layer, n_layer=4\n",
      "Layer validation accuracy = 0.8573476702508961\n",
      "Adding/Training Layer, n_layer=5\n",
      "Layer validation accuracy = 0.8576344086021506\n",
      "Adding/Training Layer, n_layer=6\n",
      "Layer validation accuracy = 0.857921146953405\n",
      "Adding/Training Layer, n_layer=7\n",
      "Layer validation accuracy = 0.8580645161290322\n",
      "Adding/Training Layer, n_layer=8\n",
      "Layer validation accuracy = 0.8582078853046595\n",
      "Adding/Training Layer, n_layer=9\n",
      "Layer validation accuracy = 0.858494623655914\n",
      "Adding/Training Layer, n_layer=10\n",
      "Layer validation accuracy = 0.8577777777777778\n",
      "\n",
      "=== Cascaded Random Forest ===\n",
      "\n",
      "+-------+------------+-------------+\n",
      "|       | Accuracy   |   AUC Score |\n",
      "+=======+============+=============+\n",
      "| Test  | 85.47%     |      0.9422 |\n",
      "+-------+------------+-------------+\n",
      "| Train | 85.87%     |      0.9462 |\n",
      "+-------+------------+-------------+\n",
      "\n",
      "Overfitting Check :\n",
      "No significant signs of overfitting.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gcf_model = gcForest(n_cascadeRF=2,n_cascadeRFtree=500) #Default values tolerance =0.0 ,n_cascadeRFtree=101\n",
    "gcf_results = evaluate_model(gcf_model, x_train_np, y_train_np, x_test_np, y_test_np, \"Cascaded Random Forest\")\n",
    "gcf_results;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `TabNet`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using standard branch (fit/predict/predict_proba)...\n",
      "\n",
      "=== TabNet Classifier ===\n",
      "\n",
      "+-------+------------+-------------+\n",
      "|       | Accuracy   |   AUC Score |\n",
      "+=======+============+=============+\n",
      "| Test  | 86.39%     |      0.9499 |\n",
      "+-------+------------+-------------+\n",
      "| Train | 86.74%     |      0.9531 |\n",
      "+-------+------------+-------------+\n",
      "\n",
      "Overfitting Check :\n",
      "No significant signs of overfitting.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tabnet_model = TabNetClassifier(verbose=0)\n",
    "tabnet_results = evaluate_model(tabnet_model, x_train_np, y_train_np, x_test_np, y_test_np, \"TabNet Classifier\")\n",
    "tabnet_results;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `NN`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KerasModelWrapper(BaseEstimator):\n",
    "    def __init__(self, model, epochs=100, batch_size=32, validation_split=0.2, callbacks=None):\n",
    "        self.model = model\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.validation_split = validation_split\n",
    "        self.callbacks = callbacks\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        self.model.fit(\n",
    "            x, y,\n",
    "            epochs=self.epochs,\n",
    "            batch_size=self.batch_size,\n",
    "            validation_split=self.validation_split,\n",
    "            callbacks=self.callbacks,\n",
    "            verbose=0\n",
    "        )\n",
    "        return self\n",
    "\n",
    "    def predict(self, x):\n",
    "        proba = self.model.predict(x)\n",
    "        return (proba > 0.5).astype(int)\n",
    "\n",
    "    def predict_proba(self, x):\n",
    "        proba = self.model.predict(x)\n",
    "        return np.hstack([1 - proba, proba])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using standard branch (fit/predict/predict_proba)...\n",
      "\u001b[1m156/156\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 633us/step\n",
      "\u001b[1m156/156\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 478us/step\n",
      "\u001b[1m1090/1090\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 406us/step\n",
      "\u001b[1m1090/1090\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 416us/step\n",
      "\n",
      "=== Keras Sequential Model ===\n",
      "\n",
      "+-------+------------+-------------+\n",
      "|       | Accuracy   |   AUC Score |\n",
      "+=======+============+=============+\n",
      "| Test  | 86.11%     |      0.9476 |\n",
      "+-------+------------+-------------+\n",
      "| Train | 86.68%     |      0.9523 |\n",
      "+-------+------------+-------------+\n",
      "\n",
      "Overfitting Check :\n",
      "No significant signs of overfitting.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Dense(32, input_dim=x_train_np.shape[1], activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(1, activation='sigmoid')  \n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "wrapped_model = KerasModelWrapper(\n",
    "    model=model,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stop]\n",
    ")\n",
    "\n",
    "nn_results = evaluate_model(\n",
    "    wrapped_model, \n",
    "    x_train_np, y_train_np,\n",
    "    x_test_np, y_test_np,\n",
    "    model_name='Keras Sequential Model'\n",
    ")\n",
    "\n",
    "nn_results;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Summary Table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary Table - Test Metrics\n",
      "+------------------------+------------+-------------+----------+------------+\n",
      "| Algorithm              |   Accuracy |   Precision |   Recall |   F1-Score |\n",
      "+========================+============+=============+==========+============+\n",
      "| Logistic Regression    |     0.843  |      0.8451 |   0.843  |     0.8427 |\n",
      "+------------------------+------------+-------------+----------+------------+\n",
      "| Random Forest          |     0.8537 |      0.8538 |   0.8537 |     0.8536 |\n",
      "+------------------------+------------+-------------+----------+------------+\n",
      "| Decision Tree          |     0.8338 |      0.8338 |   0.8338 |     0.8338 |\n",
      "+------------------------+------------+-------------+----------+------------+\n",
      "| SVM                    |     0.8605 |      0.8637 |   0.8605 |     0.86   |\n",
      "+------------------------+------------+-------------+----------+------------+\n",
      "| Naive Bayes            |     0.8452 |      0.8502 |   0.8452 |     0.8445 |\n",
      "+------------------------+------------+-------------+----------+------------+\n",
      "| KNN                    |     0.8384 |      0.8386 |   0.8384 |     0.8383 |\n",
      "+------------------------+------------+-------------+----------+------------+\n",
      "| XGBoost                |     0.8627 |      0.8639 |   0.8627 |     0.8625 |\n",
      "+------------------------+------------+-------------+----------+------------+\n",
      "| LightGBM               |     0.8635 |      0.8646 |   0.8635 |     0.8633 |\n",
      "+------------------------+------------+-------------+----------+------------+\n",
      "| CatBoost               |     0.8617 |      0.8629 |   0.8617 |     0.8615 |\n",
      "+------------------------+------------+-------------+----------+------------+\n",
      "| SnapBoost              |     0.8543 |      0.8564 |   0.8543 |     0.8539 |\n",
      "+------------------------+------------+-------------+----------+------------+\n",
      "| EBM                    |     0.8513 |      0.8534 |   0.8513 |     0.8509 |\n",
      "+------------------------+------------+-------------+----------+------------+\n",
      "| NGBoost                |     0.8503 |      0.8557 |   0.8503 |     0.8495 |\n",
      "+------------------------+------------+-------------+----------+------------+\n",
      "| AdaBoost               |     0.8436 |      0.8506 |   0.8436 |     0.8426 |\n",
      "+------------------------+------------+-------------+----------+------------+\n",
      "| Gradient Boosting      |     0.8513 |      0.8538 |   0.8513 |     0.8509 |\n",
      "+------------------------+------------+-------------+----------+------------+\n",
      "| Hist Gradient Boosting |     0.8643 |      0.8657 |   0.8643 |     0.8641 |\n",
      "+------------------------+------------+-------------+----------+------------+\n",
      "| Cascaded Random Forest |     0.8547 |      0.8567 |   0.8547 |     0.8543 |\n",
      "+------------------------+------------+-------------+----------+------------+\n",
      "| TabNet Classifier      |     0.8639 |      0.8645 |   0.8639 |     0.8638 |\n",
      "+------------------------+------------+-------------+----------+------------+\n",
      "| Keras Sequential Model |     0.8611 |      0.8622 |   0.8611 |     0.8609 |\n",
      "+------------------------+------------+-------------+----------+------------+\n",
      "Summary Table - Training Metrics\n",
      "+------------------------+------------+-------------+----------+------------+\n",
      "| Algorithm              |   Accuracy |   Precision |   Recall |   F1-Score |\n",
      "+========================+============+=============+==========+============+\n",
      "| Logistic Regression    |     0.8434 |      0.8457 |   0.8434 |     0.8431 |\n",
      "+------------------------+------------+-------------+----------+------------+\n",
      "| Random Forest          |     0.9944 |      0.9944 |   0.9944 |     0.9944 |\n",
      "+------------------------+------------+-------------+----------+------------+\n",
      "| Decision Tree          |     0.9944 |      0.9945 |   0.9944 |     0.9944 |\n",
      "+------------------------+------------+-------------+----------+------------+\n",
      "| SVM                    |     0.8642 |      0.8681 |   0.8642 |     0.8638 |\n",
      "+------------------------+------------+-------------+----------+------------+\n",
      "| Naive Bayes            |     0.8429 |      0.8481 |   0.8429 |     0.8422 |\n",
      "+------------------------+------------+-------------+----------+------------+\n",
      "| KNN                    |     0.8946 |      0.8948 |   0.8946 |     0.8946 |\n",
      "+------------------------+------------+-------------+----------+------------+\n",
      "| XGBoost                |     0.9027 |      0.9043 |   0.9027 |     0.9026 |\n",
      "+------------------------+------------+-------------+----------+------------+\n",
      "| LightGBM               |     0.878  |      0.8796 |   0.878  |     0.8779 |\n",
      "+------------------------+------------+-------------+----------+------------+\n",
      "| CatBoost               |     0.891  |      0.8926 |   0.891  |     0.8908 |\n",
      "+------------------------+------------+-------------+----------+------------+\n",
      "| SnapBoost              |     0.8635 |      0.8662 |   0.8635 |     0.8632 |\n",
      "+------------------------+------------+-------------+----------+------------+\n",
      "| EBM                    |     0.8566 |      0.8587 |   0.8566 |     0.8563 |\n",
      "+------------------------+------------+-------------+----------+------------+\n",
      "| NGBoost                |     0.8485 |      0.8542 |   0.8485 |     0.8478 |\n",
      "+------------------------+------------+-------------+----------+------------+\n",
      "| AdaBoost               |     0.8452 |      0.8535 |   0.8452 |     0.8442 |\n",
      "+------------------------+------------+-------------+----------+------------+\n",
      "| Gradient Boosting      |     0.8593 |      0.862  |   0.8593 |     0.8589 |\n",
      "+------------------------+------------+-------------+----------+------------+\n",
      "| Hist Gradient Boosting |     0.8749 |      0.8764 |   0.8749 |     0.8747 |\n",
      "+------------------------+------------+-------------+----------+------------+\n",
      "| Cascaded Random Forest |     0.8587 |      0.8608 |   0.8587 |     0.8584 |\n",
      "+------------------------+------------+-------------+----------+------------+\n",
      "| TabNet Classifier      |     0.8674 |      0.8683 |   0.8674 |     0.8673 |\n",
      "+------------------------+------------+-------------+----------+------------+\n",
      "| Keras Sequential Model |     0.8668 |      0.8685 |   0.8668 |     0.8666 |\n",
      "+------------------------+------------+-------------+----------+------------+\n"
     ]
    }
   ],
   "source": [
    "results = [logreg_results, rf_results, dt_results, \n",
    "        svm_results, nb_results, knn_results, \n",
    "        xgb_results, lgbm_results, catb_results, \n",
    "        snb_results, ebm_results, ngb_results, \n",
    "        adb_results, grb_results, hgrb_results, \n",
    "        gcf_results, tabnet_results, nn_results]\n",
    "\n",
    "create_summary_table(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_el",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
