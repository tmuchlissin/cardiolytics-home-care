{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)  # Suppress all logs except warnings\n",
    "\n",
    "from sklearn.model_selection import train_test_split,cross_val_score, RandomizedSearchCV, StratifiedKFold, GridSearchCV\n",
    "from sklearn.metrics import (make_scorer, accuracy_score, precision_score, recall_score, f1_score, \n",
    "                             roc_auc_score, classification_report, confusion_matrix, ConfusionMatrixDisplay)\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from interpret.glassbox import ExplainableBoostingClassifier\n",
    "from ngboost import NGBClassifier\n",
    "from snapml import BoostingMachineClassifier\n",
    "from tab_transformer_pytorch import TabTransformer\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "import lightgbm as lgb\n",
    "\n",
    "from skopt import BayesSearchCV\n",
    "from ray import tune\n",
    "from ngboost.distns import Bernoulli\n",
    "from ngboost.scores import LogScore\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Load & Split Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Data\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>ap_hi</th>\n",
       "      <th>ap_lo</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>gluc</th>\n",
       "      <th>smoke</th>\n",
       "      <th>alco</th>\n",
       "      <th>active</th>\n",
       "      <th>cardio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>168</td>\n",
       "      <td>62.0</td>\n",
       "      <td>110</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>156</td>\n",
       "      <td>85.0</td>\n",
       "      <td>140</td>\n",
       "      <td>90</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "      <td>165</td>\n",
       "      <td>64.0</td>\n",
       "      <td>130</td>\n",
       "      <td>70</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "      <td>169</td>\n",
       "      <td>82.0</td>\n",
       "      <td>150</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>151</td>\n",
       "      <td>67.0</td>\n",
       "      <td>120</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  gender  height  weight  ap_hi  ap_lo  cholesterol  gluc  smoke  alco  \\\n",
       "0   50       1     168    62.0    110     80            0     0      0     0   \n",
       "1   55       0     156    85.0    140     90            2     0      0     0   \n",
       "2   51       0     165    64.0    130     70            2     0      0     0   \n",
       "3   48       1     169    82.0    150    100            0     0      0     0   \n",
       "4   60       0     151    67.0    120     80            1     1      0     0   \n",
       "\n",
       "   active  cardio  \n",
       "0       1       0  \n",
       "1       1       1  \n",
       "2       0       1  \n",
       "3       1       1  \n",
       "4       0       0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/cleaned_data.csv')\n",
    "\n",
    "print('Sample Data')\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('cardio', axis=1)  \n",
    "y = df['cardio']   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapping = {0: 'Healthy', 1: 'Cardio Risk'}\n",
    "target_names = [label_mapping[label] for label in y.unique()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Scaling Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_features = ['age', 'height', 'weight','ap_hi', 'ap_lo']\n",
    "categorical_features = ['gender', 'cholesterol', 'gluc', 'smoke', 'alco', 'active']\n",
    "\n",
    "assert all(feature in X.columns for feature in standard_features + categorical_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>ap_hi</th>\n",
       "      <th>ap_lo</th>\n",
       "      <th>gender</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>gluc</th>\n",
       "      <th>smoke</th>\n",
       "      <th>alco</th>\n",
       "      <th>active</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.430687</td>\n",
       "      <td>0.441395</td>\n",
       "      <td>-0.868717</td>\n",
       "      <td>-1.155855</td>\n",
       "      <td>-0.231849</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.310810</td>\n",
       "      <td>-1.067843</td>\n",
       "      <td>0.760886</td>\n",
       "      <td>0.929802</td>\n",
       "      <td>1.064795</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.282387</td>\n",
       "      <td>0.064086</td>\n",
       "      <td>-0.727012</td>\n",
       "      <td>0.234583</td>\n",
       "      <td>-1.528493</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.727286</td>\n",
       "      <td>0.567165</td>\n",
       "      <td>0.548329</td>\n",
       "      <td>1.625020</td>\n",
       "      <td>2.361439</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.052307</td>\n",
       "      <td>-1.696692</td>\n",
       "      <td>-0.514455</td>\n",
       "      <td>-0.460636</td>\n",
       "      <td>-0.231849</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64464</th>\n",
       "      <td>0.607409</td>\n",
       "      <td>0.064086</td>\n",
       "      <td>0.406625</td>\n",
       "      <td>1.625020</td>\n",
       "      <td>-0.231849</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64465</th>\n",
       "      <td>-0.134088</td>\n",
       "      <td>0.441395</td>\n",
       "      <td>0.123215</td>\n",
       "      <td>-0.460636</td>\n",
       "      <td>-0.231849</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64466</th>\n",
       "      <td>1.200607</td>\n",
       "      <td>-0.816303</td>\n",
       "      <td>3.665832</td>\n",
       "      <td>0.929802</td>\n",
       "      <td>1.064795</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64467</th>\n",
       "      <td>1.200607</td>\n",
       "      <td>-0.187454</td>\n",
       "      <td>-0.160194</td>\n",
       "      <td>0.582192</td>\n",
       "      <td>-0.231849</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64468</th>\n",
       "      <td>0.459110</td>\n",
       "      <td>0.692935</td>\n",
       "      <td>-0.160194</td>\n",
       "      <td>-0.460636</td>\n",
       "      <td>-0.231849</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>64469 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            age    height    weight     ap_hi     ap_lo  gender  cholesterol  \\\n",
       "0     -0.430687  0.441395 -0.868717 -1.155855 -0.231849     1.0          0.0   \n",
       "1      0.310810 -1.067843  0.760886  0.929802  1.064795     0.0          2.0   \n",
       "2     -0.282387  0.064086 -0.727012  0.234583 -1.528493     0.0          2.0   \n",
       "3     -0.727286  0.567165  0.548329  1.625020  2.361439     1.0          0.0   \n",
       "4      1.052307 -1.696692 -0.514455 -0.460636 -0.231849     0.0          1.0   \n",
       "...         ...       ...       ...       ...       ...     ...          ...   \n",
       "64464  0.607409  0.064086  0.406625  1.625020 -0.231849     0.0          0.0   \n",
       "64465 -0.134088  0.441395  0.123215 -0.460636 -0.231849     1.0          0.0   \n",
       "64466  1.200607 -0.816303  3.665832  0.929802  1.064795     0.0          1.0   \n",
       "64467  1.200607 -0.187454 -0.160194  0.582192 -0.231849     0.0          0.0   \n",
       "64468  0.459110  0.692935 -0.160194 -0.460636 -0.231849     0.0          1.0   \n",
       "\n",
       "       gluc  smoke  alco  active  \n",
       "0       0.0    0.0   0.0     1.0  \n",
       "1       0.0    0.0   0.0     1.0  \n",
       "2       0.0    0.0   0.0     0.0  \n",
       "3       0.0    0.0   0.0     1.0  \n",
       "4       1.0    0.0   0.0     0.0  \n",
       "...     ...    ...   ...     ...  \n",
       "64464   0.0    0.0   0.0     1.0  \n",
       "64465   0.0    1.0   0.0     1.0  \n",
       "64466   1.0    0.0   0.0     1.0  \n",
       "64467   1.0    0.0   0.0     0.0  \n",
       "64468   0.0    0.0   0.0     1.0  \n",
       "\n",
       "[64469 rows x 11 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler_standard = StandardScaler()\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('standard', scaler_standard, standard_features)\n",
    "    ],\n",
    "    remainder='passthrough'  \n",
    ")\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor)\n",
    "])\n",
    "\n",
    "X_preprocessed = pipeline.fit_transform(X)\n",
    "\n",
    "X_preprocessed = pd.DataFrame(X_preprocessed, columns=standard_features + categorical_features)\n",
    "X_preprocessed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Train, Val, Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_temp, y_train, y_temp = train_test_split(X_preprocessed, y, test_size=0.3, random_state=42)  # Train 70%\n",
    "x_val, x_test, y_val, y_test = train_test_split(x_temp, y_temp, test_size=(1/3), random_state=42)  # Val 20%, Test 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NumPy array format\n",
    "x_train_np = np.array(x_train)\n",
    "y_train_np = np.array(y_train)\n",
    "x_test_np = np.array(x_test)\n",
    "y_test_np = np.array(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Base Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning (Optuna)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Explainable Boosting Machine (EBM)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'learning_rate': 0.0014, 'max_bins': 512, 'max_interaction_bins': 32, 'interactions': 0.75, 'outer_bags': 8, 'greedy_ratio': 0.0, 'min_samples_leaf': 4, 'smoothing_rounds': 50, 'max_rounds': 10000, 'early_stopping_rounds': 100}\n",
      "Best Validation Accuracy: 0.7350705754614549\n",
      "EBM Accuracy with Best Parameters: 0.7231\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Healthy       0.71      0.77      0.74      3261\n",
      " Cardio Risk       0.74      0.68      0.71      3186\n",
      "\n",
      "    accuracy                           0.72      6447\n",
      "   macro avg       0.72      0.72      0.72      6447\n",
      "weighted avg       0.72      0.72      0.72      6447\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def objective(trial):\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 0.01, 0.02, step=0.005)\n",
    "    max_bins = trial.suggest_int(\"max_bins\", 256, 512, step=256)\n",
    "    max_interaction_bins = trial.suggest_int(\"max_interaction_bins\", 32, 64, step=32)\n",
    "    interactions = trial.suggest_float(\"interactions\", 0.0, 0.75, step=0.25)\n",
    "    outer_bags = trial.suggest_int(\"outer_bags\", 8, 16, step=8)\n",
    "    greedy_ratio = trial.suggest_float(\"greedy_ratio\", 0.0, 10.0, step=5.0)\n",
    "    min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 2, 4, step=2)\n",
    "    smoothing_rounds = trial.suggest_int(\"smoothing_rounds\", 25, 50, step=25)\n",
    "    max_rounds = trial.suggest_int(\"max_rounds\", 5000, 10000, step=5000)\n",
    "    early_stopping_rounds = trial.suggest_int(\"early_stopping_rounds\", 50, 100, step=50)\n",
    "\n",
    "    model = ExplainableBoostingClassifier(\n",
    "        learning_rate=learning_rate,\n",
    "        max_bins=max_bins,\n",
    "        max_interaction_bins=max_interaction_bins,\n",
    "        interactions=interactions,\n",
    "        outer_bags=outer_bags,\n",
    "        greedy_ratio=greedy_ratio,\n",
    "        min_samples_leaf=min_samples_leaf,\n",
    "        smoothing_rounds=smoothing_rounds,\n",
    "        max_rounds=max_rounds,\n",
    "        early_stopping_rounds=early_stopping_rounds,\n",
    "        random_state=42,\n",
    "    )\n",
    "\n",
    "    model.fit(x_train, y_train)\n",
    "    \n",
    "    y_val_pred = model.predict(x_val)\n",
    "    val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "\n",
    "    return val_accuracy\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "print(\"Best Parameters:\", study.best_params)\n",
    "print(\"Best Validation Accuracy:\", study.best_value)\n",
    "\n",
    "best_params = study.best_params\n",
    "base_model_ebm = ExplainableBoostingClassifier(\n",
    "    learning_rate=best_params[\"learning_rate\"],\n",
    "    max_bins=best_params[\"max_bins\"],\n",
    "    max_interaction_bins=best_params[\"max_interaction_bins\"],\n",
    "    interactions=best_params[\"interactions\"],\n",
    "    outer_bags=best_params[\"outer_bags\"],\n",
    "    greedy_ratio=best_params[\"greedy_ratio\"],\n",
    "    min_samples_leaf=best_params[\"min_samples_leaf\"],\n",
    "    smoothing_rounds=best_params[\"smoothing_rounds\"],\n",
    "    max_rounds=best_params[\"max_rounds\"],\n",
    "    early_stopping_rounds=best_params[\"early_stopping_rounds\"],\n",
    "    random_state=42,\n",
    ")\n",
    "base_model_ebm.fit(x_train, y_train)\n",
    "\n",
    "y_test_pred = base_model_ebm.predict(x_test)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print(f\"EBM Accuracy with Best Parameters: {test_accuracy:.4f}\")\n",
    "\n",
    "report = classification_report(y_test, y_test_pred, target_names=target_names)\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(report)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=base_model_ebm.classes_)\n",
    "disp.plot(cmap='viridis', colorbar=False, text_kw={'fontsize': 13, 'fontweight': 'bold'})\n",
    "plt.title(\"Confusion Matrix\", fontsize=13, fontweight='bold')\n",
    "plt.xlabel(\"Predicted Label\", fontsize=12, fontweight='bold')\n",
    "plt.ylabel(\"True Label\", fontsize=12, fontweight='bold')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'learning_rate': 0.015, 'max_bins': 512, 'max_interaction_bins': 32, 'interactions': 0.25, 'outer_bags': 16, 'greedy_ratio': 5.0, 'min_samples_leaf': 2, 'smoothing_rounds': 25, 'max_rounds': 10000, 'early_stopping_rounds': 50}\n",
      "Best Validation Accuracy: 0.7299015647624315\n",
      "EBM Accuracy with Best Parameters: 0.7222\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Healthy       0.71      0.77      0.74      3261\n",
      " Cardio Risk       0.74      0.68      0.71      3186\n",
      "\n",
      "    accuracy                           0.72      6447\n",
      "   macro avg       0.72      0.72      0.72      6447\n",
      "weighted avg       0.72      0.72      0.72      6447\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Fungsi untuk optimasi hyperparameter\n",
    "def objective(trial):\n",
    "    # 1. Saran parameter hyperparameter\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 0.01, 0.02, step=0.005)\n",
    "    max_bins = trial.suggest_int(\"max_bins\", 256, 512, step=256)\n",
    "    max_interaction_bins = trial.suggest_int(\"max_interaction_bins\", 32, 64, step=32)\n",
    "    interactions = trial.suggest_float(\"interactions\", 0.0, 0.75, step=0.25)\n",
    "    outer_bags = trial.suggest_int(\"outer_bags\", 8, 16, step=8)\n",
    "    greedy_ratio = trial.suggest_float(\"greedy_ratio\", 0.0, 10.0, step=5.0)\n",
    "    min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 2, 4, step=2)  # Regularisasi melalui leaf\n",
    "    smoothing_rounds = trial.suggest_int(\"smoothing_rounds\", 25, 50, step=25)\n",
    "    max_rounds = trial.suggest_int(\"max_rounds\", 5000, 10000, step=5000)\n",
    "    early_stopping_rounds = trial.suggest_int(\"early_stopping_rounds\", 50, 100, step=50)\n",
    "\n",
    "    # 2. Model ExplainableBoostingClassifier\n",
    "    model = ExplainableBoostingClassifier(\n",
    "        learning_rate=learning_rate,\n",
    "        max_bins=max_bins,\n",
    "        max_interaction_bins=max_interaction_bins,\n",
    "        interactions=interactions,\n",
    "        outer_bags=outer_bags,\n",
    "        greedy_ratio=greedy_ratio,\n",
    "        min_samples_leaf=min_samples_leaf,  # Membantu regularisasi\n",
    "        smoothing_rounds=smoothing_rounds,\n",
    "        max_rounds=max_rounds,\n",
    "        early_stopping_rounds=early_stopping_rounds,\n",
    "        random_state=42,\n",
    "    )\n",
    "\n",
    "    scores = cross_val_score(model, x_train, y_train, cv=5, scoring='accuracy')\n",
    "    val_accuracy = scores.mean() \n",
    "\n",
    "    return val_accuracy\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "print(\"Best Parameters:\", study.best_params)\n",
    "print(\"Best Validation Accuracy:\", study.best_value)\n",
    "\n",
    "best_params = study.best_params\n",
    "base_model_ebm = ExplainableBoostingClassifier(\n",
    "    learning_rate=best_params[\"learning_rate\"],\n",
    "    max_bins=best_params[\"max_bins\"],\n",
    "    max_interaction_bins=best_params[\"max_interaction_bins\"],\n",
    "    interactions=best_params[\"interactions\"],\n",
    "    outer_bags=best_params[\"outer_bags\"],\n",
    "    greedy_ratio=best_params[\"greedy_ratio\"],\n",
    "    min_samples_leaf=best_params[\"min_samples_leaf\"],\n",
    "    smoothing_rounds=best_params[\"smoothing_rounds\"],\n",
    "    max_rounds=best_params[\"max_rounds\"],\n",
    "    early_stopping_rounds=best_params[\"early_stopping_rounds\"],\n",
    "    random_state=42,\n",
    ")\n",
    "base_model_ebm.fit(x_train, y_train)\n",
    "\n",
    "y_test_pred = base_model_ebm.predict(x_test)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print(f\"EBM Accuracy with Best Parameters: {test_accuracy:.4f}\")\n",
    "\n",
    "report = classification_report(y_test, y_test_pred, target_names=target_names)\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(report)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=base_model_ebm.classes_)\n",
    "disp.plot(cmap='viridis', colorbar=False, text_kw={'fontsize': 13, 'fontweight': 'bold'})\n",
    "plt.title(\"Confusion Matrix\", fontsize=13, fontweight='bold')\n",
    "plt.xlabel(\"Predicted Label\", fontsize=12, fontweight='bold')\n",
    "plt.ylabel(\"True Label\", fontsize=12, fontweight='bold')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EBM Accuracy with Best Parameters: 0.7313\n"
     ]
    }
   ],
   "source": [
    "y_test_pred = base_model_ebm.predict(x_train)\n",
    "test_accuracy = accuracy_score(y_train, y_test_pred)\n",
    "print(f\"EBM Accuracy with Best Parameters: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **NGBoost**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=0.6931 val_loss=0.0000 scale=2.0000 norm=4.0000\n",
      "[iter 100] loss=0.5582 val_loss=0.0000 scale=2.0000 norm=3.7695\n",
      "[iter 200] loss=0.5503 val_loss=0.0000 scale=2.0000 norm=3.8935\n",
      "[iter 300] loss=0.5495 val_loss=0.0000 scale=2.0000 norm=3.9509\n",
      "[iter 400] loss=0.5415 val_loss=0.0000 scale=1.0000 norm=1.9638\n",
      "[iter 0] loss=0.6931 val_loss=0.0000 scale=2.0000 norm=4.0000\n",
      "[iter 100] loss=0.5504 val_loss=0.0000 scale=2.0000 norm=3.8262\n",
      "[iter 200] loss=0.5499 val_loss=0.0000 scale=1.0000 norm=1.9643\n",
      "[iter 300] loss=0.5440 val_loss=0.0000 scale=2.0000 norm=3.9314\n",
      "[iter 400] loss=0.5422 val_loss=0.0000 scale=2.0000 norm=3.9373\n",
      "[iter 0] loss=0.6931 val_loss=0.0000 scale=2.0000 norm=4.0000\n",
      "[iter 100] loss=0.5410 val_loss=0.0000 scale=2.0000 norm=3.9443\n",
      "[iter 200] loss=0.5403 val_loss=0.0000 scale=1.0000 norm=1.9822\n",
      "[iter 300] loss=0.5388 val_loss=0.0000 scale=1.0000 norm=1.9871\n",
      "[iter 400] loss=0.5344 val_loss=0.0000 scale=1.0000 norm=1.9772\n",
      "[iter 0] loss=0.6931 val_loss=0.0000 scale=2.0000 norm=4.0000\n",
      "[iter 100] loss=0.6078 val_loss=0.0000 scale=2.0000 norm=3.7430\n",
      "[iter 0] loss=0.6931 val_loss=0.0000 scale=2.0000 norm=4.0000\n",
      "[iter 100] loss=0.5425 val_loss=0.0000 scale=2.0000 norm=3.9457\n",
      "[iter 0] loss=0.6931 val_loss=0.0000 scale=2.0000 norm=4.0000\n",
      "[iter 100] loss=0.5450 val_loss=0.0000 scale=1.0000 norm=1.9718\n",
      "[iter 200] loss=0.5418 val_loss=0.0000 scale=2.0000 norm=3.9623\n",
      "[iter 300] loss=0.5372 val_loss=0.0000 scale=1.0000 norm=1.9739\n",
      "[iter 400] loss=0.5370 val_loss=0.0000 scale=2.0000 norm=3.9633\n",
      "[iter 0] loss=0.6931 val_loss=0.0000 scale=2.0000 norm=4.0000\n",
      "[iter 100] loss=0.5462 val_loss=0.0000 scale=2.0000 norm=3.8000\n",
      "[iter 200] loss=0.5523 val_loss=0.0000 scale=2.0000 norm=3.9615\n",
      "[iter 0] loss=0.6931 val_loss=0.0000 scale=2.0000 norm=4.0000\n",
      "[iter 100] loss=0.5421 val_loss=0.0000 scale=2.0000 norm=3.9051\n",
      "[iter 200] loss=0.5364 val_loss=0.0000 scale=1.0000 norm=1.9605\n",
      "[iter 300] loss=0.5437 val_loss=0.0000 scale=1.0000 norm=1.9938\n",
      "[iter 0] loss=0.6931 val_loss=0.0000 scale=2.0000 norm=4.0000\n",
      "[iter 100] loss=0.5407 val_loss=0.0000 scale=1.0000 norm=1.9834\n",
      "[iter 200] loss=0.5378 val_loss=0.0000 scale=0.5000 norm=0.9905\n",
      "[iter 300] loss=0.5377 val_loss=0.0000 scale=0.5000 norm=0.9925\n",
      "[iter 400] loss=0.5361 val_loss=0.0000 scale=0.0156 norm=0.0309\n",
      "[iter 0] loss=0.6931 val_loss=0.0000 scale=2.0000 norm=4.0000\n",
      "[iter 100] loss=0.5536 val_loss=0.0000 scale=1.0000 norm=1.9224\n",
      "[iter 200] loss=0.5435 val_loss=0.0000 scale=1.0000 norm=1.9561\n",
      "[iter 0] loss=0.6931 val_loss=0.0000 scale=2.0000 norm=4.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-01-21 15:25:11,007] Trial 10 failed with parameters: {'n_estimators': 313, 'learning_rate': 0.34821763034038666, 'minibatch_frac': 0.17173149499344809, 'col_sample': 0.5142092291645832} because of the following error: LinAlgError('Singular matrix').\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/qul/Developments/College/final_project/.venv_el/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_10295/2215775020.py\", line 31, in objective\n",
      "    model.fit(x_train_np, y_train_np)\n",
      "  File \"/tmp/ipykernel_10295/2215775020.py\", line 7, in fit\n",
      "    self.ngb.fit(X, y)\n",
      "  File \"/home/qul/Developments/College/final_project/.venv_el/lib/python3.10/site-packages/ngboost/ngboost.py\", line 250, in fit\n",
      "    return self.partial_fit(\n",
      "  File \"/home/qul/Developments/College/final_project/.venv_el/lib/python3.10/site-packages/ngboost/ngboost.py\", line 383, in partial_fit\n",
      "    grads = D.grad(Y_batch, natural=self.natural_gradient)\n",
      "  File \"/home/qul/Developments/College/final_project/.venv_el/lib/python3.10/site-packages/ngboost/scores.py\", line 12, in grad\n",
      "    grad = np.linalg.solve(metric, grad)\n",
      "  File \"/home/qul/Developments/College/final_project/.venv_el/lib/python3.10/site-packages/numpy/linalg/linalg.py\", line 409, in solve\n",
      "    r = gufunc(a, b, signature=signature, extobj=extobj)\n",
      "  File \"/home/qul/Developments/College/final_project/.venv_el/lib/python3.10/site-packages/numpy/linalg/linalg.py\", line 112, in _raise_linalgerror_singular\n",
      "    raise LinAlgError(\"Singular matrix\")\n",
      "numpy.linalg.LinAlgError: Singular matrix\n",
      "[W 2025-01-21 15:25:11,008] Trial 10 failed with value None.\n"
     ]
    },
    {
     "ename": "LinAlgError",
     "evalue": "Singular matrix",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 39\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m accuracy\n\u001b[1;32m     38\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 39\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3600\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest hyperparameters:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28mprint\u001b[39m(study\u001b[38;5;241m.\u001b[39mbest_params)\n",
      "File \u001b[0;32m~/Developments/College/final_project/.venv_el/lib/python3.10/site-packages/optuna/study/study.py:475\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    375\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    382\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    383\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    384\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \n\u001b[1;32m    386\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 475\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Developments/College/final_project/.venv_el/lib/python3.10/site-packages/optuna/study/_optimize.py:63\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 63\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     76\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/Developments/College/final_project/.venv_el/lib/python3.10/site-packages/optuna/study/_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 160\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m~/Developments/College/final_project/.venv_el/lib/python3.10/site-packages/optuna/study/_optimize.py:248\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    244\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    247\u001b[0m ):\n\u001b[0;32m--> 248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m~/Developments/College/final_project/.venv_el/lib/python3.10/site-packages/optuna/study/_optimize.py:197\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 197\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    199\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    200\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn[49], line 31\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     21\u001b[0m col_sample \u001b[38;5;241m=\u001b[39m trial\u001b[38;5;241m.\u001b[39msuggest_float(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcol_sample\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m1.0\u001b[39m)\n\u001b[1;32m     23\u001b[0m model \u001b[38;5;241m=\u001b[39m NGBClassifierWrapper(\n\u001b[1;32m     24\u001b[0m     Dist\u001b[38;5;241m=\u001b[39mBernoulli,\n\u001b[1;32m     25\u001b[0m     n_estimators\u001b[38;5;241m=\u001b[39mn_estimators,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m     col_sample\u001b[38;5;241m=\u001b[39mcol_sample\n\u001b[1;32m     29\u001b[0m )\n\u001b[0;32m---> 31\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train_np\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_np\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(x_test_np)\n\u001b[1;32m     34\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m accuracy_score(y_test_np, y_pred)\n",
      "Cell \u001b[0;32mIn[49], line 7\u001b[0m, in \u001b[0;36mNGBClassifierWrapper.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y):\n\u001b[0;32m----> 7\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mngb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(y) \n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/Developments/College/final_project/.venv_el/lib/python3.10/site-packages/ngboost/ngboost.py:250\u001b[0m, in \u001b[0;36mNGBoost.fit\u001b[0;34m(self, X, Y, X_val, Y_val, sample_weight, val_sample_weight, train_loss_monitor, val_loss_monitor, early_stopping_rounds)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscalings \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcol_idxs \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 250\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartial_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mY_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mY_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_sample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loss_monitor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loss_monitor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loss_monitor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_loss_monitor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Developments/College/final_project/.venv_el/lib/python3.10/site-packages/ngboost/ngboost.py:383\u001b[0m, in \u001b[0;36mNGBoost.partial_fit\u001b[0;34m(self, X, Y, X_val, Y_val, sample_weight, val_sample_weight, train_loss_monitor, val_loss_monitor, early_stopping_rounds)\u001b[0m\n\u001b[1;32m    381\u001b[0m loss_list \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [train_loss_monitor(D, Y_batch, weight_batch)]\n\u001b[1;32m    382\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_list[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m--> 383\u001b[0m grads \u001b[38;5;241m=\u001b[39m \u001b[43mD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mY_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnatural\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnatural_gradient\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    385\u001b[0m proj_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_base(X_batch, grads, weight_batch)\n\u001b[1;32m    386\u001b[0m scale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mline_search(proj_grad, P_batch, Y_batch, weight_batch)\n",
      "File \u001b[0;32m~/Developments/College/final_project/.venv_el/lib/python3.10/site-packages/ngboost/scores.py:12\u001b[0m, in \u001b[0;36mScore.grad\u001b[0;34m(self, Y, natural)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m natural:\n\u001b[1;32m     11\u001b[0m     metric \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetric()\n\u001b[0;32m---> 12\u001b[0m     grad \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m grad\n",
      "File \u001b[0;32m~/Developments/College/final_project/.venv_el/lib/python3.10/site-packages/numpy/linalg/linalg.py:409\u001b[0m, in \u001b[0;36msolve\u001b[0;34m(a, b)\u001b[0m\n\u001b[1;32m    407\u001b[0m signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDD->D\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m isComplexType(t) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdd->d\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    408\u001b[0m extobj \u001b[38;5;241m=\u001b[39m get_linalg_error_extobj(_raise_linalgerror_singular)\n\u001b[0;32m--> 409\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43mgufunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrap(r\u001b[38;5;241m.\u001b[39mastype(result_t, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\n",
      "File \u001b[0;32m~/Developments/College/final_project/.venv_el/lib/python3.10/site-packages/numpy/linalg/linalg.py:112\u001b[0m, in \u001b[0;36m_raise_linalgerror_singular\u001b[0;34m(err, flag)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_raise_linalgerror_singular\u001b[39m(err, flag):\n\u001b[0;32m--> 112\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LinAlgError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSingular matrix\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mLinAlgError\u001b[0m: Singular matrix"
     ]
    }
   ],
   "source": [
    "class NGBClassifierWrapper(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.ngb = NGBClassifier(**kwargs)\n",
    "        self.classes_ = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.ngb.fit(X, y)\n",
    "        self.classes_ = np.unique(y) \n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.ngb.predict(X)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return self.ngb.predict_proba(X)\n",
    "\n",
    "def objective(trial):\n",
    "    n_estimators = trial.suggest_int(\"n_estimators\", 50, 500)\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 0.01, 0.5, log=True)\n",
    "    minibatch_frac = trial.suggest_float(\"minibatch_frac\", 0.1, 1.0)\n",
    "    col_sample = trial.suggest_float(\"col_sample\", 0.1, 1.0)\n",
    "\n",
    "    model = NGBClassifierWrapper(\n",
    "        Dist=Bernoulli,\n",
    "        n_estimators=n_estimators,\n",
    "        learning_rate=learning_rate,\n",
    "        minibatch_frac=minibatch_frac,\n",
    "        col_sample=col_sample\n",
    "    )\n",
    "\n",
    "    model.fit(x_train_np, y_train_np)\n",
    "\n",
    "    y_pred = model.predict(x_test_np)\n",
    "    accuracy = accuracy_score(y_test_np, y_pred)\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=50, timeout=3600)\n",
    "\n",
    "print(\"Best hyperparameters:\")\n",
    "print(study.best_params)\n",
    "\n",
    "best_params = study.best_params\n",
    "base_model_ngb = NGBClassifierWrapper(\n",
    "    Dist=Bernoulli,\n",
    "    n_estimators=best_params[\"n_estimators\"],\n",
    "    learning_rate=best_params[\"learning_rate\"],\n",
    "    minibatch_frac=best_params[\"minibatch_frac\"],\n",
    "    col_sample=best_params[\"col_sample\"]\n",
    ")\n",
    "\n",
    "base_model_ngb.fit(x_train_np, y_train_np)\n",
    "y_pred = base_model_ngb.predict(x_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test_np, y_pred)\n",
    "print(f\"Optimized NGBClassifier Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "report = classification_report(y_test_np, y_pred, target_names=target_names)\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(report)\n",
    "\n",
    "cm = confusion_matrix(y_test_np, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=base_model_ngb.classes_)\n",
    "disp.plot(cmap='viridis', colorbar=False, text_kw={'fontsize': 13, 'fontweight': 'bold'})\n",
    "plt.title(\"Confusion Matrix\", fontsize=13, fontweight='bold')\n",
    "plt.xlabel(\"Predicted Label\", fontsize=12, fontweight='bold')\n",
    "plt.ylabel(\"True Label\", fontsize=12, fontweight='bold')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **SnapBoost**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SnapBoost Accuracy: 0.7242\n"
     ]
    }
   ],
   "source": [
    "def objective(trial):\n",
    "    num_round = trial.suggest_int(\"num_round\", 50, 200, step=50)\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 0.01, 0.2, step=0.01)\n",
    "    max_depth = trial.suggest_int(\"max_depth\", 3, 10, step=1)\n",
    "    colsample_bytree = trial.suggest_float(\"colsample_bytree\", 0.5, 1.0, step=0.1)\n",
    "    subsample = trial.suggest_float(\"subsample\", 0.5, 1.0, step=0.1)\n",
    "\n",
    "    model = BoostingMachineClassifier(\n",
    "        num_round=num_round,\n",
    "        learning_rate=learning_rate,\n",
    "        max_depth=max_depth,\n",
    "        colsample_bytree=colsample_bytree,\n",
    "        subsample=subsample,\n",
    "        random_state=42,\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "    model.fit(x_train_np, y_train_np)\n",
    "    y_pred = model.predict(x_test_np)\n",
    "    accuracy = accuracy_score(y_test_np, y_pred)\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "print(\"Best Parameters:\", study.best_params)\n",
    "print(\"Best Validation Accuracy:\", study.best_value)\n",
    "\n",
    "best_params = study.best_params\n",
    "base_model_snb = BoostingMachineClassifier(\n",
    "    num_round=best_params[\"num_round\"],\n",
    "    learning_rate=best_params[\"learning_rate\"],\n",
    "    max_depth=best_params[\"max_depth\"],\n",
    "    colsample_bytree=best_params[\"colsample_bytree\"],\n",
    "    subsample=best_params[\"subsample\"],\n",
    "    random_state=42,\n",
    "    verbose=False,\n",
    ")\n",
    "base_model_snb.fit(x_train_np, y_train_np)\n",
    "\n",
    "y_pred = base_model_snb.predict(x_test_np)\n",
    "test_accuracy = accuracy_score(y_test_np, y_pred)\n",
    "print(f\"SnapBoost Accuracy with Best Parameters: {test_accuracy:.4f}\")\n",
    "\n",
    "report = classification_report(y_test_np, y_pred, target_names=target_names)\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(report)\n",
    "\n",
    "cm = confusion_matrix(y_test_np, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=base_model_snb.classes_)\n",
    "disp.plot(cmap='viridis', colorbar=False, text_kw={'fontsize': 13, 'fontweight': 'bold'})\n",
    "plt.title(\"Confusion Matrix\", fontsize=13, fontweight='bold')\n",
    "plt.xlabel(\"Predicted Label\", fontsize=12, fontweight='bold')\n",
    "plt.ylabel(\"True Label\", fontsize=12, fontweight='bold')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **TabNet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.61783 | val_0_accuracy: 0.71763 |  0:00:01s\n",
      "epoch 1  | loss: 0.56573 | val_0_accuracy: 0.72285 |  0:00:02s\n",
      "epoch 2  | loss: 0.56013 | val_0_accuracy: 0.72348 |  0:00:03s\n",
      "epoch 3  | loss: 0.55764 | val_0_accuracy: 0.72352 |  0:00:05s\n",
      "epoch 4  | loss: 0.55687 | val_0_accuracy: 0.72622 |  0:00:06s\n",
      "epoch 5  | loss: 0.55642 | val_0_accuracy: 0.7268  |  0:00:07s\n",
      "epoch 6  | loss: 0.55412 | val_0_accuracy: 0.72605 |  0:00:09s\n",
      "epoch 7  | loss: 0.55303 | val_0_accuracy: 0.72802 |  0:00:10s\n",
      "epoch 8  | loss: 0.55386 | val_0_accuracy: 0.72735 |  0:00:11s\n",
      "epoch 9  | loss: 0.55315 | val_0_accuracy: 0.72822 |  0:00:13s\n",
      "epoch 10 | loss: 0.55321 | val_0_accuracy: 0.72795 |  0:00:14s\n",
      "epoch 11 | loss: 0.55237 | val_0_accuracy: 0.72811 |  0:00:15s\n",
      "epoch 12 | loss: 0.55274 | val_0_accuracy: 0.72897 |  0:00:16s\n",
      "epoch 13 | loss: 0.55249 | val_0_accuracy: 0.7282  |  0:00:18s\n",
      "epoch 14 | loss: 0.55191 | val_0_accuracy: 0.72899 |  0:00:19s\n",
      "epoch 15 | loss: 0.55224 | val_0_accuracy: 0.72935 |  0:00:20s\n",
      "epoch 16 | loss: 0.55186 | val_0_accuracy: 0.72797 |  0:00:22s\n",
      "epoch 17 | loss: 0.55115 | val_0_accuracy: 0.73032 |  0:00:23s\n",
      "epoch 18 | loss: 0.55132 | val_0_accuracy: 0.72895 |  0:00:24s\n",
      "epoch 19 | loss: 0.55171 | val_0_accuracy: 0.73079 |  0:00:26s\n",
      "epoch 20 | loss: 0.55085 | val_0_accuracy: 0.72879 |  0:00:27s\n",
      "epoch 21 | loss: 0.55123 | val_0_accuracy: 0.73097 |  0:00:28s\n",
      "epoch 22 | loss: 0.55094 | val_0_accuracy: 0.73057 |  0:00:30s\n",
      "epoch 23 | loss: 0.55046 | val_0_accuracy: 0.72791 |  0:00:31s\n",
      "epoch 24 | loss: 0.55292 | val_0_accuracy: 0.73028 |  0:00:32s\n",
      "Stop training because you reached max_epochs = 25 with best_epoch = 21 and best_val_0_accuracy = 0.73097\n",
      "TabNet Accuracy: 0.7224\n"
     ]
    }
   ],
   "source": [
    "def objective(trial):\n",
    "    n_d = trial.suggest_int(\"n_d\", 8, 64, step=8)  \n",
    "    n_a = trial.suggest_int(\"n_a\", 8, 64, step=8)  \n",
    "    n_steps = trial.suggest_int(\"n_steps\", 3, 10, step=1) \n",
    "    gamma = trial.suggest_float(\"gamma\", 1.0, 2.0, step=0.1) \n",
    "    lambda_sparse = trial.suggest_float(\"lambda_sparse\", 0.00001, 0.001, log=True)  \n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 0.01, 0.2, step=0.01)\n",
    "    max_epochs = 25  \n",
    "    \n",
    "    model = TabNetClassifier(\n",
    "        n_d=n_d,\n",
    "        n_a=n_a,\n",
    "        n_steps=n_steps,\n",
    "        gamma=gamma,\n",
    "        lambda_sparse=lambda_sparse,\n",
    "        optimizer_params=dict(lr=learning_rate),\n",
    "        seed=42,\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        x_train_np, y_train_np,\n",
    "        eval_set=[(x_test_np, y_test_np)],\n",
    "        eval_metric=[\"accuracy\"],\n",
    "        max_epochs=max_epochs,\n",
    "        patience=10,\n",
    "        verbose=0,\n",
    "    )\n",
    "\n",
    "    y_pred = model.predict(x_test_np)\n",
    "    accuracy = accuracy_score(y_test_np, y_pred)\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "print(\"Best Parameters:\", study.best_params)\n",
    "print(\"Best Validation Accuracy:\", study.best_value)\n",
    "\n",
    "best_params = study.best_params\n",
    "base_model_tabnet = TabNetClassifier(\n",
    "    n_d=best_params[\"n_d\"],\n",
    "    n_a=best_params[\"n_a\"],\n",
    "    n_steps=best_params[\"n_steps\"],\n",
    "    gamma=best_params[\"gamma\"],\n",
    "    lambda_sparse=best_params[\"lambda_sparse\"],\n",
    "    optimizer_params=dict(lr=best_params[\"learning_rate\"]),\n",
    "    seed=42,\n",
    ")\n",
    "base_model_tabnet.fit(\n",
    "    x_train_np, y_train_np,\n",
    "    eval_set=[(x_train_np, y_train_np)],\n",
    "    eval_metric=[\"accuracy\"],\n",
    "    max_epochs=25,\n",
    "    patience=10,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "y_pred = base_model_tabnet.predict(x_test_np)\n",
    "test_accuracy = accuracy_score(y_test_np, y_pred)\n",
    "print(f\"TabNet Accuracy with Best Parameters: {test_accuracy:.4f}\")\n",
    "\n",
    "report = classification_report(y_test_np, y_pred, target_names=target_names)\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(report)\n",
    "\n",
    "cm = confusion_matrix(y_test_np, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=base_model_tabnet.classes_)\n",
    "disp.plot(cmap='viridis', colorbar=False, text_kw={'fontsize': 13, 'fontweight': 'bold'})\n",
    "plt.title(\"Confusion Matrix\", fontsize=13, fontweight='bold')\n",
    "plt.xlabel(\"Predicted Label\", fontsize=12, fontweight='bold')\n",
    "plt.ylabel(\"True Label\", fontsize=12, fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **TabTransformer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TabTransformer Accuracy: 0.6710\n"
     ]
    }
   ],
   "source": [
    "categorical_indices = [5, 6, 7, 8, 9, 10]  # Indices of categorical features\n",
    "continuous_indices = [0, 1, 2, 3, 4]       # Indices of continuous features\n",
    "\n",
    "class TabTransformerWrapper(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, categories, num_continuous, dim=32, depth=4, heads=4, dim_out=1, lr=0.001, epochs=5, batch_size=64):\n",
    "        self.categories = categories\n",
    "        self.num_continuous = num_continuous\n",
    "        self.dim = dim\n",
    "        self.depth = depth\n",
    "        self.heads = heads\n",
    "        self.dim_out = dim_out\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.model = None\n",
    "\n",
    "    def _initialize_model(self):\n",
    "        from tab_transformer_pytorch import TabTransformer\n",
    "        self.model = TabTransformer(\n",
    "            categories=self.categories,\n",
    "            num_continuous=self.num_continuous,\n",
    "            dim=self.dim,\n",
    "            depth=self.depth,\n",
    "            heads=self.heads,\n",
    "            dim_out=self.dim_out\n",
    "        ).to(self.device)\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "        self.loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = np.asarray(X)\n",
    "        y = np.asarray(y)\n",
    "        self._initialize_model()\n",
    "\n",
    "        x_categ = torch.tensor(X[:, categorical_indices], dtype=torch.long).to(self.device)\n",
    "        x_cont = torch.tensor(X[:, continuous_indices], dtype=torch.float32).to(self.device)\n",
    "        y_tensor = torch.tensor(y, dtype=torch.float32).to(self.device)\n",
    "\n",
    "        dataset = torch.utils.data.TensorDataset(x_categ, x_cont, y_tensor)\n",
    "        dataloader = torch.utils.data.DataLoader(dataset, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            self.model.train()\n",
    "            for batch_x_categ, batch_x_cont, batch_y in dataloader:\n",
    "                self.optimizer.zero_grad()\n",
    "                preds = self.model(batch_x_categ, batch_x_cont).squeeze()\n",
    "                loss = self.loss_fn(preds, batch_y)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "        self.classes_ = np.array([0, 1])\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = np.asarray(X)\n",
    "        x_categ = torch.tensor(X[:, categorical_indices], dtype=torch.long).to(self.device)\n",
    "        x_cont = torch.tensor(X[:, continuous_indices], dtype=torch.float32).to(self.device)\n",
    "\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            preds = self.model(x_categ, x_cont).squeeze()\n",
    "        return (torch.sigmoid(preds) > 0.5).cpu().numpy().astype(int)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        X = np.asarray(X)\n",
    "        x_categ = torch.tensor(X[:, categorical_indices], dtype=torch.long).to(self.device)\n",
    "        x_cont = torch.tensor(X[:, continuous_indices], dtype=torch.float32).to(self.device)\n",
    "\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            preds = self.model(x_categ, x_cont).squeeze()\n",
    "        return np.column_stack((1 - torch.sigmoid(preds).cpu().numpy(), torch.sigmoid(preds).cpu().numpy()))\n",
    "\n",
    "def objective(trial):\n",
    "    dim = trial.suggest_int(\"dim\", 16, 64, step=16)\n",
    "    depth = trial.suggest_int(\"depth\", 2, 6)\n",
    "    heads = trial.suggest_int(\"heads\", 2, 8, step=2)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True)\n",
    "    batch_size = trial.suggest_int(\"batch_size\", 32, 128, step=32)\n",
    "    epochs = 5 \n",
    "\n",
    "    model = TabTransformerWrapper(\n",
    "        categories=[2, 3, 3, 2, 2, 2],\n",
    "        num_continuous=len(continuous_indices),\n",
    "        dim=dim,\n",
    "        depth=depth,\n",
    "        heads=heads,\n",
    "        dim_out=1,\n",
    "        lr=lr,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "\n",
    "    model.fit(x_train_np, y_train_np)\n",
    "    y_pred = model.predict(x_test_np)\n",
    "    accuracy = accuracy_score(y_test_np, y_pred)\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "print(\"Best Parameters:\", study.best_params)\n",
    "print(\"Best Validation Accuracy:\", study.best_value)\n",
    "\n",
    "best_params = study.best_params\n",
    "base_model_tabtf = TabTransformerWrapper(\n",
    "    categories=[2, 3, 3, 2, 2, 2],\n",
    "    num_continuous=len(continuous_indices),\n",
    "    dim=best_params[\"dim\"],\n",
    "    depth=best_params[\"depth\"],\n",
    "    heads=best_params[\"heads\"],\n",
    "    dim_out=1,\n",
    "    lr=best_params[\"lr\"],\n",
    "    epochs=5,\n",
    "    batch_size=best_params[\"batch_size\"],\n",
    ")\n",
    "base_model_tabtf.fit(x_train_np, y_train_np)\n",
    "\n",
    "y_pred = base_model_tabtf.predict(x_test_np)\n",
    "test_accuracy = accuracy_score(y_test_np, y_pred)\n",
    "print(f\"Test Accuracy with Best Parameters: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Meta Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    num_leaves = trial.suggest_int(\"num_leaves\", 15, 50, step=5)\n",
    "    max_depth = trial.suggest_int(\"max_depth\", 3, 12)\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 0.01, 0.2, step=0.01)\n",
    "    n_estimators = trial.suggest_int(\"n_estimators\", 50, 300, step=50)\n",
    "    subsample = trial.suggest_float(\"subsample\", 0.6, 1.0, step=0.1)\n",
    "    colsample_bytree = trial.suggest_float(\"colsample_bytree\", 0.6, 1.0, step=0.1)\n",
    "    reg_alpha = trial.suggest_float(\"reg_alpha\", 0.0, 1.0, step=0.1)\n",
    "    reg_lambda = trial.suggest_float(\"reg_lambda\", 0.0, 1.0, step=0.1)\n",
    "\n",
    "    model = lgb.LGBMClassifier(\n",
    "        num_leaves=num_leaves,\n",
    "        max_depth=max_depth,\n",
    "        learning_rate=learning_rate,\n",
    "        n_estimators=n_estimators,\n",
    "        subsample=subsample,\n",
    "        colsample_bytree=colsample_bytree,\n",
    "        reg_alpha=reg_alpha,\n",
    "        reg_lambda=reg_lambda,\n",
    "        random_state=42,\n",
    "    )\n",
    "\n",
    "    model.fit(x_train_np, y_train_np, eval_set=[(x_train_np, y_train_np)])\n",
    "    y_pred = model.predict(x_test_np)\n",
    "    accuracy = accuracy_score(y_test_np, y_pred)\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "print(\"Best Parameters:\", study.best_params)\n",
    "print(\"Best Validation Accuracy:\", study.best_value)\n",
    "\n",
    "best_params = study.best_params\n",
    "meta_model_lgb = lgb.LGBMClassifier(\n",
    "    num_leaves=best_params[\"num_leaves\"],\n",
    "    max_depth=best_params[\"max_depth\"],\n",
    "    learning_rate=best_params[\"learning_rate\"],\n",
    "    n_estimators=best_params[\"n_estimators\"],\n",
    "    subsample=best_params[\"subsample\"],\n",
    "    colsample_bytree=best_params[\"colsample_bytree\"],\n",
    "    reg_alpha=best_params[\"reg_alpha\"],\n",
    "    reg_lambda=best_params[\"reg_lambda\"],\n",
    "    random_state=42,\n",
    ")\n",
    "meta_model_lgb.fit(x_train_np, y_train_np)\n",
    "\n",
    "y_pred = meta_model_lgb.predict(x_test_np)\n",
    "accuracy = accuracy_score(y_test_np, y_pred)\n",
    "print(f\"\\nMeta-Model Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "report = classification_report(y_test_np, y_pred, target_names=target_names)\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(report)\n",
    "\n",
    "cm = confusion_matrix(y_test_np, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=meta_model_lgb.classes_)\n",
    "disp.plot(cmap='viridis', colorbar=False, text_kw={'fontsize': 13, 'fontweight': 'bold'})\n",
    "plt.title(\"Confusion Matrix\", fontsize=13, fontweight='bold')\n",
    "plt.xlabel(\"Predicted Label\", fontsize=12, fontweight='bold')\n",
    "plt.ylabel(\"True Label\", fontsize=12, fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Stacking Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_models = [\n",
    "    (\"ebc\", base_model_ebm), (\"ngb\", base_model_ngb), ('snb', base_model_snb), \n",
    "    ('tabnet', base_model_tabnet), ('tabtf', base_model_tabtf)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacking_model = StackingClassifier(\n",
    "    estimators=base_models,\n",
    "    final_estimator=meta_model_lgb\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=0.6931 val_loss=0.0000 scale=2.0000 norm=4.0000\n",
      "[iter 100] loss=0.5587 val_loss=0.0000 scale=2.0000 norm=3.7921\n",
      "[iter 200] loss=0.5491 val_loss=0.0000 scale=2.0000 norm=3.8779\n",
      "[iter 300] loss=0.5460 val_loss=0.0000 scale=2.0000 norm=3.9110\n",
      "[iter 400] loss=0.5443 val_loss=0.0000 scale=1.0000 norm=1.9657\n",
      "epoch 0  | loss: 0.61783 |  0:00:00s\n",
      "epoch 1  | loss: 0.56487 |  0:00:01s\n",
      "epoch 2  | loss: 0.56014 |  0:00:02s\n",
      "epoch 3  | loss: 0.55756 |  0:00:03s\n",
      "epoch 4  | loss: 0.55684 |  0:00:04s\n",
      "epoch 5  | loss: 0.55691 |  0:00:05s\n",
      "epoch 6  | loss: 0.5552  |  0:00:06s\n",
      "epoch 7  | loss: 0.55603 |  0:00:07s\n",
      "epoch 8  | loss: 0.5556  |  0:00:08s\n",
      "epoch 9  | loss: 0.55526 |  0:00:08s\n",
      "epoch 10 | loss: 0.55473 |  0:00:09s\n",
      "epoch 11 | loss: 0.5535  |  0:00:10s\n",
      "epoch 12 | loss: 0.55359 |  0:00:11s\n",
      "epoch 13 | loss: 0.55269 |  0:00:12s\n",
      "epoch 14 | loss: 0.55382 |  0:00:12s\n",
      "epoch 15 | loss: 0.55219 |  0:00:13s\n",
      "epoch 16 | loss: 0.55244 |  0:00:14s\n",
      "epoch 17 | loss: 0.55188 |  0:00:15s\n",
      "epoch 18 | loss: 0.55088 |  0:00:16s\n",
      "epoch 19 | loss: 0.5509  |  0:00:17s\n",
      "epoch 20 | loss: 0.55125 |  0:00:18s\n",
      "epoch 21 | loss: 0.5506  |  0:00:18s\n",
      "epoch 22 | loss: 0.55051 |  0:00:19s\n",
      "epoch 23 | loss: 0.55012 |  0:00:20s\n",
      "epoch 24 | loss: 0.55016 |  0:00:21s\n",
      "epoch 25 | loss: 0.54965 |  0:00:22s\n",
      "epoch 26 | loss: 0.54989 |  0:00:23s\n",
      "epoch 27 | loss: 0.54957 |  0:00:24s\n",
      "epoch 28 | loss: 0.54839 |  0:00:24s\n",
      "epoch 29 | loss: 0.54894 |  0:00:25s\n",
      "epoch 30 | loss: 0.54984 |  0:00:26s\n",
      "epoch 31 | loss: 0.55061 |  0:00:27s\n",
      "epoch 32 | loss: 0.55039 |  0:00:28s\n",
      "epoch 33 | loss: 0.54889 |  0:00:29s\n",
      "epoch 34 | loss: 0.5491  |  0:00:30s\n",
      "epoch 35 | loss: 0.54957 |  0:00:30s\n",
      "epoch 36 | loss: 0.55021 |  0:00:31s\n",
      "epoch 37 | loss: 0.54889 |  0:00:32s\n",
      "epoch 38 | loss: 0.54971 |  0:00:33s\n",
      "epoch 39 | loss: 0.54857 |  0:00:34s\n",
      "epoch 40 | loss: 0.54815 |  0:00:35s\n",
      "epoch 41 | loss: 0.5485  |  0:00:36s\n",
      "epoch 42 | loss: 0.54805 |  0:00:36s\n",
      "epoch 43 | loss: 0.54909 |  0:00:37s\n",
      "epoch 44 | loss: 0.54803 |  0:00:38s\n",
      "epoch 45 | loss: 0.54862 |  0:00:39s\n",
      "epoch 46 | loss: 0.54934 |  0:00:40s\n",
      "epoch 47 | loss: 0.54819 |  0:00:41s\n",
      "epoch 48 | loss: 0.54961 |  0:00:42s\n",
      "epoch 49 | loss: 0.54875 |  0:00:42s\n",
      "epoch 50 | loss: 0.54894 |  0:00:43s\n",
      "epoch 51 | loss: 0.5491  |  0:00:44s\n",
      "epoch 52 | loss: 0.54787 |  0:00:45s\n",
      "epoch 53 | loss: 0.54667 |  0:00:46s\n",
      "epoch 54 | loss: 0.54751 |  0:00:47s\n",
      "epoch 55 | loss: 0.54834 |  0:00:48s\n",
      "epoch 56 | loss: 0.54792 |  0:00:49s\n",
      "epoch 57 | loss: 0.54725 |  0:00:50s\n",
      "epoch 58 | loss: 0.54745 |  0:00:50s\n",
      "epoch 59 | loss: 0.54744 |  0:00:51s\n",
      "epoch 60 | loss: 0.54707 |  0:00:52s\n",
      "epoch 61 | loss: 0.54614 |  0:00:53s\n",
      "epoch 62 | loss: 0.54756 |  0:00:54s\n",
      "epoch 63 | loss: 0.54773 |  0:00:55s\n",
      "epoch 64 | loss: 0.54726 |  0:00:56s\n",
      "epoch 65 | loss: 0.54618 |  0:00:56s\n",
      "epoch 66 | loss: 0.54635 |  0:00:57s\n",
      "epoch 67 | loss: 0.54588 |  0:00:58s\n",
      "epoch 68 | loss: 0.54593 |  0:00:59s\n",
      "epoch 69 | loss: 0.5466  |  0:01:00s\n",
      "epoch 70 | loss: 0.54648 |  0:01:01s\n",
      "epoch 71 | loss: 0.54553 |  0:01:02s\n",
      "epoch 72 | loss: 0.54656 |  0:01:02s\n",
      "epoch 73 | loss: 0.54579 |  0:01:03s\n",
      "epoch 74 | loss: 0.54582 |  0:01:04s\n",
      "epoch 75 | loss: 0.54615 |  0:01:05s\n",
      "epoch 76 | loss: 0.54627 |  0:01:06s\n",
      "epoch 77 | loss: 0.54639 |  0:01:07s\n",
      "epoch 78 | loss: 0.54613 |  0:01:08s\n",
      "epoch 79 | loss: 0.54624 |  0:01:08s\n",
      "epoch 80 | loss: 0.54587 |  0:01:09s\n",
      "epoch 81 | loss: 0.54527 |  0:01:10s\n",
      "epoch 82 | loss: 0.54532 |  0:01:11s\n",
      "epoch 83 | loss: 0.54515 |  0:01:12s\n",
      "epoch 84 | loss: 0.54436 |  0:01:13s\n",
      "epoch 85 | loss: 0.54398 |  0:01:14s\n",
      "epoch 86 | loss: 0.54496 |  0:01:15s\n",
      "epoch 87 | loss: 0.54477 |  0:01:15s\n",
      "epoch 88 | loss: 0.54536 |  0:01:16s\n",
      "epoch 89 | loss: 0.54534 |  0:01:17s\n",
      "epoch 90 | loss: 0.54426 |  0:01:18s\n",
      "epoch 91 | loss: 0.54387 |  0:01:19s\n",
      "epoch 92 | loss: 0.54393 |  0:01:20s\n",
      "epoch 93 | loss: 0.54418 |  0:01:21s\n",
      "epoch 94 | loss: 0.54613 |  0:01:22s\n",
      "epoch 95 | loss: 0.5475  |  0:01:22s\n",
      "epoch 96 | loss: 0.54866 |  0:01:23s\n",
      "epoch 97 | loss: 0.54795 |  0:01:24s\n",
      "epoch 98 | loss: 0.5481  |  0:01:25s\n",
      "epoch 99 | loss: 0.54764 |  0:01:26s\n",
      "[iter 0] loss=0.6931 val_loss=0.0000 scale=2.0000 norm=4.0000\n",
      "[iter 100] loss=0.5579 val_loss=0.0000 scale=2.0000 norm=3.7903\n",
      "[iter 200] loss=0.5482 val_loss=0.0000 scale=1.0000 norm=1.9396\n",
      "[iter 300] loss=0.5451 val_loss=0.0000 scale=1.0000 norm=1.9570\n",
      "[iter 400] loss=0.5432 val_loss=0.0000 scale=1.0000 norm=1.9681\n",
      "[iter 0] loss=0.6931 val_loss=0.0000 scale=2.0000 norm=4.0000\n",
      "[iter 100] loss=0.5589 val_loss=0.0000 scale=2.0000 norm=3.7929\n",
      "[iter 200] loss=0.5489 val_loss=0.0000 scale=2.0000 norm=3.8824\n",
      "[iter 300] loss=0.5454 val_loss=0.0000 scale=1.0000 norm=1.9586\n",
      "[iter 400] loss=0.5439 val_loss=0.0000 scale=1.0000 norm=1.9670\n",
      "[iter 0] loss=0.6931 val_loss=0.0000 scale=2.0000 norm=4.0000\n",
      "[iter 100] loss=0.5603 val_loss=0.0000 scale=2.0000 norm=3.7916\n",
      "[iter 200] loss=0.5501 val_loss=0.0000 scale=2.0000 norm=3.8751\n",
      "[iter 300] loss=0.5470 val_loss=0.0000 scale=2.0000 norm=3.9070\n",
      "[iter 400] loss=0.5450 val_loss=0.0000 scale=2.0000 norm=3.9269\n",
      "[iter 0] loss=0.6931 val_loss=0.0000 scale=2.0000 norm=4.0000\n",
      "[iter 100] loss=0.5586 val_loss=0.0000 scale=2.0000 norm=3.7919\n",
      "[iter 200] loss=0.5483 val_loss=0.0000 scale=1.0000 norm=1.9393\n",
      "[iter 300] loss=0.5453 val_loss=0.0000 scale=1.0000 norm=1.9552\n",
      "[iter 400] loss=0.5439 val_loss=0.0000 scale=1.0000 norm=1.9632\n",
      "[iter 0] loss=0.6931 val_loss=0.0000 scale=2.0000 norm=4.0000\n",
      "[iter 100] loss=0.5574 val_loss=0.0000 scale=2.0000 norm=3.7894\n",
      "[iter 200] loss=0.5473 val_loss=0.0000 scale=1.0000 norm=1.9402\n",
      "[iter 300] loss=0.5443 val_loss=0.0000 scale=2.0000 norm=3.9108\n",
      "[iter 400] loss=0.5429 val_loss=0.0000 scale=1.0000 norm=1.9640\n",
      "epoch 0  | loss: 0.63265 |  0:00:00s\n",
      "epoch 1  | loss: 0.56791 |  0:00:01s\n",
      "epoch 2  | loss: 0.56298 |  0:00:02s\n",
      "epoch 3  | loss: 0.55913 |  0:00:02s\n",
      "epoch 4  | loss: 0.55865 |  0:00:03s\n",
      "epoch 5  | loss: 0.55631 |  0:00:04s\n",
      "epoch 6  | loss: 0.55592 |  0:00:04s\n",
      "epoch 7  | loss: 0.55541 |  0:00:05s\n",
      "epoch 8  | loss: 0.55455 |  0:00:06s\n",
      "epoch 9  | loss: 0.55299 |  0:00:06s\n",
      "epoch 10 | loss: 0.55297 |  0:00:07s\n",
      "epoch 11 | loss: 0.5522  |  0:00:08s\n",
      "epoch 12 | loss: 0.55202 |  0:00:08s\n",
      "epoch 13 | loss: 0.55218 |  0:00:09s\n",
      "epoch 14 | loss: 0.55069 |  0:00:10s\n",
      "epoch 15 | loss: 0.55043 |  0:00:11s\n",
      "epoch 16 | loss: 0.55101 |  0:00:11s\n",
      "epoch 17 | loss: 0.5511  |  0:00:12s\n",
      "epoch 18 | loss: 0.55071 |  0:00:13s\n",
      "epoch 19 | loss: 0.54958 |  0:00:13s\n",
      "epoch 20 | loss: 0.55125 |  0:00:14s\n",
      "epoch 21 | loss: 0.55038 |  0:00:15s\n",
      "epoch 22 | loss: 0.55156 |  0:00:15s\n",
      "epoch 23 | loss: 0.5497  |  0:00:16s\n",
      "epoch 24 | loss: 0.55025 |  0:00:17s\n",
      "epoch 25 | loss: 0.54826 |  0:00:18s\n",
      "epoch 26 | loss: 0.54815 |  0:00:18s\n",
      "epoch 27 | loss: 0.54856 |  0:00:19s\n",
      "epoch 28 | loss: 0.54972 |  0:00:20s\n",
      "epoch 29 | loss: 0.54861 |  0:00:20s\n",
      "epoch 30 | loss: 0.54843 |  0:00:21s\n",
      "epoch 31 | loss: 0.5482  |  0:00:22s\n",
      "epoch 32 | loss: 0.54856 |  0:00:23s\n",
      "epoch 33 | loss: 0.54793 |  0:00:24s\n",
      "epoch 34 | loss: 0.54769 |  0:00:24s\n",
      "epoch 35 | loss: 0.54678 |  0:00:25s\n",
      "epoch 36 | loss: 0.54688 |  0:00:26s\n",
      "epoch 37 | loss: 0.54874 |  0:00:27s\n",
      "epoch 38 | loss: 0.549   |  0:00:28s\n",
      "epoch 39 | loss: 0.54656 |  0:00:28s\n",
      "epoch 40 | loss: 0.54693 |  0:00:29s\n",
      "epoch 41 | loss: 0.54668 |  0:00:30s\n",
      "epoch 42 | loss: 0.54638 |  0:00:31s\n",
      "epoch 43 | loss: 0.54692 |  0:00:31s\n",
      "epoch 44 | loss: 0.54721 |  0:00:32s\n",
      "epoch 45 | loss: 0.54769 |  0:00:33s\n",
      "epoch 46 | loss: 0.54758 |  0:00:34s\n",
      "epoch 47 | loss: 0.54695 |  0:00:34s\n",
      "epoch 48 | loss: 0.54633 |  0:00:35s\n",
      "epoch 49 | loss: 0.54838 |  0:00:36s\n",
      "epoch 50 | loss: 0.54675 |  0:00:37s\n",
      "epoch 51 | loss: 0.54583 |  0:00:38s\n",
      "epoch 52 | loss: 0.54497 |  0:00:38s\n",
      "epoch 53 | loss: 0.5455  |  0:00:39s\n",
      "epoch 54 | loss: 0.54527 |  0:00:40s\n",
      "epoch 55 | loss: 0.54583 |  0:00:40s\n",
      "epoch 56 | loss: 0.54658 |  0:00:41s\n",
      "epoch 57 | loss: 0.54664 |  0:00:42s\n",
      "epoch 58 | loss: 0.54601 |  0:00:42s\n",
      "epoch 59 | loss: 0.54489 |  0:00:43s\n",
      "epoch 60 | loss: 0.54629 |  0:00:44s\n",
      "epoch 61 | loss: 0.54459 |  0:00:44s\n",
      "epoch 62 | loss: 0.54467 |  0:00:45s\n",
      "epoch 63 | loss: 0.54519 |  0:00:46s\n",
      "epoch 64 | loss: 0.54545 |  0:00:46s\n",
      "epoch 65 | loss: 0.54356 |  0:00:47s\n",
      "epoch 66 | loss: 0.54519 |  0:00:48s\n",
      "epoch 67 | loss: 0.54532 |  0:00:49s\n",
      "epoch 68 | loss: 0.54339 |  0:00:49s\n",
      "epoch 69 | loss: 0.54511 |  0:00:50s\n",
      "epoch 70 | loss: 0.54423 |  0:00:51s\n",
      "epoch 71 | loss: 0.54379 |  0:00:51s\n",
      "epoch 72 | loss: 0.54373 |  0:00:52s\n",
      "epoch 73 | loss: 0.54466 |  0:00:53s\n",
      "epoch 74 | loss: 0.5451  |  0:00:53s\n",
      "epoch 75 | loss: 0.54401 |  0:00:54s\n",
      "epoch 76 | loss: 0.54335 |  0:00:55s\n",
      "epoch 77 | loss: 0.54352 |  0:00:55s\n",
      "epoch 78 | loss: 0.54369 |  0:00:56s\n",
      "epoch 79 | loss: 0.54358 |  0:00:57s\n",
      "epoch 80 | loss: 0.54263 |  0:00:57s\n",
      "epoch 81 | loss: 0.54597 |  0:00:58s\n",
      "epoch 82 | loss: 0.54414 |  0:00:59s\n",
      "epoch 83 | loss: 0.54463 |  0:00:59s\n",
      "epoch 84 | loss: 0.54395 |  0:01:00s\n",
      "epoch 85 | loss: 0.54262 |  0:01:01s\n",
      "epoch 86 | loss: 0.54253 |  0:01:01s\n",
      "epoch 87 | loss: 0.54405 |  0:01:02s\n",
      "epoch 88 | loss: 0.54383 |  0:01:03s\n",
      "epoch 89 | loss: 0.54369 |  0:01:04s\n",
      "epoch 90 | loss: 0.54385 |  0:01:04s\n",
      "epoch 91 | loss: 0.54287 |  0:01:05s\n",
      "epoch 92 | loss: 0.54453 |  0:01:06s\n",
      "epoch 93 | loss: 0.54311 |  0:01:06s\n",
      "epoch 94 | loss: 0.54287 |  0:01:07s\n",
      "epoch 95 | loss: 0.54248 |  0:01:07s\n",
      "epoch 96 | loss: 0.5416  |  0:01:08s\n",
      "epoch 97 | loss: 0.54249 |  0:01:09s\n",
      "epoch 98 | loss: 0.54352 |  0:01:10s\n",
      "epoch 99 | loss: 0.54286 |  0:01:10s\n",
      "epoch 0  | loss: 0.62524 |  0:00:00s\n",
      "epoch 1  | loss: 0.57105 |  0:00:01s\n",
      "epoch 2  | loss: 0.56204 |  0:00:01s\n",
      "epoch 3  | loss: 0.55771 |  0:00:02s\n",
      "epoch 4  | loss: 0.55761 |  0:00:03s\n",
      "epoch 5  | loss: 0.5571  |  0:00:03s\n",
      "epoch 6  | loss: 0.55509 |  0:00:04s\n",
      "epoch 7  | loss: 0.55548 |  0:00:05s\n",
      "epoch 8  | loss: 0.55387 |  0:00:05s\n",
      "epoch 9  | loss: 0.5534  |  0:00:06s\n",
      "epoch 10 | loss: 0.55317 |  0:00:07s\n",
      "epoch 11 | loss: 0.5529  |  0:00:07s\n",
      "epoch 12 | loss: 0.55334 |  0:00:08s\n",
      "epoch 13 | loss: 0.55302 |  0:00:09s\n",
      "epoch 14 | loss: 0.55307 |  0:00:09s\n",
      "epoch 15 | loss: 0.55198 |  0:00:10s\n",
      "epoch 16 | loss: 0.55195 |  0:00:11s\n",
      "epoch 17 | loss: 0.55192 |  0:00:11s\n",
      "epoch 18 | loss: 0.55054 |  0:00:12s\n",
      "epoch 19 | loss: 0.55025 |  0:00:13s\n",
      "epoch 20 | loss: 0.55048 |  0:00:13s\n",
      "epoch 21 | loss: 0.54963 |  0:00:14s\n",
      "epoch 22 | loss: 0.5492  |  0:00:15s\n",
      "epoch 23 | loss: 0.54977 |  0:00:15s\n",
      "epoch 24 | loss: 0.54907 |  0:00:16s\n",
      "epoch 25 | loss: 0.54967 |  0:00:17s\n",
      "epoch 26 | loss: 0.54881 |  0:00:17s\n",
      "epoch 27 | loss: 0.54831 |  0:00:18s\n",
      "epoch 28 | loss: 0.5494  |  0:00:19s\n",
      "epoch 29 | loss: 0.54776 |  0:00:19s\n",
      "epoch 30 | loss: 0.54918 |  0:00:20s\n",
      "epoch 31 | loss: 0.55039 |  0:00:21s\n",
      "epoch 32 | loss: 0.54939 |  0:00:21s\n",
      "epoch 33 | loss: 0.54886 |  0:00:22s\n",
      "epoch 34 | loss: 0.54816 |  0:00:23s\n",
      "epoch 35 | loss: 0.54935 |  0:00:23s\n",
      "epoch 36 | loss: 0.54832 |  0:00:24s\n",
      "epoch 37 | loss: 0.54879 |  0:00:25s\n",
      "epoch 38 | loss: 0.54767 |  0:00:25s\n",
      "epoch 39 | loss: 0.54734 |  0:00:26s\n",
      "epoch 40 | loss: 0.54706 |  0:00:27s\n",
      "epoch 41 | loss: 0.54753 |  0:00:27s\n",
      "epoch 42 | loss: 0.54681 |  0:00:28s\n",
      "epoch 43 | loss: 0.54728 |  0:00:29s\n",
      "epoch 44 | loss: 0.54671 |  0:00:29s\n",
      "epoch 45 | loss: 0.54697 |  0:00:30s\n",
      "epoch 46 | loss: 0.54708 |  0:00:31s\n",
      "epoch 47 | loss: 0.54685 |  0:00:32s\n",
      "epoch 48 | loss: 0.54535 |  0:00:32s\n",
      "epoch 49 | loss: 0.54753 |  0:00:33s\n",
      "epoch 50 | loss: 0.54716 |  0:00:34s\n",
      "epoch 51 | loss: 0.54627 |  0:00:35s\n",
      "epoch 52 | loss: 0.54707 |  0:00:35s\n",
      "epoch 53 | loss: 0.5465  |  0:00:36s\n",
      "epoch 54 | loss: 0.54548 |  0:00:37s\n",
      "epoch 55 | loss: 0.54676 |  0:00:38s\n",
      "epoch 56 | loss: 0.54677 |  0:00:39s\n",
      "epoch 57 | loss: 0.54676 |  0:00:39s\n",
      "epoch 58 | loss: 0.54606 |  0:00:40s\n",
      "epoch 59 | loss: 0.54569 |  0:00:41s\n",
      "epoch 60 | loss: 0.54608 |  0:00:42s\n",
      "epoch 61 | loss: 0.54535 |  0:00:43s\n",
      "epoch 62 | loss: 0.54587 |  0:00:43s\n",
      "epoch 63 | loss: 0.54545 |  0:00:44s\n",
      "epoch 64 | loss: 0.54628 |  0:00:45s\n",
      "epoch 65 | loss: 0.54403 |  0:00:46s\n",
      "epoch 66 | loss: 0.54414 |  0:00:46s\n",
      "epoch 67 | loss: 0.54611 |  0:00:47s\n",
      "epoch 68 | loss: 0.5451  |  0:00:48s\n",
      "epoch 69 | loss: 0.54594 |  0:00:49s\n",
      "epoch 70 | loss: 0.54541 |  0:00:50s\n",
      "epoch 71 | loss: 0.54606 |  0:00:50s\n",
      "epoch 72 | loss: 0.54501 |  0:00:51s\n",
      "epoch 73 | loss: 0.54472 |  0:00:52s\n",
      "epoch 74 | loss: 0.54568 |  0:00:53s\n",
      "epoch 75 | loss: 0.54499 |  0:00:53s\n",
      "epoch 76 | loss: 0.54539 |  0:00:54s\n",
      "epoch 77 | loss: 0.54496 |  0:00:55s\n",
      "epoch 78 | loss: 0.5466  |  0:00:56s\n",
      "epoch 79 | loss: 0.54735 |  0:00:56s\n",
      "epoch 80 | loss: 0.54494 |  0:00:57s\n",
      "epoch 81 | loss: 0.54592 |  0:00:58s\n",
      "epoch 82 | loss: 0.54464 |  0:00:59s\n",
      "epoch 83 | loss: 0.54651 |  0:01:00s\n",
      "epoch 84 | loss: 0.54542 |  0:01:00s\n",
      "epoch 85 | loss: 0.54471 |  0:01:01s\n",
      "epoch 86 | loss: 0.54496 |  0:01:02s\n",
      "epoch 87 | loss: 0.54476 |  0:01:03s\n",
      "epoch 88 | loss: 0.54367 |  0:01:03s\n",
      "epoch 89 | loss: 0.54521 |  0:01:04s\n",
      "epoch 90 | loss: 0.54491 |  0:01:05s\n",
      "epoch 91 | loss: 0.54545 |  0:01:06s\n",
      "epoch 92 | loss: 0.54542 |  0:01:07s\n",
      "epoch 93 | loss: 0.54521 |  0:01:07s\n",
      "epoch 94 | loss: 0.54529 |  0:01:08s\n",
      "epoch 95 | loss: 0.54588 |  0:01:09s\n",
      "epoch 96 | loss: 0.54581 |  0:01:10s\n",
      "epoch 97 | loss: 0.54654 |  0:01:11s\n",
      "epoch 98 | loss: 0.54513 |  0:01:11s\n",
      "epoch 99 | loss: 0.54609 |  0:01:12s\n",
      "epoch 0  | loss: 0.62256 |  0:00:00s\n",
      "epoch 1  | loss: 0.56553 |  0:00:01s\n",
      "epoch 2  | loss: 0.56023 |  0:00:02s\n",
      "epoch 3  | loss: 0.55886 |  0:00:02s\n",
      "epoch 4  | loss: 0.55913 |  0:00:03s\n",
      "epoch 5  | loss: 0.55865 |  0:00:04s\n",
      "epoch 6  | loss: 0.55646 |  0:00:04s\n",
      "epoch 7  | loss: 0.55514 |  0:00:05s\n",
      "epoch 8  | loss: 0.55537 |  0:00:06s\n",
      "epoch 9  | loss: 0.55443 |  0:00:07s\n",
      "epoch 10 | loss: 0.55483 |  0:00:07s\n",
      "epoch 11 | loss: 0.55434 |  0:00:08s\n",
      "epoch 12 | loss: 0.55329 |  0:00:09s\n",
      "epoch 13 | loss: 0.55301 |  0:00:09s\n",
      "epoch 14 | loss: 0.55308 |  0:00:10s\n",
      "epoch 15 | loss: 0.55278 |  0:00:11s\n",
      "epoch 16 | loss: 0.55227 |  0:00:11s\n",
      "epoch 17 | loss: 0.55166 |  0:00:12s\n",
      "epoch 18 | loss: 0.55158 |  0:00:13s\n",
      "epoch 19 | loss: 0.55172 |  0:00:14s\n",
      "epoch 20 | loss: 0.55154 |  0:00:14s\n",
      "epoch 21 | loss: 0.55102 |  0:00:15s\n",
      "epoch 22 | loss: 0.55107 |  0:00:16s\n",
      "epoch 23 | loss: 0.5521  |  0:00:16s\n",
      "epoch 24 | loss: 0.55122 |  0:00:17s\n",
      "epoch 25 | loss: 0.55191 |  0:00:18s\n",
      "epoch 26 | loss: 0.55017 |  0:00:18s\n",
      "epoch 27 | loss: 0.55016 |  0:00:19s\n",
      "epoch 28 | loss: 0.55039 |  0:00:20s\n",
      "epoch 29 | loss: 0.54932 |  0:00:20s\n",
      "epoch 30 | loss: 0.54894 |  0:00:21s\n",
      "epoch 31 | loss: 0.54941 |  0:00:22s\n",
      "epoch 32 | loss: 0.55021 |  0:00:23s\n",
      "epoch 33 | loss: 0.54985 |  0:00:23s\n",
      "epoch 34 | loss: 0.54912 |  0:00:24s\n",
      "epoch 35 | loss: 0.54952 |  0:00:25s\n",
      "epoch 36 | loss: 0.54905 |  0:00:25s\n",
      "epoch 37 | loss: 0.54893 |  0:00:26s\n",
      "epoch 38 | loss: 0.54976 |  0:00:27s\n",
      "epoch 39 | loss: 0.54893 |  0:00:27s\n",
      "epoch 40 | loss: 0.54834 |  0:00:28s\n",
      "epoch 41 | loss: 0.54785 |  0:00:29s\n",
      "epoch 42 | loss: 0.54823 |  0:00:29s\n",
      "epoch 43 | loss: 0.54906 |  0:00:30s\n",
      "epoch 44 | loss: 0.54796 |  0:00:31s\n",
      "epoch 45 | loss: 0.54884 |  0:00:31s\n",
      "epoch 46 | loss: 0.54916 |  0:00:32s\n",
      "epoch 47 | loss: 0.54859 |  0:00:33s\n",
      "epoch 48 | loss: 0.54798 |  0:00:33s\n",
      "epoch 49 | loss: 0.54929 |  0:00:34s\n",
      "epoch 50 | loss: 0.54901 |  0:00:35s\n",
      "epoch 51 | loss: 0.54707 |  0:00:35s\n",
      "epoch 52 | loss: 0.548   |  0:00:36s\n",
      "epoch 53 | loss: 0.54818 |  0:00:37s\n",
      "epoch 54 | loss: 0.5478  |  0:00:38s\n",
      "epoch 55 | loss: 0.54835 |  0:00:38s\n",
      "epoch 56 | loss: 0.54755 |  0:00:39s\n",
      "epoch 57 | loss: 0.54923 |  0:00:39s\n",
      "epoch 58 | loss: 0.54819 |  0:00:40s\n",
      "epoch 59 | loss: 0.54785 |  0:00:41s\n",
      "epoch 60 | loss: 0.54775 |  0:00:41s\n",
      "epoch 61 | loss: 0.54807 |  0:00:42s\n",
      "epoch 62 | loss: 0.54751 |  0:00:43s\n",
      "epoch 63 | loss: 0.54814 |  0:00:43s\n",
      "epoch 64 | loss: 0.54902 |  0:00:44s\n",
      "epoch 65 | loss: 0.5462  |  0:00:45s\n",
      "epoch 66 | loss: 0.54599 |  0:00:46s\n",
      "epoch 67 | loss: 0.54694 |  0:00:46s\n",
      "epoch 68 | loss: 0.54771 |  0:00:47s\n",
      "epoch 69 | loss: 0.5479  |  0:00:48s\n",
      "epoch 70 | loss: 0.54736 |  0:00:48s\n",
      "epoch 71 | loss: 0.54698 |  0:00:49s\n",
      "epoch 72 | loss: 0.54624 |  0:00:50s\n",
      "epoch 73 | loss: 0.54777 |  0:00:50s\n",
      "epoch 74 | loss: 0.54621 |  0:00:51s\n",
      "epoch 75 | loss: 0.54593 |  0:00:52s\n",
      "epoch 76 | loss: 0.54599 |  0:00:53s\n",
      "epoch 77 | loss: 0.54673 |  0:00:53s\n",
      "epoch 78 | loss: 0.54655 |  0:00:54s\n",
      "epoch 79 | loss: 0.54627 |  0:00:55s\n",
      "epoch 80 | loss: 0.54591 |  0:00:55s\n",
      "epoch 81 | loss: 0.54573 |  0:00:56s\n",
      "epoch 82 | loss: 0.54629 |  0:00:57s\n",
      "epoch 83 | loss: 0.54916 |  0:00:58s\n",
      "epoch 84 | loss: 0.54803 |  0:00:58s\n",
      "epoch 85 | loss: 0.54636 |  0:00:59s\n",
      "epoch 86 | loss: 0.54715 |  0:01:00s\n",
      "epoch 87 | loss: 0.54649 |  0:01:00s\n",
      "epoch 88 | loss: 0.5462  |  0:01:01s\n",
      "epoch 89 | loss: 0.54511 |  0:01:02s\n",
      "epoch 90 | loss: 0.54693 |  0:01:03s\n",
      "epoch 91 | loss: 0.54504 |  0:01:03s\n",
      "epoch 92 | loss: 0.54616 |  0:01:04s\n",
      "epoch 93 | loss: 0.5452  |  0:01:05s\n",
      "epoch 94 | loss: 0.54613 |  0:01:06s\n",
      "epoch 95 | loss: 0.54537 |  0:01:06s\n",
      "epoch 96 | loss: 0.5453  |  0:01:07s\n",
      "epoch 97 | loss: 0.54513 |  0:01:08s\n",
      "epoch 98 | loss: 0.54458 |  0:01:09s\n",
      "epoch 99 | loss: 0.54599 |  0:01:09s\n",
      "epoch 0  | loss: 0.62782 |  0:00:00s\n",
      "epoch 1  | loss: 0.56822 |  0:00:01s\n",
      "epoch 2  | loss: 0.56044 |  0:00:02s\n",
      "epoch 3  | loss: 0.55805 |  0:00:03s\n",
      "epoch 4  | loss: 0.55726 |  0:00:03s\n",
      "epoch 5  | loss: 0.55631 |  0:00:04s\n",
      "epoch 6  | loss: 0.55681 |  0:00:05s\n",
      "epoch 7  | loss: 0.55539 |  0:00:06s\n",
      "epoch 8  | loss: 0.55581 |  0:00:06s\n",
      "epoch 9  | loss: 0.55617 |  0:00:07s\n",
      "epoch 10 | loss: 0.55521 |  0:00:08s\n",
      "epoch 11 | loss: 0.55436 |  0:00:09s\n",
      "epoch 12 | loss: 0.55476 |  0:00:09s\n",
      "epoch 13 | loss: 0.55474 |  0:00:10s\n",
      "epoch 14 | loss: 0.55407 |  0:00:11s\n",
      "epoch 15 | loss: 0.55344 |  0:00:11s\n",
      "epoch 16 | loss: 0.55288 |  0:00:12s\n",
      "epoch 17 | loss: 0.5541  |  0:00:13s\n",
      "epoch 18 | loss: 0.55294 |  0:00:13s\n",
      "epoch 19 | loss: 0.55264 |  0:00:14s\n",
      "epoch 20 | loss: 0.55293 |  0:00:14s\n",
      "epoch 21 | loss: 0.55325 |  0:00:15s\n",
      "epoch 22 | loss: 0.55278 |  0:00:16s\n",
      "epoch 23 | loss: 0.55356 |  0:00:16s\n",
      "epoch 24 | loss: 0.55211 |  0:00:17s\n",
      "epoch 25 | loss: 0.55278 |  0:00:18s\n",
      "epoch 26 | loss: 0.55177 |  0:00:18s\n",
      "epoch 27 | loss: 0.55132 |  0:00:19s\n",
      "epoch 28 | loss: 0.55279 |  0:00:20s\n",
      "epoch 29 | loss: 0.55268 |  0:00:20s\n",
      "epoch 30 | loss: 0.55211 |  0:00:21s\n",
      "epoch 31 | loss: 0.55154 |  0:00:21s\n",
      "epoch 32 | loss: 0.55241 |  0:00:22s\n",
      "epoch 33 | loss: 0.55182 |  0:00:23s\n",
      "epoch 34 | loss: 0.55053 |  0:00:23s\n",
      "epoch 35 | loss: 0.55118 |  0:00:24s\n",
      "epoch 36 | loss: 0.55051 |  0:00:25s\n",
      "epoch 37 | loss: 0.55147 |  0:00:25s\n",
      "epoch 38 | loss: 0.55094 |  0:00:26s\n",
      "epoch 39 | loss: 0.55217 |  0:00:27s\n",
      "epoch 40 | loss: 0.55096 |  0:00:28s\n",
      "epoch 41 | loss: 0.55047 |  0:00:28s\n",
      "epoch 42 | loss: 0.54971 |  0:00:29s\n",
      "epoch 43 | loss: 0.5522  |  0:00:30s\n",
      "epoch 44 | loss: 0.5503  |  0:00:30s\n",
      "epoch 45 | loss: 0.54918 |  0:00:31s\n",
      "epoch 46 | loss: 0.55018 |  0:00:32s\n",
      "epoch 47 | loss: 0.54917 |  0:00:32s\n",
      "epoch 48 | loss: 0.55021 |  0:00:33s\n",
      "epoch 49 | loss: 0.54971 |  0:00:34s\n",
      "epoch 50 | loss: 0.55047 |  0:00:34s\n",
      "epoch 51 | loss: 0.55002 |  0:00:35s\n",
      "epoch 52 | loss: 0.54949 |  0:00:36s\n",
      "epoch 53 | loss: 0.54948 |  0:00:37s\n",
      "epoch 54 | loss: 0.5495  |  0:00:37s\n",
      "epoch 55 | loss: 0.5505  |  0:00:38s\n",
      "epoch 56 | loss: 0.54893 |  0:00:39s\n",
      "epoch 57 | loss: 0.54969 |  0:00:40s\n",
      "epoch 58 | loss: 0.54887 |  0:00:40s\n",
      "epoch 59 | loss: 0.54995 |  0:00:41s\n",
      "epoch 60 | loss: 0.5493  |  0:00:42s\n",
      "epoch 61 | loss: 0.54976 |  0:00:43s\n",
      "epoch 62 | loss: 0.54921 |  0:00:43s\n",
      "epoch 63 | loss: 0.55034 |  0:00:44s\n",
      "epoch 64 | loss: 0.5517  |  0:00:45s\n",
      "epoch 65 | loss: 0.55216 |  0:00:46s\n",
      "epoch 66 | loss: 0.55187 |  0:00:46s\n",
      "epoch 67 | loss: 0.54956 |  0:00:47s\n",
      "epoch 68 | loss: 0.55076 |  0:00:48s\n",
      "epoch 69 | loss: 0.54972 |  0:00:49s\n",
      "epoch 70 | loss: 0.54906 |  0:00:49s\n",
      "epoch 71 | loss: 0.54874 |  0:00:50s\n",
      "epoch 72 | loss: 0.54863 |  0:00:51s\n",
      "epoch 73 | loss: 0.54843 |  0:00:51s\n",
      "epoch 74 | loss: 0.54857 |  0:00:52s\n",
      "epoch 75 | loss: 0.54928 |  0:00:53s\n",
      "epoch 76 | loss: 0.54978 |  0:00:54s\n",
      "epoch 77 | loss: 0.54785 |  0:00:54s\n",
      "epoch 78 | loss: 0.54772 |  0:00:55s\n",
      "epoch 79 | loss: 0.54882 |  0:00:56s\n",
      "epoch 80 | loss: 0.54768 |  0:00:57s\n",
      "epoch 81 | loss: 0.54772 |  0:00:57s\n",
      "epoch 82 | loss: 0.55004 |  0:00:58s\n",
      "epoch 83 | loss: 0.54668 |  0:00:59s\n",
      "epoch 84 | loss: 0.54718 |  0:00:59s\n",
      "epoch 85 | loss: 0.54709 |  0:01:00s\n",
      "epoch 86 | loss: 0.54773 |  0:01:00s\n",
      "epoch 87 | loss: 0.5474  |  0:01:01s\n",
      "epoch 88 | loss: 0.5489  |  0:01:02s\n",
      "epoch 89 | loss: 0.54813 |  0:01:02s\n",
      "epoch 90 | loss: 0.54728 |  0:01:03s\n",
      "epoch 91 | loss: 0.54762 |  0:01:04s\n",
      "epoch 92 | loss: 0.54794 |  0:01:04s\n",
      "epoch 93 | loss: 0.54718 |  0:01:05s\n",
      "epoch 94 | loss: 0.54639 |  0:01:06s\n",
      "epoch 95 | loss: 0.54665 |  0:01:06s\n",
      "epoch 96 | loss: 0.54626 |  0:01:07s\n",
      "epoch 97 | loss: 0.54579 |  0:01:07s\n",
      "epoch 98 | loss: 0.54652 |  0:01:08s\n",
      "epoch 99 | loss: 0.5474  |  0:01:09s\n",
      "epoch 0  | loss: 0.62757 |  0:00:00s\n",
      "epoch 1  | loss: 0.57145 |  0:00:01s\n",
      "epoch 2  | loss: 0.56499 |  0:00:01s\n",
      "epoch 3  | loss: 0.56269 |  0:00:02s\n",
      "epoch 4  | loss: 0.55938 |  0:00:03s\n",
      "epoch 5  | loss: 0.55715 |  0:00:03s\n",
      "epoch 6  | loss: 0.55756 |  0:00:04s\n",
      "epoch 7  | loss: 0.55663 |  0:00:04s\n",
      "epoch 8  | loss: 0.55599 |  0:00:05s\n",
      "epoch 9  | loss: 0.55544 |  0:00:06s\n",
      "epoch 10 | loss: 0.55395 |  0:00:07s\n",
      "epoch 11 | loss: 0.55486 |  0:00:07s\n",
      "epoch 12 | loss: 0.55519 |  0:00:08s\n",
      "epoch 13 | loss: 0.55542 |  0:00:08s\n",
      "epoch 14 | loss: 0.55506 |  0:00:09s\n",
      "epoch 15 | loss: 0.5555  |  0:00:10s\n",
      "epoch 16 | loss: 0.55366 |  0:00:10s\n",
      "epoch 17 | loss: 0.55429 |  0:00:11s\n",
      "epoch 18 | loss: 0.55342 |  0:00:12s\n",
      "epoch 19 | loss: 0.55164 |  0:00:12s\n",
      "epoch 20 | loss: 0.55322 |  0:00:13s\n",
      "epoch 21 | loss: 0.55291 |  0:00:13s\n",
      "epoch 22 | loss: 0.55214 |  0:00:14s\n",
      "epoch 23 | loss: 0.55136 |  0:00:15s\n",
      "epoch 24 | loss: 0.55184 |  0:00:15s\n",
      "epoch 25 | loss: 0.55403 |  0:00:16s\n",
      "epoch 26 | loss: 0.55264 |  0:00:17s\n",
      "epoch 27 | loss: 0.55203 |  0:00:17s\n",
      "epoch 28 | loss: 0.55202 |  0:00:18s\n",
      "epoch 29 | loss: 0.55217 |  0:00:19s\n",
      "epoch 30 | loss: 0.5505  |  0:00:19s\n",
      "epoch 31 | loss: 0.54951 |  0:00:20s\n",
      "epoch 32 | loss: 0.55039 |  0:00:20s\n",
      "epoch 33 | loss: 0.55018 |  0:00:21s\n",
      "epoch 34 | loss: 0.54967 |  0:00:22s\n",
      "epoch 35 | loss: 0.54984 |  0:00:22s\n",
      "epoch 36 | loss: 0.55053 |  0:00:23s\n",
      "epoch 37 | loss: 0.54954 |  0:00:24s\n",
      "epoch 38 | loss: 0.54831 |  0:00:24s\n",
      "epoch 39 | loss: 0.54865 |  0:00:25s\n",
      "epoch 40 | loss: 0.54864 |  0:00:25s\n",
      "epoch 41 | loss: 0.54926 |  0:00:26s\n",
      "epoch 42 | loss: 0.54997 |  0:00:27s\n",
      "epoch 43 | loss: 0.55043 |  0:00:27s\n",
      "epoch 44 | loss: 0.54919 |  0:00:28s\n",
      "epoch 45 | loss: 0.54899 |  0:00:29s\n",
      "epoch 46 | loss: 0.54974 |  0:00:29s\n",
      "epoch 47 | loss: 0.54916 |  0:00:30s\n",
      "epoch 48 | loss: 0.54802 |  0:00:31s\n",
      "epoch 49 | loss: 0.54868 |  0:00:31s\n",
      "epoch 50 | loss: 0.54682 |  0:00:32s\n",
      "epoch 51 | loss: 0.548   |  0:00:33s\n",
      "epoch 52 | loss: 0.54799 |  0:00:33s\n",
      "epoch 53 | loss: 0.54806 |  0:00:34s\n",
      "epoch 54 | loss: 0.54726 |  0:00:35s\n",
      "epoch 55 | loss: 0.54849 |  0:00:35s\n",
      "epoch 56 | loss: 0.54767 |  0:00:36s\n",
      "epoch 57 | loss: 0.54669 |  0:00:36s\n",
      "epoch 58 | loss: 0.54948 |  0:00:37s\n",
      "epoch 59 | loss: 0.55121 |  0:00:38s\n",
      "epoch 60 | loss: 0.54898 |  0:00:39s\n",
      "epoch 61 | loss: 0.55137 |  0:00:39s\n",
      "epoch 62 | loss: 0.54849 |  0:00:40s\n",
      "epoch 63 | loss: 0.54763 |  0:00:40s\n",
      "epoch 64 | loss: 0.5472  |  0:00:41s\n",
      "epoch 65 | loss: 0.54871 |  0:00:42s\n",
      "epoch 66 | loss: 0.54913 |  0:00:42s\n",
      "epoch 67 | loss: 0.5488  |  0:00:43s\n",
      "epoch 68 | loss: 0.54879 |  0:00:44s\n",
      "epoch 69 | loss: 0.54731 |  0:00:44s\n",
      "epoch 70 | loss: 0.54811 |  0:00:45s\n",
      "epoch 71 | loss: 0.54682 |  0:00:46s\n",
      "epoch 72 | loss: 0.54824 |  0:00:46s\n",
      "epoch 73 | loss: 0.54682 |  0:00:47s\n",
      "epoch 74 | loss: 0.54638 |  0:00:48s\n",
      "epoch 75 | loss: 0.54724 |  0:00:48s\n",
      "epoch 76 | loss: 0.54683 |  0:00:49s\n",
      "epoch 77 | loss: 0.54642 |  0:00:50s\n",
      "epoch 78 | loss: 0.54786 |  0:00:50s\n",
      "epoch 79 | loss: 0.5475  |  0:00:51s\n",
      "epoch 80 | loss: 0.54708 |  0:00:52s\n",
      "epoch 81 | loss: 0.54721 |  0:00:52s\n",
      "epoch 82 | loss: 0.54637 |  0:00:53s\n",
      "epoch 83 | loss: 0.54595 |  0:00:54s\n",
      "epoch 84 | loss: 0.54668 |  0:00:54s\n",
      "epoch 85 | loss: 0.54626 |  0:00:55s\n",
      "epoch 86 | loss: 0.54647 |  0:00:56s\n",
      "epoch 87 | loss: 0.54691 |  0:00:56s\n",
      "epoch 88 | loss: 0.54719 |  0:00:57s\n",
      "epoch 89 | loss: 0.54613 |  0:00:58s\n",
      "epoch 90 | loss: 0.54637 |  0:00:58s\n",
      "epoch 91 | loss: 0.54596 |  0:00:59s\n",
      "epoch 92 | loss: 0.54707 |  0:01:00s\n",
      "epoch 93 | loss: 0.54617 |  0:01:00s\n",
      "epoch 94 | loss: 0.54592 |  0:01:01s\n",
      "epoch 95 | loss: 0.54634 |  0:01:02s\n",
      "epoch 96 | loss: 0.54696 |  0:01:02s\n",
      "epoch 97 | loss: 0.54887 |  0:01:03s\n",
      "epoch 98 | loss: 0.54641 |  0:01:03s\n",
      "epoch 99 | loss: 0.54762 |  0:01:04s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-2 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-2 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"â–¸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"â–¾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-2 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-2 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-2 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>StackingClassifier(estimators=[(&#x27;ebc&#x27;, ExplainableBoostingClassifier()),\n",
       "                               (&#x27;ngb&#x27;, NGBClassifierWrapper()),\n",
       "                               (&#x27;snb&#x27;,\n",
       "                                BoostingMachineClassifier(max_depth=5,\n",
       "                                                          random_state=42)),\n",
       "                               (&#x27;tabnet&#x27;,\n",
       "                                TabNetClassifier(n_d=8,\n",
       "                                                 n_a=8,\n",
       "                                                 n_steps=3,\n",
       "                                                 gamma=1.3,\n",
       "                                                 cat_idxs=[],\n",
       "                                                 cat_dims=[],\n",
       "                                                 cat_emb_dim=[],\n",
       "                                                 n_independent=2,\n",
       "                                                 n_shared=2,\n",
       "                                                 epsilon=1e-15,\n",
       "                                                 momentum=0.02,\n",
       "                                                 lambda_sparse=0.001,\n",
       "                                                 seed=42,\n",
       "                                                 clip...ue=1,\n",
       "                                                 verbose=1,\n",
       "                                                 optimizer_fn=&lt;class &#x27;torch.optim.adam.Adam&#x27;&gt;,\n",
       "                                                 optimizer_params={&#x27;lr&#x27;: 0.02},\n",
       "                                                 scheduler_fn=None,\n",
       "                                                 scheduler_params={},\n",
       "                                                 mask_type=&#x27;sparsemax&#x27;,\n",
       "                                                 input_dim=11,\n",
       "                                                 output_dim=2,\n",
       "                                                 device_name=&#x27;auto&#x27;,\n",
       "                                                 n_shared_decoder=1,\n",
       "                                                 n_indep_decoder=1,\n",
       "                                                 grouped_features=[])),\n",
       "                               (&#x27;tabtf&#x27;,\n",
       "                                TabTransformerWrapper(categories=[2, 3, 3, 2, 2,\n",
       "                                                                  2],\n",
       "                                                      num_continuous=5))],\n",
       "                   final_estimator=SVC())</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>StackingClassifier</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.ensemble.StackingClassifier.html\">?<span>Documentation for StackingClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>StackingClassifier(estimators=[(&#x27;ebc&#x27;, ExplainableBoostingClassifier()),\n",
       "                               (&#x27;ngb&#x27;, NGBClassifierWrapper()),\n",
       "                               (&#x27;snb&#x27;,\n",
       "                                BoostingMachineClassifier(max_depth=5,\n",
       "                                                          random_state=42)),\n",
       "                               (&#x27;tabnet&#x27;,\n",
       "                                TabNetClassifier(n_d=8,\n",
       "                                                 n_a=8,\n",
       "                                                 n_steps=3,\n",
       "                                                 gamma=1.3,\n",
       "                                                 cat_idxs=[],\n",
       "                                                 cat_dims=[],\n",
       "                                                 cat_emb_dim=[],\n",
       "                                                 n_independent=2,\n",
       "                                                 n_shared=2,\n",
       "                                                 epsilon=1e-15,\n",
       "                                                 momentum=0.02,\n",
       "                                                 lambda_sparse=0.001,\n",
       "                                                 seed=42,\n",
       "                                                 clip...ue=1,\n",
       "                                                 verbose=1,\n",
       "                                                 optimizer_fn=&lt;class &#x27;torch.optim.adam.Adam&#x27;&gt;,\n",
       "                                                 optimizer_params={&#x27;lr&#x27;: 0.02},\n",
       "                                                 scheduler_fn=None,\n",
       "                                                 scheduler_params={},\n",
       "                                                 mask_type=&#x27;sparsemax&#x27;,\n",
       "                                                 input_dim=11,\n",
       "                                                 output_dim=2,\n",
       "                                                 device_name=&#x27;auto&#x27;,\n",
       "                                                 n_shared_decoder=1,\n",
       "                                                 n_indep_decoder=1,\n",
       "                                                 grouped_features=[])),\n",
       "                               (&#x27;tabtf&#x27;,\n",
       "                                TabTransformerWrapper(categories=[2, 3, 3, 2, 2,\n",
       "                                                                  2],\n",
       "                                                      num_continuous=5))],\n",
       "                   final_estimator=SVC())</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><label>ebc</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>ExplainableBoostingClassifier</div></div></label><div class=\"sk-toggleable__content fitted\"><pre>ExplainableBoostingClassifier()</pre></div> </div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><label>ngb</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>NGBClassifierWrapper</div></div></label><div class=\"sk-toggleable__content fitted\"><pre>NGBClassifierWrapper()</pre></div> </div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><label>snb</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>BoostingMachineClassifier</div></div></label><div class=\"sk-toggleable__content fitted\"><pre>BoostingMachineClassifier(max_depth=5, random_state=42)</pre></div> </div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><label>tabnet</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" ><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>TabNetClassifier</div></div></label><div class=\"sk-toggleable__content fitted\"><pre>TabNetClassifier(n_d=8, n_a=8, n_steps=3, gamma=1.3, cat_idxs=[], cat_dims=[], cat_emb_dim=[], n_independent=2, n_shared=2, epsilon=1e-15, momentum=0.02, lambda_sparse=0.001, seed=42, clip_value=1, verbose=1, optimizer_fn=&lt;class &#x27;torch.optim.adam.Adam&#x27;&gt;, optimizer_params={&#x27;lr&#x27;: 0.02}, scheduler_fn=None, scheduler_params={}, mask_type=&#x27;sparsemax&#x27;, input_dim=11, output_dim=2, device_name=&#x27;auto&#x27;, n_shared_decoder=1, n_indep_decoder=1, grouped_features=[])</pre></div> </div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><label>tabtf</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" ><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>TabTransformerWrapper</div></div></label><div class=\"sk-toggleable__content fitted\"><pre>TabTransformerWrapper(categories=[2, 3, 3, 2, 2, 2], num_continuous=5)</pre></div> </div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><label>final_estimator</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-12\" type=\"checkbox\" ><label for=\"sk-estimator-id-12\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>SVC</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.svm.SVC.html\">?<span>Documentation for SVC</span></a></div></label><div class=\"sk-toggleable__content fitted\"><pre>SVC()</pre></div> </div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "StackingClassifier(estimators=[('ebc', ExplainableBoostingClassifier()),\n",
       "                               ('ngb', NGBClassifierWrapper()),\n",
       "                               ('snb',\n",
       "                                BoostingMachineClassifier(max_depth=5,\n",
       "                                                          random_state=42)),\n",
       "                               ('tabnet',\n",
       "                                TabNetClassifier(n_d=8,\n",
       "                                                 n_a=8,\n",
       "                                                 n_steps=3,\n",
       "                                                 gamma=1.3,\n",
       "                                                 cat_idxs=[],\n",
       "                                                 cat_dims=[],\n",
       "                                                 cat_emb_dim=[],\n",
       "                                                 n_independent=2,\n",
       "                                                 n_shared=2,\n",
       "                                                 epsilon=1e-15,\n",
       "                                                 momentum=0.02,\n",
       "                                                 lambda_sparse=0.001,\n",
       "                                                 seed=42,\n",
       "                                                 clip...ue=1,\n",
       "                                                 verbose=1,\n",
       "                                                 optimizer_fn=<class 'torch.optim.adam.Adam'>,\n",
       "                                                 optimizer_params={'lr': 0.02},\n",
       "                                                 scheduler_fn=None,\n",
       "                                                 scheduler_params={},\n",
       "                                                 mask_type='sparsemax',\n",
       "                                                 input_dim=11,\n",
       "                                                 output_dim=2,\n",
       "                                                 device_name='auto',\n",
       "                                                 n_shared_decoder=1,\n",
       "                                                 n_indep_decoder=1,\n",
       "                                                 grouped_features=[])),\n",
       "                               ('tabtf',\n",
       "                                TabTransformerWrapper(categories=[2, 3, 3, 2, 2,\n",
       "                                                                  2],\n",
       "                                                      num_continuous=5))],\n",
       "                   final_estimator=SVC())"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacking_model.fit(x_train_np, y_train_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = stacking_model.predict(x_test_np)\n",
    "\n",
    "accuracy = accuracy_score(y_test_np, y_pred)\n",
    "print(f\"Stacking Model Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "report = classification_report(y_test_np, y_pred, target_names=target_names)\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(report)\n",
    "\n",
    "cm = confusion_matrix(y_test_np, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=stacking_model.classes_)\n",
    "disp.plot(cmap='viridis', colorbar=False, text_kw={'fontsize': 13, 'fontweight': 'bold'})\n",
    "plt.title(\"Confusion Matrix\", fontsize=13, fontweight='bold')\n",
    "plt.xlabel(\"Predicted Label\", fontsize=12, fontweight='bold')\n",
    "plt.ylabel(\"True Label\", fontsize=12, fontweight='bold')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_el",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
